"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[6882],{1926:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"voice-to-action/quiz","title":"Chapter 11 Quiz - Voice-to-Action Pipeline","description":"Test your understanding of voice-controlled robotics, Whisper ASR, LLMs, and end-to-end pipeline optimization","source":"@site/docs/voice-to-action/quiz.mdx","sourceDirName":"voice-to-action","slug":"/voice-to-action/quiz","permalink":"/HumanoidRoboticsBook/docs/voice-to-action/quiz","draft":false,"unlisted":false,"editUrl":"https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook/tree/main/docs/voice-to-action/quiz.mdx","tags":[],"version":"current","sidebarPosition":99,"frontMatter":{"id":"quiz","title":"Chapter 11 Quiz - Voice-to-Action Pipeline","sidebar_position":99,"description":"Test your understanding of voice-controlled robotics, Whisper ASR, LLMs, and end-to-end pipeline optimization"}}');var r=s(4848),t=s(8453);const l={id:"quiz",title:"Chapter 11 Quiz - Voice-to-Action Pipeline",sidebar_position:99,description:"Test your understanding of voice-controlled robotics, Whisper ASR, LLMs, and end-to-end pipeline optimization"},c="Chapter 11 Quiz: Voice-to-Action Pipeline",o={},d=[{value:"Question 1: Whisper Model Selection",id:"question-1-whisper-model-selection",level:2},{value:"Question 2: Pipeline Latency Budget",id:"question-2-pipeline-latency-budget",level:2},{value:"Question 3: Error Handling",id:"question-3-error-handling",level:2},{value:"Question 4: LLM Command Parsing",id:"question-4-llm-command-parsing",level:2},{value:"Question 5: Multimodal Input",id:"question-5-multimodal-input",level:2},{value:"Question 6: Pipeline Optimization",id:"question-6-pipeline-optimization",level:2},{value:"Question 7: Production Deployment",id:"question-7-production-deployment",level:2},{value:"Score Interpretation",id:"score-interpretation",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function a(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-11-quiz-voice-to-action-pipeline",children:"Chapter 11 Quiz: Voice-to-Action Pipeline"})}),"\n",(0,r.jsx)(n.p,{children:"Test your mastery of voice-controlled robot systems!"}),"\n",(0,r.jsx)(n.admonition,{title:"Quiz Instructions",type:"info",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"7 questions covering voice recognition, LLM parsing, VLA integration, and pipeline optimization"}),"\n",(0,r.jsx)(n.li,{children:"Detailed explanations provided for each answer"}),"\n",(0,r.jsx)(n.li,{children:"Score 80% or higher (6/7) to demonstrate mastery"}),"\n"]})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"question-1-whisper-model-selection",children:"Question 1: Whisper Model Selection"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"You need <3 second end-to-end latency for voice commands. Which Whisper model should you use?"})}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"A) Whisper Tiny (39M parameters, ~100ms)"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Fastest but too inaccurate"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Whisper Tiny specs:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Latency: ~100ms (excellent!)"}),"\n",(0,r.jsx)(n.li,{children:"Accuracy: 88% word accuracy (poor for robotics)"}),"\n",(0,r.jsx)(n.li,{children:"VRAM: 1 GB"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," 88% accuracy means 1 in 8 words wrong  frequent command failures"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example errors:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'User: "pick up the red cup"'}),"\n",(0,r.jsxs)(n.li,{children:['Tiny: "pick up the ',(0,r.jsx)(n.strong,{children:"dead"}),' cup" (12% error  task fails)']}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better:"})," Whisper Base (92% accuracy, 200ms) for best speed/accuracy trade-off."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"B) Whisper Base (74M parameters, ~200ms)"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Correct - Best Choice!"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Whisper Base is optimal for robotics:"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Advantages:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency:"})," ~200ms (well within <3s budget)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy:"})," 92% word accuracy (reliable for commands)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VRAM:"})," 1 GB (leaves room for VLA model)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multilingual:"})," Supports 99 languages"]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Latency breakdown for <3s target:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Audio capture: ~50ms"}),"\n",(0,r.jsx)(n.li,{children:"Whisper Base: ~200ms"}),"\n",(0,r.jsx)(n.li,{children:"LLM parsing: ~500-1000ms"}),"\n",(0,r.jsx)(n.li,{children:"VLA inference: ~100-200ms"}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 publish: ~30ms"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Total:"})," ~880-1480ms  (well under 3000ms)"]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why not larger models:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Medium/Large: 800-1500ms (too slow for <3s target)"}),"\n",(0,r.jsx)(n.li,{children:"Tiny/Small: Lower accuracy  more failures"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Production recommendation:"})," Whisper Base for robotics applications."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"C) Whisper Large (1.5B parameters, ~1500ms)"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Too slow"})]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Whisper Large specs:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Latency: ~1500ms (half the latency budget!)"}),"\n",(0,r.jsx)(n.li,{children:"Accuracy: 98% (excellent but overkill)"}),"\n",(0,r.jsx)(n.li,{children:"VRAM: 10 GB (doesn't fit with VLA model)"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," 1500ms Whisper + 1000ms LLM + 200ms VLA = 2700ms  very tight budget, risks >3s failures"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better:"})," Whisper Base (200ms) gives 1300ms more headroom for other pipeline stages."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"D) Whisper Medium (769M parameters, ~800ms)"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Possible but suboptimal"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Whisper Medium specs:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Latency: ~800ms (acceptable)"}),"\n",(0,r.jsx)(n.li,{children:"Accuracy: 97% (excellent)"}),"\n",(0,r.jsx)(n.li,{children:"VRAM: 5 GB"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-off:"})," Medium accuracy (97%) vs Base (92%) = only 5% improvement\n",(0,r.jsx)(n.strong,{children:"Cost:"})," 4x slower (800ms vs 200ms)"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For robotics:"})," 92% accuracy is sufficient; save 600ms for faster pipeline."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better:"})," Whisper Base unless ultra-high accuracy required (medical, safety-critical)."]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"question-2-pipeline-latency-budget",children:"Question 2: Pipeline Latency Budget"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Your pipeline latencies: Whisper (200ms), LLM (1200ms), VLA (150ms), ROS 2 (30ms). What's the bottleneck?"})}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"A) Whisper ASR (200ms)"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Not the bottleneck"})]}),(0,r.jsxs)(n.p,{children:["Whisper at 200ms is ",(0,r.jsx)(n.strong,{children:"only 12.7%"})," of total latency (1580ms)."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Bottleneck:"})," LLM parsing at 1200ms (76% of total latency)."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"B) LLM parsing (1200ms)"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Correct - This is the bottleneck!"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"LLM parsing dominates latency:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LLM:"})," 1200ms (76% of total)"]}),"\n",(0,r.jsx)(n.li,{children:"Whisper: 200ms (13%)"}),"\n",(0,r.jsx)(n.li,{children:"VLA: 150ms (9%)"}),"\n",(0,r.jsx)(n.li,{children:"ROS 2: 30ms (2%)"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Total:"})," 1580ms"]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"How to optimize LLM bottleneck:"})}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use smaller LLM:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Switch from Llama 2-13B  Llama 2-7B (2x faster)"}),"\n",(0,r.jsx)(n.li,{children:"Use quantized INT8 model (1.5-2x faster)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Simplify parsing:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Pre-defined templates instead of full JSON parsing"}),"\n",(0,r.jsx)(n.li,{children:"Rule-based parsing for common commands"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Cache common commands:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"pick up X"  cached template'}),"\n",(0,r.jsx)(n.li,{children:"Only run LLM for novel/complex commands"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parallel processing:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Start LLM parsing while Whisper still transcribing (pipeline parallelism)"}),"\n"]}),"\n"]}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Target:"})," Reduce LLM from 1200ms  500ms (2.4x speedup)  total latency 880ms"]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"C) VLA inference (150ms)"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Not the bottleneck"})]}),(0,r.jsxs)(n.p,{children:["VLA at 150ms is only ",(0,r.jsx)(n.strong,{children:"9.5%"})," of total latency."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Already optimized:"})," INT8 quantization gives 150ms (vs 300ms FP16)."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Bottleneck:"})," LLM at 1200ms."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"D) ROS 2 publishing (30ms)"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Not the bottleneck"})]}),(0,r.jsxs)(n.p,{children:["ROS 2 at 30ms is only ",(0,r.jsx)(n.strong,{children:"1.9%"})," of total latency - already very fast."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Bottleneck:"})," LLM at 1200ms (76%)."]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"question-3-error-handling",children:"Question 3: Error Handling"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Whisper returns confidence 65%. What should the system do?"})}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"A) Proceed with command execution anyway"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Dangerous!"})]}),(0,r.jsx)(n.p,{children:"Low confidence (65%) means high probability of transcription errors."}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Risk:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'User says: "place the cup on the table"'}),"\n",(0,r.jsxs)(n.li,{children:['Whisper (65% conf): "',(0,r.jsx)(n.strong,{children:"race"})," the cup ",(0,r.jsx)(n.strong,{children:"under"}),' the table"']}),"\n",(0,r.jsx)(n.li,{children:"Robot executes wrong command  potential damage"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better:"})," Reject low-confidence transcriptions, ask user to repeat."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"B) Reject command and ask user to repeat"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Correct - Safe and user-friendly!"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Best practice for low confidence (<70%):"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'text, confidence = whisper.transcribe(audio)\n\nif confidence < 0.70:  # 70% threshold\n    print("  Low confidence. Please repeat command.")\n    return False  # Don\'t execute\n'})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why 70% threshold:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"<70%:"})," High error probability  reject"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"70-85%:"})," Acceptable for non-critical tasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:">85%:"})," High confidence  execute immediately"]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"User experience:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'User: [mumbles] "pick... uh... cup"\nSystem: "  I didn\'t catch that. Please repeat."\nUser: [clearly] "pick up the red cup"\nSystem: " Understood: \'pick up the red cup\' (confidence: 94%)"\n'})}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Production systems:"})," Add retry counter (max 3 attempts) before fallback to manual control."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"C) Use the transcription but log a warning"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Risky compromise"})}),(0,r.jsx)(n.p,{children:"Logging helps debugging but doesn't prevent errors from reaching the robot."}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," Robot still executes potentially wrong command."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better:"})," Reject and ask for repeat (safe), OR use multi-modal confirmation (show transcription on screen, ask user to confirm)."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"D) Increase microphone volume and retry automatically"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Ineffective"})]}),(0,r.jsx)(n.p,{children:"Low confidence usually due to:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Background noise (increasing volume won't help)"}),"\n",(0,r.jsx)(n.li,{children:"Unclear speech (volume doesn't improve articulation)"}),"\n",(0,r.jsx)(n.li,{children:"Out-of-vocabulary words (volume irrelevant)"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better:"})," Ask user to speak clearly in quiet environment."]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"question-4-llm-command-parsing",children:"Question 4: LLM Command Parsing"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why parse voice commands into structured JSON before sending to VLA?"})}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"A) VLA models can't understand natural language directly"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Incorrect"})]}),(0,r.jsx)(n.p,{children:"VLA models (OpenVLA, GR00T N1) ARE trained on natural language inputs!"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"VLA input:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'image = robot_camera.capture()\ntext = "pick up the red cup"\nactions = vla_model(image, text)  # Natural language works!\n'})}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"So why parse?"})," Not because VLAs can't handle it, but for other benefits (see correct answer)."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"B) Structured format enables better error handling and validation"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Correct!"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Structured parsing adds robustness:"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Without parsing:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Raw voice command\ntext = "uhhh... pick up the... red thing... no wait, blue cup"\nactions = vla_model(image, text)  # Confusing input  unpredictable output\n'})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"With parsing:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# LLM extracts clean command\nparsed = llm_parser(text)\n# Output: {"action": "pick_up", "object": "cup", "color": "blue"}\n\n# Validate before VLA\nif parsed["action"] not in ALLOWED_ACTIONS:\n    return "Error: Unknown action"\nif parsed["object"] is None:\n    return "Error: No object specified"\n\n# Clean input to VLA\ninstruction = f"pick up the {parsed[\'color\']} {parsed[\'object\']}"\nactions = vla_model(image, instruction)\n'})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Benefits:"})}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Validation:"})," Catch invalid commands before VLA"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Normalization:"}),' Convert variations ("grab", "grasp", "pick up")  standardized "pick_up"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Error messages:"}),' Clear feedback ("No object specified") vs generic VLA failure']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Logging:"})," Structured data easier to analyze than raw text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-step:"})," Break complex commands into sequence of simple actions"]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example multi-step:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Voice: "pick up the red cup and place it on the table"\nLLM parses to:\n  Step 1: {"action": "pick_up", "object": "cup", "color": "red"}\n  Step 2: {"action": "place", "object": "cup", "destination": "table"}\n'})})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"C) JSON format is required by ROS 2"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Incorrect"})]}),(0,r.jsx)(n.p,{children:"ROS 2 uses message types (sensor_msgs/JointState), not JSON."}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"VLA output  ROS 2:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"actions = vla_model(image, text)  # numpy array\n\njoint_msg = JointState()\njoint_msg.position = actions.tolist()  # Convert to list, not JSON\nrobot.publish(joint_msg)\n"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"JSON parsing is for LLM  VLA, not VLA  ROS 2."})})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"D) It reduces VLA inference time"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Incorrect - Actually INCREASES total time"})]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Latency comparison:"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Direct (no parsing):"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Whisper: 200ms"}),"\n",(0,r.jsx)(n.li,{children:"VLA: 150ms"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Total:"})," 350ms"]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"With LLM parsing:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Whisper: 200ms"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"LLM parsing: +1000ms"})}),"\n",(0,r.jsx)(n.li,{children:"VLA: 150ms"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Total:"})," 1350ms (slower!)"]}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-off:"})," LLM parsing adds latency BUT improves robustness and error handling."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optimization:"})," Use fast LLM (Llama 2-7B INT8) or rule-based parsing for simple commands."]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"question-5-multimodal-input",children:"Question 5: Multimodal Input"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"VLA models require both image AND text. What happens if camera fails but voice works?"})}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"A) VLA model automatically uses previous frame"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Not automatic"})]}),(0,r.jsx)(n.p,{children:"VLAs don't have built-in frame caching. You must implement this explicitly:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class VLAPipeline:\n    def __init__(self):\n        self.last_valid_image = None\n\n    def generate_actions(self, current_image, text):\n        if current_image is None:\n            if self.last_valid_image is not None:\n                print("  Using cached image from 100ms ago")\n                current_image = self.last_valid_image\n            else:\n                return None  # No image available\n        else:\n            self.last_valid_image = current_image  # Cache for next iteration\n\n        return vla_model(current_image, text)\n'})}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Risk:"})," Stale image data  actions based on outdated scene."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"B) System should reject command and alert user"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Correct - Safest approach!"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Best practice when camera fails:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'if current_image is None:\n    print("L ERROR: No camera image available")\n    print("   Please check camera connection")\n    return False  # Don\'t execute blind commands\n'})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why this is best:"})}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety:"})," VLA needs visual context to generate safe actions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User feedback:"})," Clear error message"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Debugging:"})," User knows to check camera, not voice system"]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Alternative for non-critical tasks:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use cached frame (with timestamp check)"}),"\n",(0,r.jsx)(n.li,{children:"Use synthetic/placeholder image for testing"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For humanoid robots:"})," Visual feedback is CRITICAL for balance and navigation  always reject if no camera."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"C) VLA runs in text-only mode"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Not possible"})]}),(0,r.jsx)(n.p,{children:"VLA models (OpenVLA, GR00T N1) are trained with BOTH modalities. They can't operate with text alone."}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Architecture:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"vision_features = vision_encoder(image)   # Required\nlanguage_features = language_model(text)  # Required\nfused = fusion_layer(vision_features, language_features)\nactions = action_decoder(fused)\n"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Missing image = missing half the input  model fails or produces nonsense."})}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Text-only robot control:"})," Use classical NLP + inverse kinematics, not VLA."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"D) Robot executes last known successful action"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Dangerous"})]}),(0,r.jsx)(n.p,{children:"Repeating previous action without visual context can cause:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Collisions (environment changed)"}),"\n",(0,r.jsx)(n.li,{children:"Incorrect grasps (object moved)"}),"\n",(0,r.jsx)(n.li,{children:"Safety hazards (person entered workspace)"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better:"})," Reject command and alert user about camera failure."]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"question-6-pipeline-optimization",children:"Question 6: Pipeline Optimization"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"You want to reduce latency from 2.5s to <2.0s. Which optimization has BIGGEST impact?"})}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"A) Switch Whisper Base  Tiny (save 100ms)"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Small impact, big accuracy cost"})}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Savings:"})," 200ms  100ms = ",(0,r.jsx)(n.strong,{children:"-100ms"}),"\n",(0,r.jsx)(n.strong,{children:"Cost:"})," 92%  88% accuracy = ",(0,r.jsx)(n.strong,{children:"-4% accuracy"})]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Not worth it:"})," Accuracy loss causes more failures than 100ms saves."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"B) Quantize LLM to INT8 (save ~600ms)"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Correct - Biggest Impact!"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"LLM quantization:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"FP16:"})," ~1200ms"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"INT8:"})," ~600ms"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Savings: -600ms"})," (24% total latency reduction!)"]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"How to implement:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from transformers import BitsAndBytesConfig\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\nllm = AutoModelForCausalLM.from_pretrained(\n    "meta-llama/Llama-2-7b-chat-hf",\n    quantization_config=quant_config,\n    device_map="auto"\n)\n'})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Result:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Before:"})," Whisper (200ms) + LLM (1200ms) + VLA (150ms) = ",(0,r.jsx)(n.strong,{children:"2550ms"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"After:"})," Whisper (200ms) + LLM (600ms) + VLA (150ms) = ",(0,r.jsx)(n.strong,{children:"1950ms"}),"  <2000ms!"]}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Accuracy impact:"})," <1% (minimal)"]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"C) Use faster GPU (RTX 4090 vs 4070 Ti)"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Expensive, moderate impact"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Speed improvement:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Whisper: 200ms  150ms (-50ms)"}),"\n",(0,r.jsx)(n.li,{children:"LLM: 1200ms  900ms (-300ms)"}),"\n",(0,r.jsx)(n.li,{children:"VLA: 150ms  100ms (-50ms)"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Total savings: ~400ms"})}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cost:"})," $1600 GPU upgrade"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better:"})," INT8 quantization (free, -600ms) then optimize code before buying new hardware."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"D) Run Whisper and LLM in parallel"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Not possible - they're sequential"})]}),(0,r.jsx)(n.p,{children:"Pipeline stages have dependencies:"}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Whisper outputs text"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LLM needs Whisper's text"})," as input"]}),"\n",(0,r.jsx)(n.li,{children:"VLA needs LLM's parsed command"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Can't parallelize:"})," LLM must wait for Whisper to finish."]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What you CAN parallelize:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Whisper + VLA model loading (initialization)"}),"\n",(0,r.jsx)(n.li,{children:"Multiple voice commands in batch (if processing queue)"}),"\n"]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"question-7-production-deployment",children:"Question 7: Production Deployment"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"You're deploying voice control in noisy warehouse. What's the MOST important addition?"})}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"A) Louder microphone"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Makes problem worse"})]}),(0,r.jsx)(n.p,{children:"Louder microphone amplifies BOTH:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"User voice (good)"}),"\n",(0,r.jsx)(n.li,{children:"Background noise (bad)"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Result:"})," Even lower signal-to-noise ratio!"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better:"})," Noise-canceling microphone or noise suppression model."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:'B) Wake word detection ("Hey Robot")'}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Correct - Essential for noisy environments!"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Wake word prevents false activations:"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Without wake word:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"System listens continuously"}),"\n",(0,r.jsx)(n.li,{children:'Background voices: "pick up that box"  robot activates incorrectly L'}),"\n",(0,r.jsx)(n.li,{children:'Machine noise: "sshhh-pick-shhh"  spurious transcriptions L'}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"With wake word:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Stage 0: Wake word detection (runs continuously)\naudio_stream = microphone.stream()\nif wake_word_detector.detect("hey robot", audio_stream):\n    # Only NOW start voice command pipeline\n    command_audio = record_command(duration=3.0)\n    process_pipeline(command_audio)\n'})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Benefits:"})}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"False activation prevention:"}),' Only activates when user says "hey robot"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Privacy:"})," Not constantly transcribing ambient speech"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency:"})," Wake word runs on lightweight model (10-50ms), full pipeline only when needed"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User control:"})," Explicit activation signal"]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Implementation:"})}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from openwakeword import Model\n\nwake_model = Model(wakeword_models=["hey_robot.tflite"])\n\nwhile True:\n    audio_chunk = stream.read(1600)  # 100ms @ 16kHz\n    prediction = wake_model.predict(audio_chunk)\n\n    if prediction["hey_robot"] > 0.7:  # Confidence threshold\n        print("< Wake word detected! Listening for command...")\n        process_voice_command()\n'})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Popular wake word models:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Porcupine (Picovoice)"}),"\n",(0,r.jsx)(n.li,{children:"Snowboy (offline)"}),"\n",(0,r.jsx)(n.li,{children:"OpenWakeWord (open-source)"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Production:"}),' Use custom wake word ("Humanoid", "Robot", company name) for uniqueness.']})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"C) Faster Whisper model (Tiny instead of Base)"}),(0,r.jsxs)(n.p,{children:["L ",(0,r.jsx)(n.strong,{children:"Wrong direction"})]}),(0,r.jsx)(n.p,{children:"Noisy environments need MORE accuracy, not faster speed."}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Better:"})," Whisper Large (98% accuracy) to handle noisy audio, OR add noise suppression preprocessing."]})]}),"\n",(0,r.jsxs)(s,{children:[(0,r.jsx)("summary",{children:"D) Visual confirmation screen"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Helpful but not most important"})}),(0,r.jsx)(n.p,{children:"Visual confirmation is good UX but doesn't prevent false activations from background noise."}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Wake word"})," is more important - prevents system from listening to non-user voices."]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Best practice:"})," Wake word + visual confirmation together."]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"score-interpretation",children:"Score Interpretation"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Calculate your score:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["7/7 correct: < ",(0,r.jsx)(n.strong,{children:"Expert"})," - Ready to deploy voice-controlled robots"]}),"\n",(0,r.jsxs)(n.li,{children:["6/7 correct:  ",(0,r.jsx)(n.strong,{children:"Proficient"})," - Strong understanding"]}),"\n",(0,r.jsxs)(n.li,{children:["4-5/7 correct:  ",(0,r.jsx)(n.strong,{children:"Developing"})," - Review key optimization strategies"]}),"\n",(0,r.jsxs)(n.li,{children:["0-3/7 correct: = ",(0,r.jsx)(n.strong,{children:"Needs Review"})," - Re-read Chapter 11"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Question"}),(0,r.jsx)(n.th,{children:"Topic"}),(0,r.jsx)(n.th,{children:"Review Section"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Q1"}),(0,r.jsx)(n.td,{children:"Whisper Model Selection"}),(0,r.jsx)(n.td,{children:"Stage 2: Speech Recognition"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Q2"}),(0,r.jsx)(n.td,{children:"Latency Bottlenecks"}),(0,r.jsx)(n.td,{children:"Pipeline Architecture  Latency Budget"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Q3"}),(0,r.jsx)(n.td,{children:"Error Handling"}),(0,r.jsx)(n.td,{children:"Testing & Evaluation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Q4"}),(0,r.jsx)(n.td,{children:"Structured Parsing"}),(0,r.jsx)(n.td,{children:"Stage 3: Command Parsing with LLM"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Q5"}),(0,r.jsx)(n.td,{children:"Multimodal Requirements"}),(0,r.jsx)(n.td,{children:"Stage 4: VLA Action Generation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Q6"}),(0,r.jsx)(n.td,{children:"Optimization Strategies"}),(0,r.jsx)(n.td,{children:"Pipeline Architecture"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Q7"}),(0,r.jsx)(n.td,{children:"Production Deployment"}),(0,r.jsx)(n.td,{children:"Testing & Evaluation"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"After completing this quiz:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Score >= 80%:"})," Deploy complete voice-controlled robot!"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Score < 80%:"})," Review Chapter 11 sections"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hands-on:"})," Run ",(0,r.jsx)(n.code,{children:"code-examples/voice-pipeline/voice_robot.py"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Production:"})," Add wake word detection, test in real environment"]}),"\n"]}),"\n",(0,r.jsxs)(n.admonition,{title:"Final Challenge",type:"tip",children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Build complete voice-controlled humanoid:"})}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Integrate Chapters 6 (Isaac Sim) + 10 (VLA) + 11 (Voice)"}),"\n",(0,r.jsx)(n.li,{children:"Deploy on RTX 4070 Ti with microphone"}),"\n",(0,r.jsx)(n.li,{children:"Test with 20 real voice commands"}),"\n",(0,r.jsx)(n.li,{children:"Achieve <3 second end-to-end latency"}),"\n",(0,r.jsx)(n.li,{children:"Measure 80%+ success rate"}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Success = You've built a working voice-controlled robot!"})})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>c});var i=s(6540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);