"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[4994],{3254:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"vla-models/index","title":"10. Vision-Language-Action Models (VLA)","description":"This chapter introduces Vision-Language-Action (VLA) models, a new paradigm in AI that integrates visual perception, natural language understanding, and robotic control. VLAs enable robots to comprehend high-level commands and perform complex tasks in unstructured environments.","source":"@site/docs/10-vla-models/index.mdx","sourceDirName":"10-vla-models","slug":"/vla-models/","permalink":"/HumanoidRoboticsBook/docs/vla-models/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/10-vla-models/index.mdx","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"sidebar_position":10,"title":"10. Vision-Language-Action Models (VLA)"},"sidebar":"tutorialSidebar","previous":{"title":"9. Dexterous Manipulation with Humanoid Hands","permalink":"/HumanoidRoboticsBook/docs/dexterous-manipulation/"},"next":{"title":"11. Voice-to-Action Pipeline","permalink":"/HumanoidRoboticsBook/docs/voice-to-action/"}}');var a=o(4848),i=o(8453);const s={sidebar_position:10,title:"10. Vision-Language-Action Models (VLA)"},r="10. Vision-Language-Action Models (VLA)",d={},l=[{value:"What are VLA Models?",id:"what-are-vla-models",level:2},{value:"Architectures of VLA Models",id:"architectures-of-vla-models",level:2},{value:"Training and Deployment",id:"training-and-deployment",level:2},{value:"Data Acquisition and Formatting for VLA Models",id:"data-acquisition-and-formatting-for-vla-models",level:2},{value:"Further Exploration",id:"further-exploration",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",p:"p",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"10-vision-language-action-models-vla",children:"10. Vision-Language-Action Models (VLA)"})}),"\n",(0,a.jsx)(n.p,{children:"This chapter introduces Vision-Language-Action (VLA) models, a new paradigm in AI that integrates visual perception, natural language understanding, and robotic control. VLAs enable robots to comprehend high-level commands and perform complex tasks in unstructured environments."}),"\n",(0,a.jsx)(n.h2,{id:"what-are-vla-models",children:"What are VLA Models?"}),"\n",(0,a.jsx)(n.p,{children:"VLAs combine large language models (LLMs) with visual encoders and robotic action primitives, allowing for semantic understanding of the world and grounded action."}),"\n",(0,a.jsx)(n.h2,{id:"architectures-of-vla-models",children:"Architectures of VLA Models"}),"\n",(0,a.jsx)(n.p,{children:"We will explore different architectural approaches for building VLA systems, including those based on transformers and reinforcement learning."}),"\n",(0,a.jsx)(n.h2,{id:"training-and-deployment",children:"Training and Deployment"}),"\n",(0,a.jsx)(n.p,{children:"Training VLAs often requires vast datasets of paired visual observations, language instructions, and robot actions. This section will cover data collection, training strategies, and deployment considerations."}),"\n",(0,a.jsx)(n.h2,{id:"data-acquisition-and-formatting-for-vla-models",children:"Data Acquisition and Formatting for VLA Models"}),"\n",(0,a.jsx)(n.p,{children:"(Content to be added later describing how to acquire and format data for VLA models, e.g., synthetic data generation, real-world data collection, annotation pipelines, data augmentation strategies.)"}),"\n",(0,a.jsx)(n.h2,{id:"further-exploration",children:"Further Exploration"}),"\n",(0,a.jsx)(n.p,{children:"(Content to be added later)"})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>r});var t=o(6540);const a={},i=t.createContext(a);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);