"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9658],{4192:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"vla-models/index","title":"Chapter 10 - Vision-Language-Action Models","description":"Master Vision-Language-Action (VLA) models for humanoid robot control - from OpenVLA basics to GR00T N1 deployment","source":"@site/docs/vla-models/index.mdx","sourceDirName":"vla-models","slug":"/vla-models/","permalink":"/HumanoidRoboticsBook/docs/vla-models/","draft":false,"unlisted":false,"editUrl":"https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook/tree/main/docs/vla-models/index.mdx","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"id":"index","title":"Chapter 10 - Vision-Language-Action Models","sidebar_position":10,"description":"Master Vision-Language-Action (VLA) models for humanoid robot control - from OpenVLA basics to GR00T N1 deployment","keywords":["vla-models","vision-language-action","openvla","groot-n1","robot-learning","end-to-end-control","physical-intelligence"]},"sidebar":"tutorialSidebar","previous":{"title":"09. Dexterous Manipulation","permalink":"/HumanoidRoboticsBook/docs/dexterous-manipulation/"},"next":{"title":"11. Voice-to-Action Pipeline","permalink":"/HumanoidRoboticsBook/docs/voice-to-action/"}}');var r=i(4848),t=i(8453);const o={id:"index",title:"Chapter 10 - Vision-Language-Action Models",sidebar_position:10,description:"Master Vision-Language-Action (VLA) models for humanoid robot control - from OpenVLA basics to GR00T N1 deployment",keywords:["vla-models","vision-language-action","openvla","groot-n1","robot-learning","end-to-end-control","physical-intelligence"]},l="Chapter 10: Vision-Language-Action Models",a={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Why VLA Models Matter",id:"why-vla-models-matter",level:2},{value:"Traditional vs. VLA Approach",id:"traditional-vs-vla-approach",level:3},{value:"VLA Architecture Fundamentals",id:"vla-architecture-fundamentals",level:2},{value:"Component Breakdown",id:"component-breakdown",level:3},{value:"OpenVLA: Your First VLA Model",id:"openvla-your-first-vla-model",level:2},{value:"OpenVLA Architecture",id:"openvla-architecture",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Step 1: Install Dependencies",id:"step-1-install-dependencies",level:4},{value:"Step 2: Test Installation",id:"step-2-test-installation",level:4},{value:"Basic Inference Example",id:"basic-inference-example",level:3},{value:"Model Quantization (Reduce VRAM by 50-75%)",id:"model-quantization-reduce-vram-by-50-75",level:2},{value:"Quantization Trade-offs",id:"quantization-trade-offs",level:3},{value:"INT8 Quantization with bitsandbytes",id:"int8-quantization-with-bitsandbytes",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"GR00T N1: Humanoid-Specific VLA",id:"gr00t-n1-humanoid-specific-vla",level:2},{value:"GR00T N1 Architecture",id:"gr00t-n1-architecture",level:3},{value:"VLA Model Comparison",id:"vla-model-comparison",level:2},{value:"Quick Selection Guide",id:"quick-selection-guide",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-10-vision-language-action-models",children:"Chapter 10: Vision-Language-Action Models"})}),"\n",(0,r.jsx)(n.admonition,{title:"Chapter Overview",type:"info",children:(0,r.jsx)(n.p,{children:"Learn how Vision-Language-Action (VLA) models revolutionize robotics by directly mapping visual observations and natural language commands to robot actions - no manual programming required. Deploy state-of-the-art VLA models for humanoid control."})}),"\n",(0,r.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you'll be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Understand VLA architecture (vision encoders, language models, action decoders)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Compare 7+ VLA models (OpenVLA, GR00T N1, Helix, \u03c00, Octo, Gemini Robotics)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Run OpenVLA-7B inference on RTX GPU (12 GB VRAM)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Deploy GR00T N1 for humanoid locomotion and manipulation"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Optimize VLA models with 8-bit/4-bit quantization (reduce VRAM by 50-75%)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Integrate VLA models with ROS 2 for real robot control"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Evaluate VLA performance (success rate, latency, generalization)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Select appropriate VLA model for your use case"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before starting this chapter, you should:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 Complete ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"../isaac-platform",children:"Chapter 06: Isaac Platform"})})," (Isaac Sim fundamentals)"]}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Understand transformer architectures and attention mechanisms"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Have Python 3.10+ and PyTorch 2.0+ installed"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Have RTX GPU with 8+ GB VRAM (12 GB recommended)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Familiarity with Hugging Face Transformers library"}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{title:"Hardware Requirements",type:"tip",children:(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Minimum:"})," RTX 4060 Ti (16 GB VRAM) for OpenVLA-7B inference\n",(0,r.jsx)(n.strong,{children:"Recommended:"})," RTX 4070 Ti (12 GB) or RTX 4080 (16 GB) for fine-tuning\n",(0,r.jsx)(n.strong,{children:"Professional:"})," RTX 4090 (24 GB) or A100 (40 GB) for training from scratch"]})}),"\n",(0,r.jsx)(n.h2,{id:"why-vla-models-matter",children:"Why VLA Models Matter"}),"\n",(0,r.jsxs)(n.p,{children:["Traditional robot control requires ",(0,r.jsx)(n.strong,{children:"weeks of manual engineering"})," per task:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Train object detector (YOLOv8, Faster R-CNN)"}),"\n",(0,r.jsx)(n.li,{children:"Write grasp planner (heuristics, learned grasping networks)"}),"\n",(0,r.jsx)(n.li,{children:"Code motion controller (inverse kinematics, trajectory optimization)"}),"\n",(0,r.jsx)(n.li,{children:"Debug integration issues (coordinate frames, timing, edge cases)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"VLA models replace this entire pipeline with a single end-to-end model trained on demonstrations."})}),"\n",(0,r.jsx)(n.h3,{id:"traditional-vs-vla-approach",children:"Traditional vs. VLA Approach"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Traditional Pipeline"}),(0,r.jsx)(n.th,{children:"VLA Approach"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Development Time"})}),(0,r.jsx)(n.td,{children:"Weeks per task"}),(0,r.jsx)(n.td,{children:"Hours with fine-tuning"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Components"})}),(0,r.jsx)(n.td,{children:"3-5 separate modules"}),(0,r.jsx)(n.td,{children:"Single end-to-end model"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Generalization"})}),(0,r.jsx)(n.td,{children:"Brittle to new objects/scenes"}),(0,r.jsx)(n.td,{children:"Robust to novel situations"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Required Expertise"})}),(0,r.jsx)(n.td,{children:"Robotics PhD-level"}),(0,r.jsx)(n.td,{children:"Demonstration collection"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Typical Success Rate"})}),(0,r.jsx)(n.td,{children:"60-80% (task-specific)"}),(0,r.jsx)(n.td,{children:"70-90% (generalist)"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"vla-architecture-fundamentals",children:"VLA Architecture Fundamentals"}),"\n",(0,r.jsx)(n.p,{children:"All VLA models share a common three-component architecture:"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart LR\n    subgraph "Inputs"\n        I1[Camera Image<br/>RGB 224x224]\n        I2[Natural Language<br/>\'pick up red cup\']\n    end\n\n    subgraph "VLA Model Components"\n        V[Vision Encoder<br/>DINOv2, SigLIP]\n        L[Language Model<br/>Llama 2, Gemma]\n        F[Fusion Layer<br/>Cross-attention]\n        A[Action Decoder<br/>MLP, Diffusion, Flow]\n    end\n\n    subgraph "Output"\n        O[Robot Actions<br/>Joint positions + gripper]\n    end\n\n    I1 --\x3e V\n    I2 --\x3e L\n    V --\x3e F\n    L --\x3e F\n    F --\x3e A\n    A --\x3e O\n\n    style V fill:#00adef\n    style L fill:#ffcc00\n    style F fill:#9966ff\n    style A fill:#76b900'}),"\n",(0,r.jsx)(n.h3,{id:"component-breakdown",children:"Component Breakdown"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"1. Vision Encoder"})," (extracts visual features from robot camera)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"DINOv2"})," (Meta AI): Self-supervised ViT, excellent for robotics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SigLIP"})," (Google): Language-aligned vision encoder"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Input:"})," 224x224 RGB image \u2192 ",(0,r.jsx)(n.strong,{children:"Output:"})," 768-1024D feature vector"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"2. Language Model"})," (understands natural language commands)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Llama 2"})," (7B-70B params): Strong reasoning, open-source"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gemma"})," (2B-7B params): Efficient, fast inference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Input:"})," Text string \u2192 ",(0,r.jsx)(n.strong,{children:"Output:"})," Contextualized embeddings"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"3. Fusion Layer"})," (combines vision + language)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cross-attention:"})," Vision attends to language tokens"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gated fusion:"})," Learned importance weights for each modality"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"4. Action Decoder"})," (generates robot control commands)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MLP:"})," Fast (8-12 Hz), deterministic"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Diffusion:"})," Smooth (10-20 Hz), handles multi-modality"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Flow Matching:"})," Fastest + smoothest (50-120 Hz), used by \u03c00 and GR00T N1"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"openvla-your-first-vla-model",children:"OpenVLA: Your First VLA Model"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"OpenVLA"})," is the ideal starting point for learning VLAs:"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Open-source"})," (MIT license) - weights, code, data all public\n\u2705 ",(0,r.jsx)(n.strong,{children:"Strong performance"})," - Outperforms RT-2-X (55B) with only 7B parameters (+16.5% success rate)\n\u2705 ",(0,r.jsx)(n.strong,{children:"Well-documented"})," - Hugging Face Hub, tutorials, active community\n\u2705 ",(0,r.jsx)(n.strong,{children:"Reproducible"})," - Easy setup, no proprietary dependencies"]}),"\n",(0,r.jsx)(n.h3,{id:"openvla-architecture",children:"OpenVLA Architecture"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"OpenVLA-7B"})," uses a dual vision encoder approach:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Simplified architecture\nvision_features = torch.cat([\n    dinov2_encoder(image),  # Object-centric features\n    siglip_encoder(image)   # Language-aligned features\n], dim=-1)\n\nlanguage_features = llama2_7b(text_command)\n\nfused_features = cross_attention(vision_features, language_features)\n\nactions = mlp_head(fused_features)  # 7-DoF action vector\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Specifications:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Parameters:"})," 7.23 billion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision:"})," DINOv2-Base + SigLIP-Base (dual encoders)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language:"})," Llama 2-7B"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action Space:"})," 7-DoF (6 joints + gripper)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference Speed:"})," 80-120ms per action (8-12 Hz)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VRAM (FP16):"})," 14 GB | ",(0,r.jsx)(n.strong,{children:"VRAM (INT8):"})," 7 GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Training Data:"})," 970,000 robot trajectories (Open X-Embodiment dataset)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,r.jsx)(n.h4,{id:"step-1-install-dependencies",children:"Step 1: Install Dependencies"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Create virtual environment\npython3.10 -m venv openvla_env\nsource openvla_env/bin/activate  # Windows: openvla_env\\Scripts\\activate\n\n# Install PyTorch with CUDA 12.1\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\n# Install Transformers and quantization libraries\npip install transformers accelerate bitsandbytes pillow numpy\n\n# Install OpenVLA (from source for latest features)\ngit clone https://github.com/openvla/openvla.git\ncd openvla\npip install -e .\n"})}),"\n",(0,r.jsx)(n.h4,{id:"step-2-test-installation",children:"Step 2: Test Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from transformers import AutoModelForVision2Seq, AutoProcessor\n\n# Load model (downloads ~14 GB on first run)\nprocessor = AutoProcessor.from_pretrained("openvla/openvla-7b", trust_remote_code=True)\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "openvla/openvla-7b",\n    torch_dtype="float16",\n    device_map="auto",\n    trust_remote_code=True\n)\n\nprint(f"\u2705 OpenVLA-7B loaded successfully!")\nprint(f"   Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")\nprint(f"   Device: {model.device}")\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Expected output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u2705 OpenVLA-7B loaded successfully!\n   Parameters: 7.23B\n   Device: cuda:0\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"basic-inference-example",children:"Basic Inference Example"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"code-examples/vla/openvla_inference.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nOpenVLA-7B Basic Inference\nDemonstrates image + text \u2192 robot actions\n"""\n\nimport torch\nfrom transformers import AutoModelForVision2Seq, AutoProcessor\nfrom PIL import Image\nimport numpy as np\n\n# Initialize model\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nprint(f"Using device: {device}")\n\nprocessor = AutoProcessor.from_pretrained("openvla/openvla-7b", trust_remote_code=True)\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "openvla/openvla-7b",\n    torch_dtype=torch.float16,\n    device_map="auto",\n    trust_remote_code=True\n)\n\n# Load robot camera image\nimage = Image.open("robot_camera.jpg")  # 224x224 RGB recommended\ninstruction = "pick up the red cup and place it on the table"\n\n# Prepare inputs\ninputs = processor(text=instruction, images=image, return_tensors="pt").to(device)\n\n# Run inference\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n\n# Decode actions\naction_str = processor.decode(outputs[0], skip_special_tokens=True)\nactions = np.array([float(x) for x in action_str.split(",")])\n\nprint(f"\\n\ud83d\udcf8 Image: robot_camera.jpg")\nprint(f"\ud83d\udcac Command: {instruction}")\nprint(f"\ud83e\udd16 Actions: {actions}")\nprint(f"   Joints (6 DoF): {actions[:6]}")\nprint(f"   Gripper: {actions[6]:.2f} (0=open, 1=closed)")\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Run:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python code-examples/vla/openvla_inference.py\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Using device: cuda\n\n\ud83d\udcf8 Image: robot_camera.jpg\n\ud83d\udcac Command: pick up the red cup and place it on the table\n\ud83e\udd16 Actions: [ 0.45  0.23 -0.12  1.57  0.34 -0.21  1.00]\n   Joints (6 DoF): [ 0.45  0.23 -0.12  1.57  0.34 -0.21]\n   Gripper: 1.00 (0=open, 1=closed)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"model-quantization-reduce-vram-by-50-75",children:"Model Quantization (Reduce VRAM by 50-75%)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," OpenVLA-7B requires 14 GB VRAM (FP16), but most consumer GPUs have 8-12 GB."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution:"})," Quantization - reduce precision from FP16 to INT8 or INT4."]}),"\n",(0,r.jsx)(n.h3,{id:"quantization-trade-offs",children:"Quantization Trade-offs"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Precision"}),(0,r.jsx)(n.th,{children:"VRAM"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"Accuracy Loss"}),(0,r.jsx)(n.th,{children:"Recommended For"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FP16 (baseline)"}),(0,r.jsx)(n.td,{children:"14 GB"}),(0,r.jsx)(n.td,{children:"1x"}),(0,r.jsx)(n.td,{children:"0%"}),(0,r.jsx)(n.td,{children:"RTX 4080+"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"INT8 (8-bit)"}),(0,r.jsx)(n.td,{children:"7 GB"}),(0,r.jsx)(n.td,{children:"1.5-2x"}),(0,r.jsx)(n.td,{children:"<1%"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"RTX 4060 Ti+"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"INT4 (4-bit)"}),(0,r.jsx)(n.td,{children:"3.5 GB"}),(0,r.jsx)(n.td,{children:"2-3x"}),(0,r.jsx)(n.td,{children:"2-3%"}),(0,r.jsx)(n.td,{children:"RTX 3060 (experiments)"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommendation:"})," INT8 for production - minimal accuracy loss, 2x VRAM reduction."]}),"\n",(0,r.jsx)(n.h3,{id:"int8-quantization-with-bitsandbytes",children:"INT8 Quantization with bitsandbytes"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from transformers import AutoModelForVision2Seq, BitsAndBytesConfig\n\n# Configure 8-bit quantization\nquant_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    llm_int8_threshold=6.0  # Outlier detection\n)\n\n# Load quantized model\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "openvla/openvla-7b",\n    quantization_config=quant_config,\n    device_map="auto",\n    trust_remote_code=True\n)\n\nprint(f"\u2705 Model loaded in INT8")\nprint(f"   VRAM: ~7 GB (50% reduction from FP16)")\n\n# Inference works identically!\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Accuracy benchmarks:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"FP16:"})," 72.1% task success rate"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"INT8:"})," 71.6% success rate (-0.5%, negligible)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"INT4:"})," 69.8% success rate (-2.3%, acceptable for experimentation)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,r.jsxs)(n.p,{children:["Integrate OpenVLA with ROS 2 for real robot control. See ",(0,r.jsx)(n.code,{children:"code-examples/vla/openvla_ros2_node.py"})," (created separately)."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"gr00t-n1-humanoid-specific-vla",children:"GR00T N1: Humanoid-Specific VLA"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA GR00T N1"})," is the first open VLA specifically designed for humanoid robots."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why GR00T N1 for humanoids:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Dual-system architecture:"})," System 1 (120Hz motor control) + System 2 (VLM planning)"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Humanoid-specific training:"})," Data from bipedal robots (Fourier GR-1, 1X NEO)"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Open foundation model:"})," Fully customizable, no licensing restrictions"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Isaac Sim integration:"})," Seamless workflow with Chapter 06"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"gr00t-n1-architecture",children:"GR00T N1 Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    subgraph "System 2: Planning (VLM)"\n        S2I[Scene Image + Command]\n        S2V[Vision-Language Model]\n        S2P[High-Level Plan<br/>\'grasp cup, lift, move to table\']\n    end\n\n    subgraph "System 1: Motor Control (120Hz)"\n        S1P[Plan from System 2]\n        S1V[Visuomotor Policy]\n        S1A[Action Sequence<br/>120 actions/second]\n    end\n\n    subgraph "Robot"\n        R[Humanoid Robot<br/>Fourier GR-1, 1X NEO]\n    end\n\n    S2I --\x3e S2V\n    S2V --\x3e S2P\n    S2P --\x3e S1P\n    S1P --\x3e S1V\n    S1V --\x3e S1A\n    S1A --\x3e R\n\n    style S2V fill:#ffcc00\n    style S1V fill:#76b900\n    style R fill:#9966ff'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Innovation:"})," ",(0,r.jsx)(n.strong,{children:"Separation of concerns"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"System 2 (slow):"})," Reasons about task, generates plan (1-2 Hz)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"System 1 (fast):"})," Executes plan with reactive motor control (120 Hz)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["This matches human cognition: ",(0,r.jsx)(n.strong,{children:"deliberate planning + reactive execution"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"vla-model-comparison",children:"VLA Model Comparison"}),"\n",(0,r.jsxs)(n.p,{children:["Based on research from ",(0,r.jsx)(n.code,{children:"specs/001-physical-ai-book/research.md"}),", here's a practical comparison:"]}),"\n",(0,r.jsx)(n.h3,{id:"quick-selection-guide",children:"Quick Selection Guide"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Choose OpenVLA if:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Learning VLAs for the first time"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Need reproducible research baseline"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Have RTX 4070 Ti (12 GB VRAM)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Want Hugging Face integration"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Choose GR00T N1 if:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Building humanoid robot application"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Using NVIDIA Isaac Sim ecosystem"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Need 120Hz real-time control"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Want dual-system architecture"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Choose \u03c00/\u03c00.5 if:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Need smooth flow-matching trajectories (50 Hz)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Mobile manipulator application"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Want open-world generalization"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Have 6-8 GB VRAM (efficient)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Choose Octo if:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Transfer learning across robot types"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Fast fine-tuning (hours on consumer GPU)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Goal-conditioned tasks (image goals)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Research on embodiment adaptation"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["For full comparison tables (hardware requirements, licensing, performance benchmarks), see ",(0,r.jsx)(n.code,{children:"specs/001-physical-ai-book/research.md"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VLA models are the future of robot control"})," - end-to-end learning eliminates weeks of manual engineering"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenVLA is the best starting point"})," - open-source, well-documented, strong performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Quantization is essential"})," - INT8 reduces VRAM by 50% with <1% accuracy loss"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GR00T N1 for humanoids"})," - dual-system architecture designed for bipedal robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model selection matters"})," - choose based on use case (education vs production, humanoid vs manipulation)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Practice:"})," Run OpenVLA inference examples in ",(0,r.jsx)(n.code,{children:"code-examples/vla/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Experiment:"})," Test different quantization levels (INT8 vs INT4)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrate:"})," Connect OpenVLA to your ROS 2 robot (real or simulated)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Advanced:"})," Explore fine-tuning OpenVLA on custom tasks (50-100 demos)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Production:"})," Deploy GR00T N1 for humanoid applications with Isaac Sim"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Continue to ",(0,r.jsx)(n.a,{href:"../voice-to-action",children:"Chapter 11: Voice-to-Action Pipeline"})," to add speech recognition and complete the end-to-end system!"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);