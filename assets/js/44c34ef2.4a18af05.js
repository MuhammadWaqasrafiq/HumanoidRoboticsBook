"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[224],{6533:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"capstone-butler/index","title":"Chapter 13 - Complete Capstone Autonomous Butler Project","description":"Synthesize all skills to build an end-to-end autonomous humanoid butler capable of performing complex household tasks.","source":"@site/docs/capstone-butler/index.mdx","sourceDirName":"capstone-butler","slug":"/capstone-butler/","permalink":"/HumanoidRoboticsBook/docs/capstone-butler/","draft":false,"unlisted":false,"editUrl":"https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook/tree/main/docs/capstone-butler/index.mdx","tags":[],"version":"current","sidebarPosition":13,"frontMatter":{"id":"index","title":"Chapter 13 - Complete Capstone Autonomous Butler Project","sidebar_position":13,"description":"Synthesize all skills to build an end-to-end autonomous humanoid butler capable of performing complex household tasks.","keywords":["capstone-project","autonomous-robot","humanoid-butler","integration","physical-ai"]},"sidebar":"tutorialSidebar","previous":{"title":"12. Sim-to-Real Transfer","permalink":"/HumanoidRoboticsBook/docs/sim-to-real/"},"next":{"title":"A. Lab Build Guides","permalink":"/HumanoidRoboticsBook/docs/appendices/lab-build-guides"}}');var i=t(4848),s=t(8453);const a={id:"index",title:"Chapter 13 - Complete Capstone Autonomous Butler Project",sidebar_position:13,description:"Synthesize all skills to build an end-to-end autonomous humanoid butler capable of performing complex household tasks.",keywords:["capstone-project","autonomous-robot","humanoid-butler","integration","physical-ai"]},r="Chapter 13: Complete Capstone Autonomous Butler Project",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Project Overview: The Autonomous Butler",id:"project-overview-the-autonomous-butler",level:2},{value:"Core Capabilities of the Butler",id:"core-capabilities-of-the-butler",level:3},{value:"High-Level System Architecture",id:"high-level-system-architecture",level:3},{value:"Component Integration Diagram",id:"component-integration-diagram",level:2},{value:"Data Flow for &quot;Pick Up Object&quot; Task",id:"data-flow-for-pick-up-object-task",level:2},{value:"Task Planner State Machine",id:"task-planner-state-machine",level:2},{value:"Error Recovery Workflow",id:"error-recovery-workflow",level:2},{value:"High-Level Task Decomposition",id:"high-level-task-decomposition",level:2},{value:"Integration Challenges and SolutionsBuilding an integrated system introduces new complexities beyond individual component development.",id:"integration-challenges-and-solutionsbuilding-an-integrated-system-introduces-new-complexities-beyond-individual-component-development",level:2},{value:"1. Inter-Process Communication (IPC)",id:"1-inter-process-communication-ipc",level:3},{value:"2. State Management",id:"2-state-management",level:3},{value:"3. Error Handling and Robustness",id:"3-error-handling-and-robustness",level:3},{value:"High-Level Task Decomposition",id:"high-level-task-decomposition-1",level:2},{value:"Building the Butler: Step-by-Step Guide",id:"building-the-butler-step-by-step-guide",level:2},{value:"Step 1: Initialize Capstone Repository",id:"step-1-initialize-capstone-repository",level:3},{value:"Step 2: Implement Core Modules",id:"step-2-implement-core-modules",level:3},{value:"A. Perception Module (<code>capstone_perception</code>)",id:"a-perception-module-capstone_perception",level:4},{value:"B. Navigation Module (<code>capstone_navigation</code>)",id:"b-navigation-module-capstone_navigation",level:4},{value:"C. Manipulation Module (<code>capstone_manipulation</code>)",id:"c-manipulation-module-capstone_manipulation",level:4},{value:"D. VLA Integration Module (<code>capstone_vla_integration</code>)",id:"d-vla-integration-module-capstone_vla_integration",level:4},{value:"E. Voice Handler Module (<code>capstone_voice_handler</code>)",id:"e-voice-handler-module-capstone_voice_handler",level:4},{value:"F. Task Planner Module (<code>capstone_task_planner</code>)",id:"f-task-planner-module-capstone_task_planner",level:4},{value:"Step 3: Implement World Model (<code>capstone_world_model</code>)",id:"step-3-implement-world-model-capstone_world_model",level:3},{value:"Step 4: Create a Central Launch File",id:"step-4-create-a-central-launch-file",level:3},{value:"Step 5: Test and Refine",id:"step-5-test-and-refine",level:3},{value:"Butler Capabilities Checklist (SC-006)",id:"butler-capabilities-checklist-sc-006",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-13-complete-capstone-autonomous-butler-project",children:"Chapter 13: Complete Capstone Autonomous Butler Project"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:["Throughout this book, you've acquired foundational knowledge in ROS 2, URDF modeling, Isaac Sim, VLA models, voice command processing, and sim-to-real transfer. Now, it's time to bring all these pieces together in an ambitious capstone project: building an ",(0,i.jsx)(n.strong,{children:"Autonomous Humanoid Butler"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"This chapter guides you through the process of integrating all the technologies and concepts learned previously to create a fully functional, voice-controlled humanoid robot capable of executing multi-step household tasks. This project will serve as a comprehensive demonstration of Physical AI in action."}),"\n",(0,i.jsxs)(n.admonition,{title:"Learning Objectives",type:"tip",children:[(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design and implement a high-level architecture for an autonomous humanoid butler."}),"\n",(0,i.jsx)(n.li,{children:"Integrate ROS 2 nodes for perception, navigation, manipulation, and VLA inference."}),"\n",(0,i.jsx)(n.li,{children:"Develop a task planner to break down complex natural language commands into robot action sequences."}),"\n",(0,i.jsx)(n.li,{children:"Configure and launch an entire Physical AI system."}),"\n",(0,i.jsx)(n.li,{children:"Evaluate the performance and capabilities of your integrated system."}),"\n"]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"project-overview-the-autonomous-butler",children:"Project Overview: The Autonomous Butler"}),"\n",(0,i.jsx)(n.p,{children:"Our capstone project is an autonomous humanoid robot butler designed to perform basic household assistance tasks. Imagine a robot that can understand your natural language commands, perceive its environment, navigate through your home, manipulate objects, and execute sequences of actions to achieve a goal."}),"\n",(0,i.jsx)(n.h3,{id:"core-capabilities-of-the-butler",children:"Core Capabilities of the Butler"}),"\n",(0,i.jsx)(n.p,{children:"The ideal autonomous butler should demonstrate the following 7 capabilities:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice Command Understanding"}),': Accurately interpret natural language commands (e.g., "Clean up the living room").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment Mapping & Localization"}),": Build and maintain a map of the environment and constantly know its position within it."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Recognition & Tracking"}),': Identify and track household objects (e.g., "red cup," "book," "remote control").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Autonomous Navigation"}),": Plan and execute collision-free paths to various locations in the home."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dexterous Manipulation"}),": Grasp, pick, place, and transfer a variety of common household objects."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Planning & Execution"}),": Break down high-level commands into a sequence of low-level actions, and execute them reliably."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Handling & Recovery"}),": Detect when a task fails and attempt recovery or report failure intelligently."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"high-level-system-architecture",children:"High-Level System Architecture"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph TD\n    A[User Voice Command] --\x3e B[Whisper: Speech-to-Text]\n    B --\x3e C[LLM: Intent Parser<br/>(Function Calling)]\n    C --\x3e D[Task Planner<br/>(Goal to Action Sequence)]\n\n    E[Robot Camera/Sensors] --\x3e F[Perception Stack<br/>(Object Detection, SLAM)]\n    F --\x3e G[World Model<br/>(Object States, Map)]\n\n    D --\x3e H[Navigation Controller]\n    D --\x3e I[Manipulation Controller]\n    D --\x3e J[VLA Inference Engine]\n\n    G --\x3e H\n    G --\x3e I\n    G --\x3e J\n\n    H --\x3e K[ROS 2 Base Controller]\n    I --\x3e K\n    J --\x3e K\n\n    K --\x3e L[Robot Actuators]\n        L --\x3e M[Physical World]\n        M --\x3e E"}),"\n",(0,i.jsx)(n.p,{children:"This architecture diagram illustrates the flow from a user's voice command to physical robot actions, integrating the various components you've learned to build a cohesive system."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"component-integration-diagram",children:"Component Integration Diagram"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph LR\n    subgraph Voice Command Processing\n        VC[Whisper] -- Text --\x3e LP[LLM Parser]\n    end\n\n    subgraph Robot Core\n        P[Perception] -- Object Detections & Pose --\x3e WM[World Model]\n        WM -- Map & Object Poses --\x3e N[Navigation]\n        WM -- Object Poses & Robot Pose --\x3e M[Manipulation]\n        WM -- Object Poses & Robot Pose --\x3e VLA[VLA Integration]\n    end\n\n    subgraph Control\n        N -- Cmd Vel --\x3e BC[Base Controller]\n        M -- Joint Cmds --\x3e BC\n        VLA -- Actions --\x3e BC\n    end\n\n    subgraph Task Management\n        LP -- Intent --\x3e TP[Task Planner]\n        TP -- Nav Goals --\x3e N\n        TP -- Manip Goals --\x3e M\n        TP -- VLA Goals --\x3e VLA\n    end\n\n    TP -- Robot Actions --\x3e BC\n    BC -- Actuators --\x3e RW[Real World]\n    RW -- Sensors --\x3e P"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"data-flow-for-pick-up-object-task",children:'Data Flow for "Pick Up Object" Task'}),"\n",(0,i.jsx)(n.mermaid,{value:'sequenceDiagram\n    actor User\n    User->>Voice Handler: "Pick up the red ball"\n    Voice Handler->>Task Planner: {action: "pick", object: "red ball"}\n    Task Planner->>World Model: Query: "Location of red ball?"\n    World Model->>Perception: Request Object Detection\n    Perception->>Robot Sensors: Capture Image/Depth\n    Robot Sensors->>Perception: Image/Depth Data\n    Perception->>World Model: Report: "Red ball at [x,y,z]"\n    World Model->>Task Planner: Response: "Red ball at [x,y,z]"\n    Task Planner->>Navigation: Command: "Go to [x,y,z]"\n    Navigation->>Base Controller: Cmd_Vel\n    Base Controller->>Robot: Move\n    Robot->>World Model: Update Robot Pose\n    Navigation--\x3e>Task Planner: Status: "Arrived at [x,y,z]"\n    Task Planner->>Manipulation: Command: "Grasp red ball"\n    Manipulation->>Robot: Joint Cmds (Arm/Gripper)\n    Robot->>World Model: Update Robot Pose\n    Manipulation--\x3e>Task Planner: Status: "Grasped red ball"\n        Task Planner->>User: "Red ball picked up."'}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"task-planner-state-machine",children:"Task Planner State Machine"}),"\n",(0,i.jsx)(n.mermaid,{value:"stateDiagram\n    direction LR\n    [*] --\x3e Idle: System Start\n    Idle --\x3e Listening: User Command\n    Listening --\x3e Parsing: Voice Input\n    Parsing --\x3e Planning: Intent Identified\n    Planning --\x3e Executing: Task Sequence\n    Executing --\x3e Success: Task Complete\n    Executing --\x3e Error: Failure Detected\n    Error --\x3e Recovering: Attempt Recovery\n    Recovering --\x3e Planning: Recovery Successful\n    Recovering --\x3e [*]: Recovery Failed\n    Executing --\x3e Pause: User Pause\n    Pause --\x3e Executing: User Resume\n    Pause --\x3e [*]: User Stop"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"error-recovery-workflow",children:"Error Recovery Workflow"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph TD\n    A[Task Execution Step] --\x3e B{Step Successful?}\n    B -- Yes --\x3e C[Next Task Step]\n    B -- No --\x3e D{Error Type?}\n    D -- Object Not Found --\x3e E[Search Area]\n    D -- Path Blocked --\x3e F[Re-plan Path]\n    D -- Manipulation Failed --\x3e G[Retry Grasp]\n    E --\x3e A\n    F --\x3e A\n    G --\x3e A\n    D -- Other Error --\x3e H[Report to User/Log]\n    H --\x3e I[End Task]"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"high-level-task-decomposition",children:"High-Level Task Decomposition"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph TD\n    A[User Command: "Clean up living room"] --\x3e B{Task Planner}\n    B --\x3e C[Sub-Task 1: Navigate to kitchen]\n    B --\x3e D[Sub-Task 2: Find dirty dishes]\n    B --\x3e E[Sub-Task 3: Grasp dish]\n    B --\x3e F[Sub-Task 4: Place dish in dishwasher]\n    B --\x3e G[Sub-Task 5: Navigate to living room]\n    B --\x3e H[Sub-Task 6: Find remote]\n    B --\x3e I[Sub-Task 7: Pick up remote]\n    B --\x3e J[Sub-Task 8: Place remote on table]\n    C --\x3e B\n    D --\x3e B\n    E --\x3e B\n    F --\x3e B\n    G --\x3e B\n    H --\x3e B\n    I --\x3e B\n    J --\x3e B'}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"integration-challenges-and-solutionsbuilding-an-integrated-system-introduces-new-complexities-beyond-individual-component-development",children:"Integration Challenges and SolutionsBuilding an integrated system introduces new complexities beyond individual component development."}),"\n",(0,i.jsx)(n.h3,{id:"1-inter-process-communication-ipc",children:"1. Inter-Process Communication (IPC)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Challenge"}),": How do different ROS 2 nodes (Perception, Navigation, Manipulation, VLA) communicate efficiently and reliably?"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Topics"}),": For streaming data (e.g., camera images, joint states, odometry)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Services"}),': For request/response interactions (e.g., "get object pose," "plan path to kitchen").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Actions"}),': For long-running, cancellable tasks with feedback (e.g., "navigate to target," "grasp object").']}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-state-management",children:"2. State Management"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Challenge"}),": How does the system maintain a consistent understanding of the world and the robot's state across multiple nodes?"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"World Model"}),": A dedicated node that aggregates sensor data, tracks object locations, and maintains a probabilistic map of the environment."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"TF Tree"}),": Crucial for tracking spatial relationships between all robot links, sensors, and objects."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Parameters"}),": For configuration parameters that need to be shared or dynamically changed."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-error-handling-and-robustness",children:"3. Error Handling and Robustness"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Challenge"}),": Real-world robotics is prone to failures. How does the butler handle unexpected events (e.g., object not found, path blocked)?"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Exception Handling"}),": Implement robust try-catch blocks in each node."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Machine"}),": Design the task planner as a state machine with explicit states for success, failure, and recovery attempts."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback Mechanisms"}),": Use ROS 2 Action feedback to monitor task progress and detect failures early."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fallback Strategies"}),": Define alternative actions if a primary task fails (e.g., if object not found, try searching a wider area)."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"high-level-task-decomposition-1",children:"High-Level Task Decomposition"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph TD\n    A[User Command: "Clean up living room"] --\x3e B{Task Planner}\n    B --\x3e C[Sub-Task 1: Navigate to kitchen]\n    B --\x3e D[Sub-Task 2: Find dirty dishes]\n    B --\x3e E[Sub-Task 3: Grasp dish]\n    B --\x3e F[Sub-Task 4: Place dish in dishwasher]\n    B --\x3e G[Sub-Task 5: Navigate to living room]\n    B --\x3e H[Sub-Task 6: Find remote]\n    B --\x3e I[Sub-Task 7: Pick up remote]\n    B --\x3e J[Sub-Task 8: Place remote on table]\n    C --\x3e B\n    D --\x3e B\n    E --\x3e B\n    F --\x3e B\n    G --\x3e B\n    H --\x3e B\n    I --\x3e B\n    J --\x3e B'}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"building-the-butler-step-by-step-guide",children:"Building the Butler: Step-by-Step Guide"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-initialize-capstone-repository",children:"Step 1: Initialize Capstone Repository"}),"\n",(0,i.jsx)(n.p,{children:"You will start with a dedicated ROS 2 workspace for the capstone project."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/capstone_ws/src\ncd ~/capstone_ws/src\n# Create sub-packages for each major component\nros2 pkg create --build-type ament_python capstone_perception\nros2 pkg create --build-type ament_python capstone_navigation\nros2 pkg create --build-type ament_python capstone_manipulation\nros2 pkg create --build-type ament_python capstone_vla_integration\nros2 pkg create --build-type ament_python capstone_voice_handler\nros2 pkg create --build-type ament_python capstone_task_planner\ncd ~/capstone_ws\ncolcon build\nsource install/setup.bash\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-implement-core-modules",children:"Step 2: Implement Core Modules"}),"\n",(0,i.jsx)(n.p,{children:"Each module will be developed as a ROS 2 package, leveraging the code examples and concepts from previous chapters."}),"\n",(0,i.jsxs)(n.h4,{id:"a-perception-module-capstone_perception",children:["A. Perception Module (",(0,i.jsx)(n.code,{children:"capstone_perception"}),")"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Functionality"}),": Object detection (e.g., YOLO), depth estimation, basic visual SLAM or AMCL for localization."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inputs"}),": Camera images, depth data, IMU."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Outputs"}),": Object poses, robot pose estimates."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Key Chapters"}),": Chapter 7 (Perception Stack)."]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"b-navigation-module-capstone_navigation",children:["B. Navigation Module (",(0,i.jsx)(n.code,{children:"capstone_navigation"}),")"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Functionality"}),": Map management (SLAM), global and local path planning, obstacle avoidance."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inputs"}),": Lidar scans, odometry, target pose."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Outputs"}),": Velocity commands to base controller."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Key Chapters"}),": Chapter 8 (Bipedal Locomotion - adapted for humanoid navigation)."]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"c-manipulation-module-capstone_manipulation",children:["C. Manipulation Module (",(0,i.jsx)(n.code,{children:"capstone_manipulation"}),")"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Functionality"}),": Inverse Kinematics (IK), motion planning for arm, gripper control, grasp planning."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inputs"}),": Target object pose, robot current state."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Outputs"}),": Joint commands for arm and gripper."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Key Chapters"}),": Chapter 9 (Dexterous Manipulation)."]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"d-vla-integration-module-capstone_vla_integration",children:["D. VLA Integration Module (",(0,i.jsx)(n.code,{children:"capstone_vla_integration"}),")"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Functionality"}),": Load and run VLA model inference (e.g., OpenVLA), translate VLA output to robot actions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inputs"}),": Camera images, processed natural language commands."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Outputs"}),": Low-level actions for manipulation or navigation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Key Chapters"}),": Chapter 10 (VLA Models)."]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"e-voice-handler-module-capstone_voice_handler",children:["E. Voice Handler Module (",(0,i.jsx)(n.code,{children:"capstone_voice_handler"}),")"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Functionality"}),": Speech-to-text (Whisper), natural language understanding (LLM)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inputs"}),": Raw audio from microphone."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Outputs"}),": Structured intent (e.g., JSON with action, object, location)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Key Chapters"}),": Chapter 11 (Voice-to-Action Pipeline)."]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"f-task-planner-module-capstone_task_planner",children:["F. Task Planner Module (",(0,i.jsx)(n.code,{children:"capstone_task_planner"}),")"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Functionality"}),": High-level task decomposition, state machine management, coordination of other modules."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inputs"}),": Structured intent from voice handler, world model updates."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Outputs"}),": Sequence of actions (navigation goals, manipulation commands) for robot."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Key Chapters"}),": Integrates concepts from all previous chapters."]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"step-3-implement-world-model-capstone_world_model",children:["Step 3: Implement World Model (",(0,i.jsx)(n.code,{children:"capstone_world_model"}),")"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Functionality"}),": A central repository for the robot's understanding of its environment. Tracks known objects, their properties (color, type, location), and the robot's current pose."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inputs"}),": Object detection results from Perception, robot odometry, user commands."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Outputs"}),": Provides world state queries to Navigation, Manipulation, and Task Planner."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-4-create-a-central-launch-file",children:"Step 4: Create a Central Launch File"}),"\n",(0,i.jsx)(n.p,{children:"A single launch file will bring up the entire butler system."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# ~/capstone_ws/src/capstone_bringup/launch/butler_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='capstone_voice_handler',\n            executable='voice_handler_node',\n            name='voice_handler',\n            output='screen',\n        ),\n        Node(\n            package='capstone_perception',\n            executable='perception_node',\n            name='perception',\n            output='screen',\n        ),\n        Node(\n            package='capstone_world_model',\n            executable='world_model_node',\n            name='world_model',\n            output='screen',\n        ),\n        Node(\n            package='capstone_task_planner',\n            executable='task_planner_node',\n            name='task_planner',\n            output='screen',\n        ),\n        Node(\n            package='capstone_navigation',\n            executable='navigation_node',\n            name='navigation',\n            output='screen',\n        ),\n        Node(\n            package='capstone_manipulation',\n            executable='manipulation_node',\n            name='manipulation',\n            output='screen',\n        ),\n        Node(\n            package='capstone_vla_integration',\n            executable='vla_integration_node',\n            name='vla_integration',\n            output='screen',\n        ),\n        # Add robot_state_publisher and joint_state_publisher if using a URDF\n        # Add rviz2 for visualization\n    ])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-5-test-and-refine",children:"Step 5: Test and Refine"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Modular Testing"}),": Test each component individually before integrating."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sub-System Testing"}),": Test pairs or small groups of integrated components (e.g., Perception + World Model, Voice Handler + Task Planner)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"End-to-End Testing"}),": Test the full system with a comprehensive set of voice commands and household tasks."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance Metrics"}),": Focus on success rate, latency (voice command to action), and robustness to variations."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"butler-capabilities-checklist-sc-006",children:"Butler Capabilities Checklist (SC-006)"}),"\n",(0,i.jsx)(n.p,{children:"This checklist helps you track the 7 core capabilities of your autonomous butler project and validates against Success Criteria SC-006 (5 out of 7 capabilities demonstrated)."}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,i.jsx)(n.strong,{children:"Voice Command Understanding"}),": The robot correctly interprets at least 80% of spoken commands."]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,i.jsx)(n.strong,{children:"Environment Mapping & Localization"}),": The robot can build a map of a new room and accurately localize itself within it."]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,i.jsx)(n.strong,{children:"Object Recognition & Tracking"}),": The robot can identify and track at least 10 common household objects (e.g., cup, book, phone)."]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,i.jsx)(n.strong,{children:"Autonomous Navigation"}),": The robot can navigate to a specified room/location, avoiding obstacles, at least 80% of the time."]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,i.jsx)(n.strong,{children:"Dexterous Manipulation"}),": The robot can grasp and place at least 5 different household objects from various surfaces."]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,i.jsx)(n.strong,{children:"Task Planning & Execution"}),': The robot can execute 3-step sequential tasks (e.g., "Go to kitchen, pick up cup, bring to living room").']}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,i.jsx)(n.strong,{children:"Error Handling & Recovery"}),": The robot can detect simple task failures (e.g., object not found) and attempt a predefined recovery or report failure."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"The Capstone Autonomous Butler Project is the culmination of your journey through Physical AI. It challenges you to integrate diverse technologies and solve real-world problems. While ambitious, successfully implementing even a subset of the capabilities will provide invaluable experience and a tangible demonstration of your expertise. Remember to approach this project iteratively, testing each component and sub-system before attempting full integration."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.admonition,{title:"Next Steps",type:"note",children:[(0,i.jsx)(n.p,{children:"With the Capstone Project outlined, you are ready to explore the appendices for further resources:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Appendix A: Lab Build Guides"}),": For setting up your physical AI lab."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Appendix B: Troubleshooting Bible"}),": For debugging common issues."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Appendix C: Future Roadmap"}),": To understand where Physical AI is headed next."]}),"\n"]})]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var o=t(6540);const i={},s=o.createContext(i);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);