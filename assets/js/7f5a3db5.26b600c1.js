"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[4536],{7616:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"perception-stack/perception-stack","title":"Chapter 07 - Perception Stack","description":"Master camera sensors, depth perception, VSLAM, and object detection for humanoid robot perception systems","source":"@site/docs/perception-stack/index.mdx","sourceDirName":"perception-stack","slug":"/perception-stack/","permalink":"/HumanoidRoboticsBook/docs/perception-stack/","draft":false,"unlisted":false,"editUrl":"https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook/tree/main/docs/perception-stack/index.mdx","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"id":"perception-stack","title":"Chapter 07 - Perception Stack","sidebar_position":7,"description":"Master camera sensors, depth perception, VSLAM, and object detection for humanoid robot perception systems","keywords":["perception","rgb-d-cameras","depth-sensing","vslam","object-detection","yolov8","sam2","sensor-fusion"]},"sidebar":"tutorialSidebar","previous":{"title":"06. NVIDIA Isaac Platform","permalink":"/HumanoidRoboticsBook/docs/isaac-platform/"},"next":{"title":"08. Bipedal Locomotion","permalink":"/HumanoidRoboticsBook/docs/bipedal-locomotion/"}}');var r=s(4848),t=s(8453);const l={id:"perception-stack",title:"Chapter 07 - Perception Stack",sidebar_position:7,description:"Master camera sensors, depth perception, VSLAM, and object detection for humanoid robot perception systems",keywords:["perception","rgb-d-cameras","depth-sensing","vslam","object-detection","yolov8","sam2","sensor-fusion"]},o="Chapter 07: Perception Stack for Humanoid Robots",c={},a=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Why Perception Matters",id:"why-perception-matters",level:2},{value:"Perception Pipeline Architecture",id:"perception-pipeline-architecture",level:2},{value:"RGB-D Camera Setup",id:"rgb-d-camera-setup",level:2},{value:"Supported Cameras (2026 Market)",id:"supported-cameras-2026-market",level:3},{value:"RealSense D455 ROS 2 Installation",id:"realsense-d455-ros-2-installation",level:3},{value:"Camera Calibration",id:"camera-calibration",level:3},{value:"Depth Processing and Point Clouds",id:"depth-processing-and-point-clouds",level:2},{value:"Depth Image to Point Cloud Conversion",id:"depth-image-to-point-cloud-conversion",level:3},{value:"Point Cloud Generation",id:"point-cloud-generation",level:3},{value:"Visualizing Point Clouds in RViz2",id:"visualizing-point-clouds-in-rviz2",level:3},{value:"Visual SLAM (VSLAM)",id:"visual-slam-vslam",level:2},{value:"What is VSLAM?",id:"what-is-vslam",level:3},{value:"VSLAM Pipeline",id:"vslam-pipeline",level:3},{value:"ORB-SLAM3 Setup (Recommended)",id:"orb-slam3-setup-recommended",level:3},{value:"VSLAM Alternatives",id:"vslam-alternatives",level:3},{value:"Object Detection and Segmentation",id:"object-detection-and-segmentation",level:2},{value:"YOLOv8: Real-Time Object Detection",id:"yolov8-real-time-object-detection",level:3},{value:"YOLOv8 ROS 2 Integration",id:"yolov8-ros-2-integration",level:3},{value:"SAM 2: Segment Anything Model",id:"sam-2-segment-anything-model",level:3},{value:"Grounding DINO: Open-Vocabulary Detection",id:"grounding-dino-open-vocabulary-detection",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Why Fuse Multiple Sensors?",id:"why-fuse-multiple-sensors",level:3},{value:"Extended Kalman Filter (EKF) for Visual-Inertial Odometry",id:"extended-kalman-filter-ekf-for-visual-inertial-odometry",level:3},{value:"Robot Localization (ROS 2 Package)",id:"robot-localization-ros-2-package",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Target Latencies for Real-Time Perception",id:"target-latencies-for-real-time-perception",level:3},{value:"Optimization Strategies",id:"optimization-strategies",level:3},{value:"Debugging Perception Systems",id:"debugging-perception-systems",level:2},{value:"Common Failure Modes",id:"common-failure-modes",level:3},{value:"Integration Example: Perception for Manipulation",id:"integration-example-perception-for-manipulation",level:2},{value:"End-to-End Workflow",id:"end-to-end-workflow",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Research Papers",id:"research-papers",level:3},{value:"Open-Source Repositories",id:"open-source-repositories",level:3},{value:"ROS 2 Documentation",id:"ros-2-documentation",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-07-perception-stack-for-humanoid-robots",children:"Chapter 07: Perception Stack for Humanoid Robots"})}),"\n",(0,r.jsx)(n.admonition,{title:"Chapter Overview",type:"info",children:(0,r.jsx)(n.p,{children:'Build the complete perception system enabling robots to "see" and understand their environment. Learn camera configuration, depth sensing, Visual SLAM (VSLAM), object detection/segmentation, and integrate state-of-the-art 2025-2026 perception models achieving >85% detection accuracy in complex scenes.'})}),"\n",(0,r.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you'll be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Set up RGB-D cameras (Intel RealSense D455, ZED 2i) on ROS 2"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Process depth data and generate point clouds for 3D perception"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Implement Visual SLAM (ORB-SLAM3, VINS-Fusion) for localization"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Deploy YOLOv8, SAM 2, and Grounding DINO for object detection"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Fuse multi-sensor data (camera + LiDAR + IMU) for robust perception"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Achieve <100ms perception latency for real-time control"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Optimize perception pipelines for production deployment"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Debug common perception failures (lighting, occlusion, calibration)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before starting this chapter, you should:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 Complete ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"../ros2-fundamentals",children:"Chapter 03: ROS 2 Fundamentals"})})," (topics, services, tf2)"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 Complete ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"../simulation-ecosystem",children:"Chapter 05: Simulation Ecosystem"})})," (Isaac Sim sensors)"]}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Have Python 3.10+, ROS 2 Iron, PyTorch 2.1+ installed"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Have RTX GPU with 8+ GB VRAM (12 GB recommended for YOLOv8 + SAM 2)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Understand basic computer vision concepts (image processing, camera models)"}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{title:"Hardware Requirements",type:"tip",children:(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Minimum:"})," RTX 4060 Ti (16 GB) for perception inference\n",(0,r.jsx)(n.strong,{children:"Recommended:"})," RTX 4070 Ti (12 GB) for multi-model pipelines\n",(0,r.jsx)(n.strong,{children:"Professional:"})," RTX 4080/4090 (16-24 GB) for real-time perception + VLA integration"]})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"why-perception-matters",children:"Why Perception Matters"}),"\n",(0,r.jsxs)(n.p,{children:["Humanoid robots operate in ",(0,r.jsx)(n.strong,{children:"unstructured, dynamic environments"})," - homes, offices, warehouses - where:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Objects move and change position"}),"\n",(0,r.jsx)(n.li,{children:"Lighting conditions vary (shadows, reflections, darkness)"}),"\n",(0,r.jsx)(n.li,{children:"Obstacles appear unexpectedly (pets, humans, furniture)"}),"\n",(0,r.jsx)(n.li,{children:"No pre-mapped environments available"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Perception systems enable robots to:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Localize"})," themselves in 3D space (SLAM)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Detect"})," objects, people, obstacles (object detection)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Segment"})," scenes for manipulation planning (instance segmentation)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Track"})," moving objects over time (multi-object tracking)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reconstruct"})," 3D geometry for navigation (point clouds, meshes)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"perception-pipeline-architecture",children:"Perception Pipeline Architecture"}),"\n",(0,r.jsx)(n.p,{children:"A production perception stack has 5 core components:"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TB\n    subgraph "Stage 1: Sensor Acquisition"\n        S1[RGB-D Camera<br/>RealSense D455]\n        S2[IMU<br/>9-DoF]\n        S3[LiDAR<br/>Optional]\n    end\n\n    subgraph "Stage 2: Preprocessing"\n        P1[Image Rectification]\n        P2[Depth Alignment]\n        P3[Noise Filtering]\n    end\n\n    subgraph "Stage 3: Feature Extraction"\n        F1[Visual Features<br/>ORB, SIFT]\n        F2[Depth Features<br/>Point Cloud]\n        F3[Semantic Features<br/>DINOv2]\n    end\n\n    subgraph "Stage 4: Perception Modules"\n        M1[VSLAM<br/>ORB-SLAM3]\n        M2[Object Detection<br/>YOLOv8]\n        M3[Segmentation<br/>SAM 2]\n        M4[Tracking<br/>ByteTrack]\n    end\n\n    subgraph "Stage 5: Sensor Fusion"\n        SF[Extended Kalman Filter<br/>Camera + IMU + LiDAR]\n    end\n\n    subgraph "Output"\n        O1[Robot Pose<br/>6-DoF]\n        O2[Object List<br/>Bounding Boxes]\n        O3[3D Map<br/>Point Cloud]\n    end\n\n    S1 --\x3e P1\n    S2 --\x3e SF\n    S3 --\x3e SF\n    P1 --\x3e P2\n    P2 --\x3e P3\n    P3 --\x3e F1\n    P3 --\x3e F2\n    F1 --\x3e M1\n    F2 --\x3e M1\n    P3 --\x3e F3\n    F3 --\x3e M2\n    F3 --\x3e M3\n    M2 --\x3e M4\n    M1 --\x3e SF\n    M2 --\x3e O2\n    M3 --\x3e O2\n    M4 --\x3e O2\n    SF --\x3e O1\n    F2 --\x3e O3\n\n    style S1 fill:#00adef\n    style M1 fill:#76b900\n    style M2 fill:#ffcc00\n    style SF fill:#9966ff'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Design Principles:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Modularity"}),": Each component can be swapped independently"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Redundancy"}),": Multiple sensors provide robustness"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time"}),": <100ms latency from sensor to output"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scalability"}),": Add more sensors/models without rewriting pipeline"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"rgb-d-camera-setup",children:"RGB-D Camera Setup"}),"\n",(0,r.jsx)(n.h3,{id:"supported-cameras-2026-market",children:"Supported Cameras (2026 Market)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Camera"}),(0,r.jsx)(n.th,{children:"Resolution"}),(0,r.jsx)(n.th,{children:"FPS"}),(0,r.jsx)(n.th,{children:"Depth Range"}),(0,r.jsx)(n.th,{children:"ROS 2 Support"}),(0,r.jsx)(n.th,{children:"Price"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Intel RealSense D455"})}),(0,r.jsx)(n.td,{children:"1280x720"}),(0,r.jsx)(n.td,{children:"90"}),(0,r.jsx)(n.td,{children:"0.6-6m"}),(0,r.jsx)(n.td,{children:"Native"}),(0,r.jsx)(n.td,{children:"$329"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Stereolabs ZED 2i"})}),(0,r.jsx)(n.td,{children:"4416x1242"}),(0,r.jsx)(n.td,{children:"120"}),(0,r.jsx)(n.td,{children:"0.2-20m"}),(0,r.jsx)(n.td,{children:"Native"}),(0,r.jsx)(n.td,{children:"$449"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Orbbec Astra Pro"})}),(0,r.jsx)(n.td,{children:"640x480"}),(0,r.jsx)(n.td,{children:"30"}),(0,r.jsx)(n.td,{children:"0.6-8m"}),(0,r.jsx)(n.td,{children:"Community"}),(0,r.jsx)(n.td,{children:"$149"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Microsoft Azure Kinect DK"})}),(0,r.jsx)(n.td,{children:"3840x2160"}),(0,r.jsx)(n.td,{children:"30"}),(0,r.jsx)(n.td,{children:"0.5-5.46m"}),(0,r.jsx)(n.td,{children:"Community"}),(0,r.jsx)(n.td,{children:"$399 (discontinued)"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommendation for humanoids:"})," Intel RealSense D455 (best price/performance, excellent ROS 2 support)"]}),"\n",(0,r.jsx)(n.h3,{id:"realsense-d455-ros-2-installation",children:"RealSense D455 ROS 2 Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Install RealSense SDK 2.0\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key F6E65AC044F831AC80A06380C8B3A55A6F3EFCDE\nsudo add-apt-repository "deb https://librealsense.intel.com/Debian/apt-repo $(lsb_release -cs) main"\nsudo apt update\nsudo apt install librealsense2-dkms librealsense2-utils librealsense2-dev\n\n# Test camera\nrealsense-viewer\n\n# Install ROS 2 wrapper\nsudo apt install ros-iron-realsense2-camera ros-iron-realsense2-description\n\n# Launch camera node\nros2 launch realsense2_camera rs_launch.py \\\n    enable_color:=true \\\n    enable_depth:=true \\\n    depth_module.profile:=1280x720x30 \\\n    rgb_camera.profile:=1280x720x30\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Verify camera topics:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 topic list | grep camera\n# Expected output:\n# /camera/color/image_raw\n# /camera/depth/image_rect_raw\n# /camera/aligned_depth_to_color/image_raw\n# /camera/color/camera_info\n"})}),"\n",(0,r.jsx)(n.h3,{id:"camera-calibration",children:"Camera Calibration"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why calibration matters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Corrects lens distortion (barrel, pincushion)"}),"\n",(0,r.jsx)(n.li,{children:"Aligns RGB and depth images"}),"\n",(0,r.jsx)(n.li,{children:"Improves depth accuracy by 10-20%"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install calibration tools\nsudo apt install ros-iron-camera-calibration\n\n# Calibrate RGB camera (print 8x6 checkerboard, 25mm squares)\nros2 run camera_calibration cameracalibrator \\\n    --size 8x6 \\\n    --square 0.025 \\\n    image:=/camera/color/image_raw \\\n    camera:=/camera/color\n\n# Calibrate depth-to-RGB alignment\nros2 run camera_calibration cameracalibrator \\\n    --size 8x6 \\\n    --square 0.025 \\\n    image:=/camera/aligned_depth_to_color/image_raw \\\n    camera:=/camera/aligned_depth_to_color\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Save calibration file:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# ~/.ros/camera_info/camera.yaml\nimage_width: 1280\nimage_height: 720\ncamera_matrix:\n  rows: 3\n  cols: 3\n  data: [615.3, 0, 640.2, 0, 615.7, 360.1, 0, 0, 1]\ndistortion_coefficients:\n  rows: 1\n  cols: 5\n  data: [-0.041, 0.052, -0.0003, 0.0002, 0.0]\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"depth-processing-and-point-clouds",children:"Depth Processing and Point Clouds"}),"\n",(0,r.jsx)(n.h3,{id:"depth-image-to-point-cloud-conversion",children:"Depth Image to Point Cloud Conversion"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Theory:"})," Each depth pixel D(u,v) projects to 3D point P(X,Y,Z):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"X = (u - cx) * Z / fx\nY = (v - cy) * Z / fy\nZ = D(u,v)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Where ",(0,r.jsx)(n.code,{children:"(cx, cy)"})," is principal point and ",(0,r.jsx)(n.code,{children:"(fx, fy)"})," is focal length from calibration."]}),"\n",(0,r.jsx)(n.h3,{id:"point-cloud-generation",children:"Point Cloud Generation"}),"\n",(0,r.jsxs)(n.p,{children:["See ",(0,r.jsx)(n.code,{children:"code-examples/perception/depth_processor.py"})," for complete implementation:"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key steps:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Align depth to RGB camera frame"}),"\n",(0,r.jsx)(n.li,{children:"Convert depth image to XYZ coordinates"}),"\n",(0,r.jsx)(n.li,{children:"Filter invalid points (Z=0, Z>max_range)"}),"\n",(0,r.jsx)(n.li,{children:"Downsample with voxel grid (reduce 1M points \u2192 50K)"}),"\n",(0,r.jsx)(n.li,{children:"Remove outliers (statistical filtering)"}),"\n",(0,r.jsxs)(n.li,{children:["Publish as ",(0,r.jsx)(n.code,{children:"sensor_msgs/PointCloud2"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Expected performance:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Input:"})," 1280x720 depth image (921,600 pixels)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output:"})," 50,000-100,000 3D points"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency:"})," 20-30ms on RTX 4070 Ti"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Frequency:"})," 30 Hz"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"visualizing-point-clouds-in-rviz2",children:"Visualizing Point Clouds in RViz2"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Launch RViz2\nrviz2\n\n# Add PointCloud2 display\n# Topic: /camera/pointcloud\n# Fixed Frame: camera_link\n# Color Transformer: RGB8\n\n# Optional: Save point cloud to PCD file\nros2 run pcl_ros pointcloud_to_pcd input:=/camera/pointcloud\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"visual-slam-vslam",children:"Visual SLAM (VSLAM)"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-vslam",children:"What is VSLAM?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simultaneous Localization and Mapping (SLAM)"})," solves two problems simultaneously:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Localization:"})," Where is the robot? (6-DoF pose: x, y, z, roll, pitch, yaw)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mapping:"})," What does the environment look like? (3D point cloud map)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM"})," uses cameras as the primary sensor (vs. LiDAR SLAM)."]}),"\n",(0,r.jsx)(n.h3,{id:"vslam-pipeline",children:"VSLAM Pipeline"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart LR\n    subgraph "Frontend: Visual Odometry"\n        F1[Extract Features<br/>ORB, SIFT]\n        F2[Match Features<br/>Frame-to-Frame]\n        F3[Estimate Motion<br/>RANSAC + PnP]\n    end\n\n    subgraph "Backend: Optimization"\n        B1[Loop Closure<br/>Detect Revisits]\n        B2[Bundle Adjustment<br/>g2o, Ceres]\n        B3[Pose Graph<br/>Optimize Trajectory]\n    end\n\n    subgraph "Output"\n        O1[Robot Pose<br/>TF Camera \u2192 Map]\n        O2[Sparse Map<br/>3D Landmarks]\n    end\n\n    F1 --\x3e F2\n    F2 --\x3e F3\n    F3 --\x3e B1\n    B1 --\x3e B2\n    B2 --\x3e B3\n    B3 --\x3e O1\n    B2 --\x3e O2\n\n    style F1 fill:#00adef\n    style B1 fill:#76b900\n    style O1 fill:#ffcc00'}),"\n",(0,r.jsx)(n.h3,{id:"orb-slam3-setup-recommended",children:"ORB-SLAM3 Setup (Recommended)"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why ORB-SLAM3?"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"State-of-the-art accuracy (ATE <0.1m on TUM dataset)"}),"\n",(0,r.jsx)(n.li,{children:"Supports monocular, stereo, RGB-D, multi-camera"}),"\n",(0,r.jsx)(n.li,{children:"Active loop closure detection"}),"\n",(0,r.jsx)(n.li,{children:"Real-time performance (30+ FPS on laptop CPU)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Installation:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Dependencies\nsudo apt install libeigen3-dev libopencv-dev libpangolin-dev\n\n# Clone ORB-SLAM3\ncd ~/ros2_ws/src\ngit clone https://github.com/UZ-SLAMLab/ORB_SLAM3.git\ncd ORB_SLAM3\n\n# Build (takes 10-15 minutes)\nchmod +x build.sh\n./build.sh\n\n# Build ROS 2 wrapper\ncd ~/ros2_ws/src\ngit clone https://github.com/zang09/ORB-SLAM3-ROS2.git\ncd ~/ros2_ws\ncolcon build --packages-select orbslam3\n\n# Source workspace\nsource install/setup.bash\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Run ORB-SLAM3 with RealSense:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch camera\nros2 launch realsense2_camera rs_launch.py enable_depth:=true\n\n# Terminal 2: Run ORB-SLAM3\nros2 run orbslam3 rgbd \\\n    ~/ros2_ws/src/ORB_SLAM3/Vocabulary/ORBvoc.txt \\\n    ~/ros2_ws/src/ORB_SLAM3/Examples/RGB-D/RealSense_D455.yaml \\\n    /camera/color/image_raw:=/camera/color/image_raw \\\n    /camera/depth/image_raw:=/camera/aligned_depth_to_color/image_raw\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Check SLAM output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Robot pose published to /tf\nros2 run tf2_ros tf2_echo map camera_link\n\n# 3D map points published as PointCloud2\nros2 topic echo /orbslam3/map_points --no-arr\n"})}),"\n",(0,r.jsx)(n.h3,{id:"vslam-alternatives",children:"VSLAM Alternatives"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"VSLAM System"}),(0,r.jsx)(n.th,{children:"Accuracy"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"ROS 2 Support"}),(0,r.jsx)(n.th,{children:"Best Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"ORB-SLAM3"})}),(0,r.jsx)(n.td,{children:"Excellent"}),(0,r.jsx)(n.td,{children:"30 FPS"}),(0,r.jsx)(n.td,{children:"Community"}),(0,r.jsx)(n.td,{children:"Indoor navigation, static scenes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"VINS-Fusion"})}),(0,r.jsx)(n.td,{children:"Excellent"}),(0,r.jsx)(n.td,{children:"20 FPS"}),(0,r.jsx)(n.td,{children:"Native"}),(0,r.jsx)(n.td,{children:"High-speed motion, IMU fusion"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"SVO2"})}),(0,r.jsx)(n.td,{children:"Good"}),(0,r.jsx)(n.td,{children:"100+ FPS"}),(0,r.jsx)(n.td,{children:"Community"}),(0,r.jsx)(n.td,{children:"Drones, fast-moving robots"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"RTAB-Map"})}),(0,r.jsx)(n.td,{children:"Good"}),(0,r.jsx)(n.td,{children:"10 FPS"}),(0,r.jsx)(n.td,{children:"Native"}),(0,r.jsx)(n.td,{children:"Large-scale mapping, 3D reconstruction"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For humanoids:"})," ORB-SLAM3 (best accuracy) or VINS-Fusion (if using IMU)"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"object-detection-and-segmentation",children:"Object Detection and Segmentation"}),"\n",(0,r.jsx)(n.h3,{id:"yolov8-real-time-object-detection",children:"YOLOv8: Real-Time Object Detection"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"YOLOv8 (2023) improvements over YOLOv5:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"+3.7 mAP on COCO dataset (53.9% vs 50.2%)"}),"\n",(0,r.jsx)(n.li,{children:"2x faster training convergence"}),"\n",(0,r.jsx)(n.li,{children:"Anchor-free design (simpler, more robust)"}),"\n",(0,r.jsx)(n.li,{children:"Improved small object detection"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Model Sizes:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Parameters"}),(0,r.jsx)(n.th,{children:"COCO mAP"}),(0,r.jsx)(n.th,{children:"Speed (RTX 4070 Ti)"}),(0,r.jsx)(n.th,{children:"VRAM"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLOv8n"}),(0,r.jsx)(n.td,{children:"3.2M"}),(0,r.jsx)(n.td,{children:"37.3%"}),(0,r.jsx)(n.td,{children:"150 FPS"}),(0,r.jsx)(n.td,{children:"1.5 GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLOv8s"}),(0,r.jsx)(n.td,{children:"11.2M"}),(0,r.jsx)(n.td,{children:"44.9%"}),(0,r.jsx)(n.td,{children:"120 FPS"}),(0,r.jsx)(n.td,{children:"2.5 GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLOv8m"}),(0,r.jsx)(n.td,{children:"25.9M"}),(0,r.jsx)(n.td,{children:"50.2%"}),(0,r.jsx)(n.td,{children:"80 FPS"}),(0,r.jsx)(n.td,{children:"4.5 GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"YOLOv8l"})}),(0,r.jsx)(n.td,{children:"43.7M"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"52.9%"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"60 FPS"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"6.5 GB"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLOv8x"}),(0,r.jsx)(n.td,{children:"68.2M"}),(0,r.jsx)(n.td,{children:"53.9%"}),(0,r.jsx)(n.td,{children:"40 FPS"}),(0,r.jsx)(n.td,{children:"9.5 GB"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommendation:"})," YOLOv8l (best accuracy/speed trade-off for humanoids)"]}),"\n",(0,r.jsx)(n.h3,{id:"yolov8-ros-2-integration",children:"YOLOv8 ROS 2 Integration"}),"\n",(0,r.jsxs)(n.p,{children:["See ",(0,r.jsx)(n.code,{children:"code-examples/perception/yolo_detector.py"})," for complete implementation."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Installation:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install ultralytics>=8.0.0\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Test YOLOv8 inference:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n\n# Load model (downloads on first run)\nmodel = YOLO('yolov8l.pt')\n\n# Run inference\nresults = model('/camera/color/image_raw', stream=True)\n\n# Process detections\nfor result in results:\n    boxes = result.boxes.xyxy.cpu().numpy()  # [N, 4] bounding boxes\n    confs = result.boxes.conf.cpu().numpy()   # [N] confidences\n    classes = result.boxes.cls.cpu().numpy()  # [N] class IDs\n\n    for box, conf, cls in zip(boxes, confs, classes):\n        print(f\"Detected {model.names[int(cls)]} with {conf:.2f} confidence\")\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Expected performance:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"COCO classes:"})," 80 objects (person, car, cup, bottle, etc.)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy:"})," 52.9% mAP (YOLOv8l)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency:"})," 16ms (60 FPS on RTX 4070 Ti)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"False positives:"})," ~5-10% (tune confidence threshold)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"sam-2-segment-anything-model",children:"SAM 2: Segment Anything Model"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"SAM 2 (2024)"})," is Meta's foundation model for segmentation:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Zero-shot segmentation (no fine-tuning needed)"}),"\n",(0,r.jsx)(n.li,{children:"Supports points, boxes, masks as prompts"}),"\n",(0,r.jsx)(n.li,{children:"Works on images AND videos"}),"\n",(0,r.jsx)(n.li,{children:"6x faster than SAM 1"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use cases for humanoids:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Segment unknown objects (not in COCO dataset)"}),"\n",(0,r.jsx)(n.li,{children:"Generate precise masks for grasp planning"}),"\n",(0,r.jsx)(n.li,{children:"Track objects across video frames"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Installation:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install segment-anything-2\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Usage:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\n# Load SAM 2 model\ncheckpoint = "sam2_hiera_large.pt"\nmodel_cfg = "sam2_hiera_l.yaml"\npredictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))\n\n# Set image\npredictor.set_image(rgb_image)\n\n# Prompt with bounding box from YOLOv8\ninput_box = np.array([x1, y1, x2, y2])  # From YOLO detection\nmasks, scores, _ = predictor.predict(\n    point_coords=None,\n    point_labels=None,\n    box=input_box[None, :],\n    multimask_output=False\n)\n\n# masks[0] is binary mask (H, W)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"YOLOv8 + SAM 2 Pipeline:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"YOLOv8 detects objects \u2192 bounding boxes"}),"\n",(0,r.jsx)(n.li,{children:"For each box, run SAM 2 \u2192 precise segmentation mask"}),"\n",(0,r.jsx)(n.li,{children:"Use mask for grasp planning or obstacle avoidance"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Expected performance:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency:"})," 50-80ms per object (SAM 2 Large)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy:"})," 95%+ IoU (Intersection over Union)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VRAM:"})," 4.5 GB (SAM 2 Large)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"grounding-dino-open-vocabulary-detection",children:"Grounding DINO: Open-Vocabulary Detection"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem with YOLOv8:"})," Only detects 80 pre-defined COCO classes."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Grounding DINO (2023)"})," detects ANY object from text prompt:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"red cup" \u2192 detects red cups specifically'}),"\n",(0,r.jsx)(n.li,{children:'"smartphone" \u2192 detects phones (not in COCO)'}),"\n",(0,r.jsx)(n.li,{children:'"potted plant with yellow flowers" \u2192 fine-grained queries'}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Installation:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install groundingdino-py\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Usage:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from groundingdino.util.inference import Model\n\nmodel = Model(\n    model_config_path="GroundingDINO_SwinT_OGC.py",\n    model_checkpoint_path="groundingdino_swint_ogc.pth"\n)\n\n# Detect with text prompt\nTEXT_PROMPT = "red cup . blue bottle . smartphone"\ndetections = model.predict_with_classes(\n    image=rgb_image,\n    classes=TEXT_PROMPT.split(" . "),\n    box_threshold=0.35,\n    text_threshold=0.25\n)\n\n# detections.xyxy: bounding boxes [N, 4]\n# detections.confidence: scores [N]\n# detections.class_id: which prompt matched\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"When to use Grounding DINO:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Detecting objects not in COCO (tools, specific products)"}),"\n",(0,r.jsx)(n.li,{children:'Fine-grained queries ("red cup" vs "blue cup")'}),"\n",(0,r.jsx)(n.li,{children:'Dynamic task specifications ("pick up the tallest bottle")'}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-off:"})," Slower than YOLOv8 (150ms vs 16ms) but more flexible."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,r.jsx)(n.h3,{id:"why-fuse-multiple-sensors",children:"Why Fuse Multiple Sensors?"}),"\n",(0,r.jsx)(n.p,{children:"Single sensor limitations:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera:"})," Fails in low light, affected by motion blur"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR:"})," No color/texture information, expensive"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU:"})," Drifts over time without visual correction"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sensor fusion"})," combines complementary sensors for robustness."]}),"\n",(0,r.jsx)(n.h3,{id:"extended-kalman-filter-ekf-for-visual-inertial-odometry",children:"Extended Kalman Filter (EKF) for Visual-Inertial Odometry"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart LR\n    subgraph "Inputs"\n        I1[Camera<br/>30 Hz]\n        I2[IMU<br/>200 Hz]\n    end\n\n    subgraph "EKF State"\n        S1[Position<br/>x, y, z]\n        S2[Velocity<br/>vx, vy, vz]\n        S3[Orientation<br/>qw, qx, qy, qz]\n        S4[IMU Bias<br/>ba, bg]\n    end\n\n    subgraph "Prediction Step"\n        P1[IMU Propagation<br/>200 Hz]\n    end\n\n    subgraph "Update Step"\n        U1[Visual Correction<br/>30 Hz]\n    end\n\n    subgraph "Output"\n        O1[Fused Pose<br/>200 Hz]\n    end\n\n    I2 --\x3e P1\n    P1 --\x3e S1\n    P1 --\x3e S2\n    P1 --\x3e S3\n    I1 --\x3e U1\n    U1 --\x3e S1\n    U1 --\x3e S3\n    S1 --\x3e O1\n    S2 --\x3e O1\n    S3 --\x3e O1\n\n    style I1 fill:#00adef\n    style I2 fill:#76b900\n    style P1 fill:#ffcc00\n    style U1 fill:#9966ff'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key idea:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prediction:"})," IMU provides high-frequency pose estimates (200 Hz)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Update:"})," Camera corrects accumulated IMU drift (30 Hz)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Result:"})," Smooth, drift-free pose at 200 Hz"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"robot-localization-ros-2-package",children:"Robot Localization (ROS 2 Package)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"robot_localization"})," implements EKF/UKF for multi-sensor fusion:"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Installation:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"sudo apt install ros-iron-robot-localization\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["Configuration (",(0,r.jsx)(n.code,{children:"ekf_config.yaml"}),"):"]})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"ekf_filter_node:\n  ros__parameters:\n    frequency: 200.0\n    sensor_timeout: 0.1\n    two_d_mode: false\n\n    # Camera odometry (from ORB-SLAM3)\n    odom0: /orbslam3/odom\n    odom0_config: [true, true, true,    # x, y, z\n                   false, false, false, # roll, pitch, yaw\n                   false, false, false, # vx, vy, vz\n                   false, false, false, # vroll, vpitch, vyaw\n                   false, false, false] # ax, ay, az\n    odom0_differential: false\n    odom0_relative: false\n\n    # IMU data\n    imu0: /camera/imu\n    imu0_config: [false, false, false,  # x, y, z\n                  true, true, true,     # roll, pitch, yaw (orientation)\n                  false, false, false,  # vx, vy, vz\n                  true, true, true,     # vroll, vpitch, vyaw (angular vel)\n                  true, true, true]     # ax, ay, az (linear accel)\n    imu0_differential: false\n    imu0_relative: true\n    imu0_remove_gravitational_acceleration: true\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Launch EKF:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 run robot_localization ekf_node --ros-args --params-file ekf_config.yaml\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Output:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Fused odometry published to ",(0,r.jsx)(n.code,{children:"/odometry/filtered"})]}),"\n",(0,r.jsxs)(n.li,{children:["TF published: ",(0,r.jsx)(n.code,{children:"odom \u2192 base_link"})]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"target-latencies-for-real-time-perception",children:"Target Latencies for Real-Time Perception"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Target Latency"}),(0,r.jsx)(n.th,{children:"Notes"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Camera capture"}),(0,r.jsx)(n.td,{children:"<5ms"}),(0,r.jsx)(n.td,{children:"Hardware-dependent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Image preprocessing"}),(0,r.jsx)(n.td,{children:"<10ms"}),(0,r.jsx)(n.td,{children:"Rectification, alignment"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Object detection (YOLOv8)"}),(0,r.jsx)(n.td,{children:"<20ms"}),(0,r.jsx)(n.td,{children:"50+ FPS on RTX 4070 Ti"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Depth processing"}),(0,r.jsx)(n.td,{children:"<15ms"}),(0,r.jsx)(n.td,{children:"Point cloud generation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"VSLAM frontend"}),(0,r.jsx)(n.td,{children:"<30ms"}),(0,r.jsx)(n.td,{children:"Feature extraction + matching"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Total perception latency"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"<100ms"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"For VLA integration"})})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"optimization-strategies",children:"Optimization Strategies"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. GPU Acceleration"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Use TensorRT for YOLOv8 (2-3x speedup)\nfrom ultralytics import YOLO\nmodel = YOLO('yolov8l.pt')\nmodel.export(format='engine')  # Export to TensorRT\ntrt_model = YOLO('yolov8l.engine')  # Load TensorRT model\n\n# Latency: 16ms \u2192 6ms on RTX 4070 Ti\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Asynchronous Processing"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import threading\n\nclass PerceptionPipeline:\n    def __init__(self):\n        self.latest_image = None\n        self.latest_detections = None\n\n        # Detection runs in background thread\n        self.detection_thread = threading.Thread(target=self._detection_loop, daemon=True)\n        self.detection_thread.start()\n\n    def image_callback(self, msg):\n        self.latest_image = msg  # Non-blocking update\n\n    def _detection_loop(self):\n        while True:\n            if self.latest_image is not None:\n                self.latest_detections = self.detector(self.latest_image)\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Reduce Resolution for Detection"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# YOLOv8 accepts 640x640 input (not 1280x720)\n# Downsampling reduces computation by 3-4x\nmodel = YOLO('yolov8l.pt')\nresults = model(image, imgsz=640)  # Faster than imgsz=1280\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. Skip Frames for Slow Models"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"frame_count = 0\nfor frame in camera_stream:\n    frame_count += 1\n\n    # Run YOLOv8 every frame (fast)\n    detections = yolo_model(frame)\n\n    # Run SAM 2 every 5th frame (slow)\n    if frame_count % 5 == 0:\n        masks = sam_model(frame, detections)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"debugging-perception-systems",children:"Debugging Perception Systems"}),"\n",(0,r.jsx)(n.h3,{id:"common-failure-modes",children:"Common Failure Modes"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Low Detection Accuracy (<60%)"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Objects not detected"}),"\n",(0,r.jsx)(n.li,{children:"Many false positives"}),"\n",(0,r.jsx)(n.li,{children:"Bounding boxes incorrect"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Debugging steps:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check image quality\nros2 topic hz /camera/color/image_raw  # Should be 30 Hz\nros2 topic echo /camera/color/camera_info  # Verify resolution\n\n# Visualize raw detections\nros2 run rqt_image_view rqt_image_view /yolo/detections\n\n# Test on static image\npython yolo_detector.py --image test.jpg --visualize\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Common causes:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Poor lighting:"})," Add auxiliary lighting or use auto-exposure"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Motion blur:"})," Increase camera shutter speed"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Wrong model:"})," Use YOLOv8l instead of YOLOv8n for better accuracy"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Low confidence threshold:"})," Increase from 0.25 \u2192 0.5"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. VSLAM Tracking Lost"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Robot pose jumps erratically"}),"\n",(0,r.jsx)(n.li,{children:"Map points disappear"}),"\n",(0,r.jsx)(n.li,{children:'"Tracking lost" warnings in ORB-SLAM3'}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Debugging steps:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check feature density\nros2 topic echo /orbslam3/tracking_state\n# State should be 2 (OK), not 1 (NOT_INITIALIZED) or 3 (LOST)\n\n# Visualize tracked features\n# Green points = good, red points = lost\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Common causes:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Low-texture scenes:"})," Add visual markers (AprilTags, posters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fast motion:"})," Reduce robot speed during navigation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera motion blur:"})," Use global shutter camera (avoid rolling shutter)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lighting changes:"})," Disable auto-exposure, use fixed exposure"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Depth Noise and Holes"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Depth image has black holes (no data)"}),"\n",(0,r.jsx)(n.li,{children:"Noisy point clouds (scattered points)"}),"\n",(0,r.jsx)(n.li,{children:"Depth values jump erratically"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Debugging:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Visualize depth image\nros2 run rqt_image_view rqt_image_view /camera/depth/image_rect_raw\n\n# Check depth quality\nros2 topic hz /camera/depth/image_rect_raw  # Should be 30 Hz\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Common causes:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Infrared interference:"})," Avoid sunlight, multiple RealSense cameras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reflective surfaces:"})," Glass, mirrors, polished metal (no depth return)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transparent objects:"})," Water, clear plastic (infrared passes through)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Distance limits:"})," Objects >6m (D455 max range)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use depth hole filling (OpenCV inpainting)"}),"\n",(0,r.jsx)(n.li,{children:"Switch to stereo depth (ZED 2i) for outdoor use"}),"\n",(0,r.jsx)(n.li,{children:"Add texture to reflective surfaces (masking tape, stickers)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. High Latency (>200ms)"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Robot reacts slowly to obstacles"}),"\n",(0,r.jsx)(n.li,{children:"VLA pipeline timeout errors"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Profiling:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\n\nstart = time.time()\ndetections = yolo_model(image)\nprint(f"YOLOv8 latency: {(time.time() - start) * 1000:.1f}ms")\n\nstart = time.time()\npoint_cloud = depth_processor(depth_image)\nprint(f"Depth processing: {(time.time() - start) * 1000:.1f}ms")\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use TensorRT for YOLOv8 (2-3x speedup)"}),"\n",(0,r.jsx)(n.li,{children:"Reduce image resolution (1280x720 \u2192 640x480)"}),"\n",(0,r.jsx)(n.li,{children:"Use smaller model (YOLOv8l \u2192 YOLOv8m)"}),"\n",(0,r.jsxs)(n.li,{children:["Profile with ",(0,r.jsx)(n.code,{children:"nvprof"})," to find bottlenecks"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"integration-example-perception-for-manipulation",children:"Integration Example: Perception for Manipulation"}),"\n",(0,r.jsx)(n.h3,{id:"end-to-end-workflow",children:"End-to-End Workflow"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"}),' "Pick up the red cup from the table"']}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Perception pipeline:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'sequenceDiagram\n    participant V as VLA Model\n    participant P as Perception Stack\n    participant C as Camera\n    participant R as Robot Controller\n\n    V->>P: Request: "Find red cup"\n    P->>C: Capture RGB-D image\n    C--\x3e>P: Image + Depth\n    P->>P: Run YOLOv8 (detect cups)\n    P->>P: Run Grounding DINO ("red cup")\n    P->>P: Run SAM 2 (segment cup)\n    P->>P: Estimate 3D position from depth\n    P--\x3e>V: Cup pose: (x, y, z, quaternion)\n    V->>V: Generate grasp action\n    V->>R: Execute grasp\n    R--\x3e>V: Success feedback'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Code example:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# 1. Detect all cups with YOLOv8\nresults = yolo_model(rgb_image)\ncup_boxes = [box for box, cls in zip(results.boxes.xyxy, results.boxes.cls)\n             if model.names[int(cls)] == 'cup']\n\n# 2. Filter to red cups with Grounding DINO\nred_cup_boxes = grounding_dino_model(rgb_image, \"red cup\")\n\n# 3. Segment cup with SAM 2\ncup_mask = sam_model(rgb_image, red_cup_boxes[0])\n\n# 4. Get 3D position from depth\ncup_3d_pos = get_3d_position(cup_mask, depth_image, camera_intrinsics)\n\n# 5. Send to VLA model\nvla_input = {\n    'image': rgb_image,\n    'text': 'pick up red cup',\n    'target_position': cup_3d_pos\n}\nactions = vla_model.predict(vla_input)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ORB-SLAM3"})," (2021) - ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2007.11898",children:"arXiv:2007.11898"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"State-of-the-art visual-inertial SLAM with loop closure"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"YOLOv8"})," (2023) - ",(0,r.jsx)(n.a,{href:"https://docs.ultralytics.com",children:"Ultralytics Docs"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Real-time object detection improvements"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SAM 2"})," (2024) - ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2408.00714",children:"arXiv:2408.00714"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Segment Anything in images and videos"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Grounding DINO"})," (2023) - ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2303.05499",children:"arXiv:2303.05499"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Open-vocabulary detection with text prompts"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VINS-Fusion"})," (2019) - ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1901.03642",children:"arXiv:1901.03642"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Visual-inertial odometry with global optimization"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"open-source-repositories",children:"Open-Source Repositories"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ORB-SLAM3:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/UZ-SLAMLab/ORB_SLAM3",children:"github.com/UZ-SLAMLab/ORB_SLAM3"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"YOLOv8:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/ultralytics/ultralytics",children:"github.com/ultralytics/ultralytics"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SAM 2:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/facebookresearch/segment-anything-2",children:"github.com/facebookresearch/segment-anything-2"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Grounding DINO:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/IDEA-Research/GroundingDINO",children:"github.com/IDEA-Research/GroundingDINO"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RealSense ROS 2:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/IntelRealSense/realsense-ros",children:"github.com/IntelRealSense/realsense-ros"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"robot_localization:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/cra-ros-pkg/robot_localization",children:"github.com/cra-ros-pkg/robot_localization"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-documentation",children:"ROS 2 Documentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tf2:"})," ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/iron/Tutorials/Intermediate/Tf2/",children:"docs.ros.org/en/iron/Tutorials/Intermediate/Tf2/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"sensor_msgs:"})," ",(0,r.jsx)(n.a,{href:"https://docs.ros2.org/latest/api/sensor_msgs/",children:"docs.ros2.org/latest/api/sensor_msgs/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"image_transport:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/ros-perception/image_common",children:"github.com/ros-perception/image_common"})]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"RGB-D camera setup"})," with Intel RealSense D455 on ROS 2"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Depth processing"})," for 3D point cloud generation"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Visual SLAM"})," with ORB-SLAM3 for localization and mapping"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Object detection"})," with YOLOv8 (52.9% mAP, 60 FPS)"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Segmentation"})," with SAM 2 for precise object masks"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Open-vocabulary detection"})," with Grounding DINO"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Sensor fusion"})," with Extended Kalman Filter (camera + IMU)"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Performance optimization"})," achieving <100ms perception latency"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Debugging strategies"})," for common perception failures"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Next steps:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chapter 08:"})," Bipedal Locomotion (gait generation, balance control)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chapter 09:"})," Dexterous Manipulation (grasp planning, IK, force control)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Capstone Project:"})," Integrate perception with VLA for autonomous butler"]}),"\n"]}),"\n",(0,r.jsxs)(n.admonition,{title:"Practical Exercise",type:"tip",children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Build your own perception pipeline:"})}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Set up RealSense D455 camera on ROS 2"}),"\n",(0,r.jsx)(n.li,{children:"Run ORB-SLAM3 for real-time localization"}),"\n",(0,r.jsx)(n.li,{children:"Deploy YOLOv8 for object detection"}),"\n",(0,r.jsx)(n.li,{children:"Achieve >85% detection accuracy in Isaac Sim test scene"}),"\n",(0,r.jsx)(n.li,{children:"Measure end-to-end perception latency (<100ms target)"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time estimate:"})," 2-3 hours\n",(0,r.jsx)(n.strong,{children:"Difficulty:"})," Intermediate\n",(0,r.jsx)(n.strong,{children:"Hardware:"})," RealSense D455, RTX 4070 Ti"]})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>o});var i=s(6540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);