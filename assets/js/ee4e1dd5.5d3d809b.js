"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[4258],{2306:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>a,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"vla-models/quiz","title":"Chapter 10 Quiz - Vision-Language-Action Models","description":"Test your understanding of VLA models, OpenVLA, GR00T N1, and robot learning","source":"@site/docs/vla-models/quiz.mdx","sourceDirName":"vla-models","slug":"/vla-models/quiz","permalink":"/HumanoidRoboticsBook/docs/vla-models/quiz","draft":false,"unlisted":false,"editUrl":"https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook/tree/main/docs/vla-models/quiz.mdx","tags":[],"version":"current","sidebarPosition":99,"frontMatter":{"id":"quiz","title":"Chapter 10 Quiz - Vision-Language-Action Models","sidebar_position":99,"description":"Test your understanding of VLA models, OpenVLA, GR00T N1, and robot learning"}}');var i=s(4848),t=s(8453);const l={id:"quiz",title:"Chapter 10 Quiz - Vision-Language-Action Models",sidebar_position:99,description:"Test your understanding of VLA models, OpenVLA, GR00T N1, and robot learning"},o="Chapter 10 Quiz: Vision-Language-Action Models",c={},d=[{value:"Question 1: VLA Architecture Components",id:"question-1-vla-architecture-components",level:2},{value:"Question 2: OpenVLA Performance",id:"question-2-openvla-performance",level:2},{value:"Question 3: Model Quantization",id:"question-3-model-quantization",level:2},{value:"Question 4: GR00T N1 Dual-System Architecture",id:"question-4-gr00t-n1-dual-system-architecture",level:2},{value:"Question 5: VLA Model Selection",id:"question-5-vla-model-selection",level:2},{value:"Question 6: Action Decoder Types",id:"question-6-action-decoder-types",level:2},{value:"Question 7: Training Data Scale",id:"question-7-training-data-scale",level:2},{value:"Question 8: Practical Deployment",id:"question-8-practical-deployment",level:2},{value:"Score Interpretation",id:"score-interpretation",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function h(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-10-quiz-vision-language-action-models",children:"Chapter 10 Quiz: Vision-Language-Action Models"})}),"\n",(0,i.jsx)(n.p,{children:"Test your mastery of VLA concepts, model architectures, and deployment strategies!"}),"\n",(0,i.jsx)(n.admonition,{title:"Quiz Instructions",type:"info",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"8 questions covering VLA fundamentals through advanced topics"}),"\n",(0,i.jsx)(n.li,{children:"Each question has detailed explanations"}),"\n",(0,i.jsx)(n.li,{children:"Score 80% or higher to demonstrate mastery"}),"\n",(0,i.jsx)(n.li,{children:"Review relevant sections if you score below 80%"}),"\n"]})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-1-vla-architecture-components",children:"Question 1: VLA Architecture Components"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Which component is responsible for generating actual robot control commands in a VLA model?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) Vision Encoder (DINOv2, SigLIP)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsx)(n.p,{children:"The vision encoder extracts visual features from camera images but does NOT generate actions."}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it does:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Input: 224x224 RGB image"}),"\n",(0,i.jsx)(n.li,{children:"Output: 768-1024D feature vector"}),"\n",(0,i.jsx)(n.li,{children:"Purpose: Scene understanding, object detection"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For action generation:"})," You need the Action Decoder (MLP, Diffusion, or Flow Matching)."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) Language Model (Llama 2, Gemma)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsx)(n.p,{children:"The language model processes text commands but doesn't directly output robot actions."}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it does:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Input: Natural language string"}),"\n",(0,i.jsx)(n.li,{children:"Output: Contextualized embeddings"}),"\n",(0,i.jsx)(n.li,{children:"Purpose: Command understanding, task reasoning"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For actions:"})," The fusion layer combines language + vision, then the action decoder generates motor commands."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) Fusion Layer (Cross-Attention)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsx)(n.p,{children:"The fusion layer combines vision and language features but doesn't generate actions."}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it does:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Input: Vision features + Language embeddings"}),"\n",(0,i.jsx)(n.li,{children:"Output: Fused multimodal representation"}),"\n",(0,i.jsx)(n.li,{children:"Purpose: Align visual observations with language commands"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next step:"})," Fused features go to the action decoder."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Action Decoder (MLP, Diffusion, Flow Matching)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Correct!"})}),(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"action decoder"})," is the final component that generates robot control commands."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Three main types:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"MLP Decoder (OpenVLA)"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Direct regression: fused_features  actions"}),"\n",(0,i.jsx)(n.li,{children:"Speed: 80-120ms (8-12 Hz)"}),"\n",(0,i.jsx)(n.li,{children:"Use case: Fast, deterministic control"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Diffusion Decoder (Octo)"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Iterative denoising: noise  actions"}),"\n",(0,i.jsx)(n.li,{children:"Speed: ~100ms (10-20 Hz)"}),"\n",(0,i.jsx)(n.li,{children:"Use case: Smooth trajectories, multimodal distributions"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Flow Matching (0, GR00T N1)"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Direct flow: noise  actions (no iteration)"}),"\n",(0,i.jsx)(n.li,{children:"Speed: 8-20ms (50-120 Hz)"}),"\n",(0,i.jsx)(n.li,{children:"Use case: Real-time humanoid control"}),"\n"]}),"\n"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Output format:"})," Typically 7-DoF vector: [joint1, joint2, joint3, joint4, joint5, joint6, gripper]"]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-2-openvla-performance",children:"Question 2: OpenVLA Performance"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"OpenVLA-7B outperforms RT-2-X (55B) by how much on the Open X-Embodiment benchmark?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) +5.2% absolute task success rate"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsx)(n.p,{children:"OpenVLA's improvement is much larger than 5.2%."}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Actual performance:"})," +16.5% absolute task success rate with 7x fewer parameters."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) +16.5% absolute task success rate"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Correct!"})}),(0,i.jsxs)(n.p,{children:["OpenVLA-7B achieves ",(0,i.jsx)(n.strong,{children:"+16.5% absolute improvement"})," over RT-2-X (55B) on 29 tasks."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key results:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenVLA-7B:"})," 72.1% task success rate (7.23B parameters)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RT-2-X:"})," 55.6% task success rate (55B parameters)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Improvement:"})," +16.5 percentage points"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Efficiency:"})," 7x fewer parameters"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why OpenVLA wins:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dual vision encoders:"})," DINOv2 + SigLIP (complementary features)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Better training data:"})," 970K trajectories from Open X-Embodiment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Efficient architecture:"})," Llama 2-7B backbone optimized for robotics"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Implication:"})," Smaller, open-source models can outperform large proprietary models!"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) -8.3% (RT-2-X is better)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsx)(n.p,{children:"This is backwards! OpenVLA significantly outperforms RT-2-X."}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Actual:"})," OpenVLA +16.5% better than RT-2-X."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Equal performance (same success rate)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsx)(n.p,{children:"OpenVLA substantially outperforms RT-2-X."}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Actual:"})," +16.5% absolute improvement with 7x fewer parameters."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-3-model-quantization",children:"Question 3: Model Quantization"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"You have an RTX 4060 Ti with 16 GB VRAM. OpenVLA-7B requires 14 GB in FP16. What's the BEST quantization strategy for production deployment?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) Keep FP16 precision (14 GB VRAM)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Suboptimal"})}),(0,i.jsx)(n.p,{children:"While FP16 fits on 16 GB VRAM, it leaves only 2 GB for:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"OS and other processes"}),"\n",(0,i.jsx)(n.li,{children:"Inference batch processing"}),"\n",(0,i.jsx)(n.li,{children:"Safety margin"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Better:"})," INT8 gives 50% VRAM reduction with <1% accuracy loss."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) Use INT8 quantization (7 GB VRAM)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Correct - Best Choice!"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"INT8 is optimal for production:"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Advantages:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VRAM:"})," 7 GB (50% reduction from FP16)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 1.5-2x faster inference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," 71.6% vs 72.1% FP16 (-0.5%, negligible)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Headroom:"})," 9 GB free for batch processing, OS, safety margin"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Code:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from transformers import BitsAndBytesConfig\n\nquant_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    llm_int8_threshold=6.0\n)\n\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "openvla/openvla-7b",\n    quantization_config=quant_config,\n    device_map="auto"\n)\n'})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Production recommendation:"})," INT8 for best balance of speed, accuracy, and resource efficiency."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) Use INT4 quantization (3.5 GB VRAM)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Over-aggressive"})}),(0,i.jsx)(n.p,{children:"INT4 saves maximum VRAM but sacrifices too much accuracy for production:"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Trade-offs:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VRAM:"})," 3.5 GB (great!)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 2-3x faster (great!)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," 69.8% vs 72.1% FP16 (-2.3%, ",(0,i.jsx)(n.strong,{children:"significant"}),")"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"When to use INT4:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Experimentation on low-VRAM GPUs (RTX 3060 8GB)"}),"\n",(0,i.jsx)(n.li,{children:"Rapid prototyping"}),"\n",(0,i.jsx)(n.li,{children:"Non-critical applications"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For production:"})," INT8 is better - minimal accuracy loss with good VRAM savings."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Don't quantize, buy more VRAM"}),(0,i.jsxs)(n.p,{children:["L ",(0,i.jsx)(n.strong,{children:"Impractical"})]}),(0,i.jsx)(n.p,{children:"Upgrading GPU is expensive and unnecessary when INT8 gives 50% VRAM reduction with <1% accuracy loss."}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Better:"})," Use INT8 quantization for production deployment."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-4-gr00t-n1-dual-system-architecture",children:"Question 4: GR00T N1 Dual-System Architecture"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"In GR00T N1's dual-system architecture, what is the primary role of System 1?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) High-level task planning and reasoning"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsxs)(n.p,{children:["That's ",(0,i.jsx)(n.strong,{children:"System 2's"})," role (the VLM)."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"System 2 (slow thinking):"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Frequency: 1-2 Hz"}),"\n",(0,i.jsx)(n.li,{children:"Purpose: Reasons about scene, generates high-level plan"}),"\n",(0,i.jsx)(n.li,{children:'Example: "To pick cup: 1) approach, 2) grasp, 3) lift, 4) move to table"'}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"System 1:"})," Fast motor control (120 Hz)."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) Fast reactive motor control at 120Hz"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Correct!"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"System 1 (fast thinking)"})," executes reactive motor control at ",(0,i.jsx)(n.strong,{children:"120Hz"}),"."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"System 1 characteristics:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Frequency:"})," 120 Hz (8ms per action)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input:"})," High-level plan from System 2 + current visual observations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output:"})," Continuous action sequence (joint positions, velocities)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Purpose:"})," Real-time reactive control, balance recovery, obstacle avoidance"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"System 2 characteristics:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Frequency:"})," 1-2 Hz"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input:"})," Scene image + natural language command"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output:"})," High-level task plan"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Purpose:"})," Deliberate reasoning, task understanding"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Analogy to human cognition:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System 2:"}),' Conscious planning ("I need to pick up this cup")']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System 1:"})," Automatic execution (muscle memory, reflexes)"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why this matters for humanoids:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Bipedal balance requires fast feedback (100+ Hz)"}),"\n",(0,i.jsx)(n.li,{children:"Task planning can be slower (1-2 Hz)"}),"\n",(0,i.jsx)(n.li,{children:"Separation allows optimization of each system independently"}),"\n"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) Vision encoding from DINOv2"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsx)(n.p,{children:"Vision encoding is part of both systems but not the primary role of System 1."}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"System 1's role:"})," Fast motor control at 120Hz, not vision processing."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Language command parsing"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsxs)(n.p,{children:["Language parsing is done by ",(0,i.jsx)(n.strong,{children:"System 2"})," (the VLM)."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"System 1:"})," Fast motor control based on System 2's plan."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-5-vla-model-selection",children:"Question 5: VLA Model Selection"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"You're building a mobile manipulator for warehouse automation. Which VLA model is MOST appropriate?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) OpenVLA-7B"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Good but not optimal"})}),(0,i.jsx)(n.p,{children:"OpenVLA is excellent for learning and research but:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Strength:"})," Open-source, well-documented, strong baseline"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Limitation:"})," Not specifically optimized for mobile manipulation or open-world scenarios"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Better choice:"})," 0.5 (designed for mobile manipulators with open-world generalization)."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) GR00T N1"}),(0,i.jsxs)(n.p,{children:["L ",(0,i.jsx)(n.strong,{children:"Incorrect - Wrong Use Case"})]}),(0,i.jsxs)(n.p,{children:["GR00T N1 is designed for ",(0,i.jsx)(n.strong,{children:"humanoid robots"}),", not mobile manipulators."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"GR00T N1 strengths:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Bipedal locomotion"}),"\n",(0,i.jsx)(n.li,{children:"Whole-body coordination"}),"\n",(0,i.jsx)(n.li,{children:"Humanoid-specific training data (Fourier GR-1, 1X NEO)"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For warehouse mobile manipulation:"})," Use 0.5 or Octo instead."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) 0.5 (Physical Intelligence)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Correct - Best Choice!"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"0.5 is optimal for warehouse mobile manipulation:"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key advantages:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Open-world generalization:"})," Can clean entirely new environments (kitchens, bedrooms) zero-shot"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mobile manipulator focus:"})," Trained on mobile manipulator platforms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Flow matching:"})," 50Hz smooth trajectories"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Large dataset:"})," 10,000+ hours from 7 robot platforms"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Real-world demonstration:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Physical Intelligence deployed 0.5 on mobile manipulators"}),"\n",(0,i.jsx)(n.li,{children:"Successfully cleaned new, unseen environments"}),"\n",(0,i.jsx)(n.li,{children:"Robust to environmental variations"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Alternative:"})," Octo (best for multi-embodiment transfer, fast fine-tuning)"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why not others:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"OpenVLA: Good baseline but not specialized for mobile manipulation"}),"\n",(0,i.jsx)(n.li,{children:"GR00T N1: Humanoid-specific"}),"\n",(0,i.jsx)(n.li,{children:"Helix: Humanoid upper-body only (Figure 02)"}),"\n"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Helix (Figure AI)"}),(0,i.jsxs)(n.p,{children:["L ",(0,i.jsx)(n.strong,{children:"Incorrect - Wrong Platform"})]}),(0,i.jsxs)(n.p,{children:["Helix is designed for ",(0,i.jsx)(n.strong,{children:"humanoid robots"})," (specifically Figure 02), not mobile manipulators."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Helix strengths:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Full humanoid upper-body control (35 DoF)"}),"\n",(0,i.jsx)(n.li,{children:"200Hz ultra-low-latency"}),"\n",(0,i.jsx)(n.li,{children:"Dexterous manipulation"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For warehouse mobile manipulation:"})," Use 0.5 instead."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-6-action-decoder-types",children:"Question 6: Action Decoder Types"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Which action decoder type produces the SMOOTHEST trajectories at the FASTEST inference speed?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) MLP Regressor (Direct prediction)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsx)(n.p,{children:"MLP is fast but NOT the smoothest."}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"MLP characteristics:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 80-120ms (8-12 Hz) - moderate"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Smoothness:"})," Deterministic but can be jerky"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use case:"})," OpenVLA, simple tasks"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Smoothest + fastest:"})," Flow Matching (0, GR00T N1) at 50-120 Hz."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) Diffusion Policy (Iterative denoising)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsx)(n.p,{children:"Diffusion produces smooth trajectories but is SLOWER."}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Diffusion characteristics:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," ~100ms (10-20 Hz) - slow due to iteration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Smoothness:"})," Excellent (gradual denoising)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use case:"})," Octo, complex multi-modal tasks"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Faster:"})," Flow Matching eliminates iteration, achieving 50-120 Hz."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) Flow Matching (Direct flow)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Correct!"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Flow Matching"})," achieves both smoothness AND speed."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How it works:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Method:"})," Direct flow from noise distribution to action distribution (no iteration)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 8-20ms per action (50-120 Hz)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Smoothness:"})," Continuous flow  smooth trajectories"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Used by:"})," 0 (50Hz), 0-FAST (50+Hz), GR00T N1 (120Hz)"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Comparison:"})}),(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Decoder Type"}),(0,i.jsx)(n.th,{children:"Speed (Hz)"}),(0,i.jsx)(n.th,{children:"Smoothness"}),(0,i.jsx)(n.th,{children:"Inference Method"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"MLP"})}),(0,i.jsx)(n.td,{children:"8-12"}),(0,i.jsx)(n.td,{children:"Moderate"}),(0,i.jsx)(n.td,{children:"Direct regression"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Diffusion"})}),(0,i.jsx)(n.td,{children:"10-20"}),(0,i.jsx)(n.td,{children:"Excellent"}),(0,i.jsx)(n.td,{children:"Iterative (20-100 steps)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Flow Matching"})}),(0,i.jsx)(n.td,{children:"50-120"}),(0,i.jsx)(n.td,{children:"Excellent"}),(0,i.jsx)(n.td,{children:"Direct (1 step)"})]})]})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key innovation:"})," Flow matching gets diffusion's smoothness without iteration penalty."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it matters:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Humanoid control needs 50+ Hz for balance"}),"\n",(0,i.jsx)(n.li,{children:"Flow matching enables real-time reactive control"}),"\n",(0,i.jsx)(n.li,{children:"Smoother than MLP, faster than diffusion"}),"\n"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Transformer Decoder (Autoregressive)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incorrect"})}),(0,i.jsx)(n.p,{children:"Transformer autoregressive decoding is typically SLOWER than all three options."}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Transformer characteristics:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," Sequential generation (slow)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Smoothness:"})," Depends on training"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use case:"})," Language modeling, not typical for VLA action decoders"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Best for actions:"})," Flow Matching (fast + smooth)."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-7-training-data-scale",children:"Question 7: Training Data Scale"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Which VLA model was trained on the LARGEST dataset?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) OpenVLA - 970K robot trajectories"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Large but not the largest"})}),(0,i.jsx)(n.p,{children:"OpenVLA uses 970,000 trajectories from Open X-Embodiment dataset."}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Estimated hours:"})," ~5,000-7,000 hours of robot operation."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Largest:"})," 0 with 10,000 hours from 7 platforms."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) Octo - 800K robot trajectories"}),(0,i.jsxs)(n.p,{children:["L ",(0,i.jsx)(n.strong,{children:"Not the largest"})]}),(0,i.jsx)(n.p,{children:"Octo uses 800,000 trajectories from 25 datasets."}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Estimated hours:"})," ~4,000-6,000 hours."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Largest:"})," 0 with 10,000 hours."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) 0 - 10,000 hours from 7 robot platforms"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Correct!"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"0 has the largest training dataset:"})," 10,000 hours of robot operation."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Dataset breakdown:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Platforms:"})," 7 different robot embodiments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tasks:"})," 68 unique task categories"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data type:"})," Real-world teleoperation demonstrations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quality:"})," High-quality, multi-operator data"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why this matters:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Better generalization:"})," More diverse scenarios covered"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robust policies:"})," Learned from many edge cases"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Zero-shot transfer:"})," Can handle novel situations"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Comparison:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"0:"})," 10,000 hours (largest)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenVLA:"})," ~6,000 hours equivalent (970K trajectories)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Octo:"})," ~5,000 hours equivalent (800K trajectories)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Helix:"})," 500 hours (smallest but highest quality)"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," Helix's 500 hours is multi-robot, multi-operator, specifically for humanoids."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) RT-2-X - Web-scale vision-language data + robot data"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Different metric"})}),(0,i.jsx)(n.p,{children:"RT-2-X uses web-scale pretraining BUT has less robot-specific data."}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Robot data:"})," Estimated <1,000 hours of robot trajectories\n",(0,i.jsx)(n.strong,{children:"Web data:"})," Billions of image-text pairs (not robot-specific)"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"0 has more robot-specific training data:"})," 10,000 hours pure robot demonstrations."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-8-practical-deployment",children:"Question 8: Practical Deployment"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"You need to deploy a VLA model on a Jetson AGX Orin (32GB RAM, 8-core ARM CPU, moderate GPU). What's your BEST strategy?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) Run OpenVLA-7B in FP16 (14 GB VRAM)"}),(0,i.jsxs)(n.p,{children:["L ",(0,i.jsx)(n.strong,{children:"Won't fit"})]}),(0,i.jsx)(n.p,{children:"Jetson AGX Orin has integrated GPU sharing system RAM, not 14+ GB dedicated VRAM."}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Jetson AGX Orin specs:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total RAM:"})," 32 GB (shared CPU/GPU)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU VRAM:"})," ~8-12 GB effective after OS"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FP16 OpenVLA:"})," Requires 14 GB  ",(0,i.jsx)(n.strong,{children:"won't fit"})]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution:"})," Use INT8 or smaller model."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) Run OpenVLA-7B with INT8 quantization (7 GB VRAM)"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Correct - Best Strategy!"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"INT8 quantization makes OpenVLA viable on Jetson:"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why this works:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VRAM:"})," 7 GB (fits in Jetson's effective GPU memory)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," 71.6% (-0.5% from FP16, negligible)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 1.5-2x faster than FP16 (important for ARM CPU)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deployment:"})," Production-ready with minimal accuracy loss"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Implementation:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from transformers import BitsAndBytesConfig\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "openvla/openvla-7b",\n    quantization_config=quant_config,\n    device_map="auto"  # Automatic GPU placement\n)\n'})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Alternative:"})," 0-FAST (3.5 GB, optimized for edge deployment)."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Jetson deployment tips:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Use INT8 quantization"}),"\n",(0,i.jsx)(n.li,{children:"Enable TensorRT optimization (NVIDIA native)"}),"\n",(0,i.jsx)(n.li,{children:"Reduce batch size to 1 for real-time inference"}),"\n",(0,i.jsx)(n.li,{children:"Monitor temperature (Jetson can throttle under load)"}),"\n"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) Use cloud API (Gemini Robotics)"}),(0,i.jsxs)(n.p,{children:["L ",(0,i.jsx)(n.strong,{children:"Defeats purpose of edge deployment"})]}),(0,i.jsx)(n.p,{children:"Cloud APIs require internet connectivity, introducing:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency:"})," 50-200ms network round-trip (too slow for real-time control)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reliability:"})," Fails when internet drops"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Privacy:"})," Sends camera/sensor data to cloud"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost:"})," API fees per request"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Edge deployment purpose:"})," Low latency, offline operation, privacy."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Better:"})," INT8 quantized local model."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Run GR00T N1 with INT4 quantization"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Possible but not optimal"})}),(0,i.jsx)(n.p,{children:"GR00T N1 requires more resources than OpenVLA even with INT4:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Architecture:"})," Dual-system (System 1 + System 2) = 2 models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Complexity:"})," 120Hz control loop requires more compute"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VRAM:"})," ~6-8 GB with INT4 (tight fit on Jetson)"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Better:"})," OpenVLA-7B INT8 is simpler and proven on edge devices."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"GR00T N1 edge deployment:"})," Wait for optimized edge-specific version."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"score-interpretation",children:"Score Interpretation"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Calculate your score:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["8/8 correct: ",(0,i.jsx)(n.strong,{children:"Expert"})," - Ready for VLA production deployment"]}),"\n",(0,i.jsxs)(n.li,{children:["6-7/8 correct: ",(0,i.jsx)(n.strong,{children:"Proficient"})," - Strong understanding, minor gaps"]}),"\n",(0,i.jsxs)(n.li,{children:["4-5/8 correct: ",(0,i.jsx)(n.strong,{children:"Developing"})," - Review key sections"]}),"\n",(0,i.jsxs)(n.li,{children:["0-3/8 correct: ",(0,i.jsx)(n.strong,{children:"Needs Review"})," - Re-read Chapter 10"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsx)(n.p,{children:"If you struggled with specific topics, review these sections:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Question"}),(0,i.jsx)(n.th,{children:"Topic"}),(0,i.jsx)(n.th,{children:"Review Section"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q1"}),(0,i.jsx)(n.td,{children:"VLA Architecture"}),(0,i.jsx)(n.td,{children:"VLA Architecture Fundamentals"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q2"}),(0,i.jsx)(n.td,{children:"OpenVLA Performance"}),(0,i.jsx)(n.td,{children:"OpenVLA: Your First VLA Model"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q3"}),(0,i.jsx)(n.td,{children:"Quantization"}),(0,i.jsx)(n.td,{children:"Model Quantization (Reduce VRAM)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q4"}),(0,i.jsx)(n.td,{children:"GR00T N1 Architecture"}),(0,i.jsx)(n.td,{children:"GR00T N1: Humanoid-Specific VLA"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q5"}),(0,i.jsx)(n.td,{children:"Model Selection"}),(0,i.jsx)(n.td,{children:"VLA Model Comparison"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q6"}),(0,i.jsx)(n.td,{children:"Action Decoders"}),(0,i.jsx)(n.td,{children:"VLA Architecture  Action Decoder"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q7"}),(0,i.jsx)(n.td,{children:"Training Data"}),(0,i.jsx)(n.td,{children:"Research: VLA Model Landscape"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q8"}),(0,i.jsx)(n.td,{children:"Edge Deployment"}),(0,i.jsx)(n.td,{children:"Model Quantization + Practical Tips"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"After completing this quiz:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Score >= 80%:"})," Proceed to Chapter 11 (Voice-to-Action Pipeline)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Score < 80%:"})," Review Chapter 10 sections and retry quiz"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hands-on:"})," Run ",(0,i.jsx)(n.code,{children:"code-examples/vla/openvla-inference.py"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advanced:"})," Experiment with INT8 vs INT4 quantization"]}),"\n"]}),"\n",(0,i.jsxs)(n.admonition,{title:"Practice Challenge",type:"tip",children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Deploy OpenVLA on your GPU:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Install dependencies (PyTorch, transformers, bitsandbytes)"}),"\n",(0,i.jsxs)(n.li,{children:["Run ",(0,i.jsx)(n.code,{children:"openvla-inference.py"})," with INT8 quantization"]}),"\n",(0,i.jsx)(n.li,{children:"Test with 5 different natural language commands"}),"\n",(0,i.jsx)(n.li,{children:"Measure inference latency and VRAM usage"}),"\n",(0,i.jsx)(n.li,{children:"Compare FP16 vs INT8 performance"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Time budget:"})," 30-45 minutes\n",(0,i.jsx)(n.strong,{children:"Difficulty:"})," Intermediate\n",(0,i.jsx)(n.strong,{children:"Prerequisites:"})," RTX GPU (8+ GB VRAM), Python 3.10+"]})]})]})}function a(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>o});var r=s(6540);const i={},t=r.createContext(i);function l(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);