"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9784],{5761:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"perception-stack/quiz","title":"Chapter 07 Quiz - Perception Stack","description":"Test your understanding of perception systems, RGB-D cameras, VSLAM, object detection, and sensor fusion","source":"@site/docs/perception-stack/quiz.mdx","sourceDirName":"perception-stack","slug":"/perception-stack/quiz","permalink":"/HumanoidRoboticsBook/docs/perception-stack/quiz","draft":false,"unlisted":false,"editUrl":"https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook/tree/main/docs/perception-stack/quiz.mdx","tags":[],"version":"current","sidebarPosition":99,"frontMatter":{"id":"quiz","title":"Chapter 07 Quiz - Perception Stack","sidebar_position":99,"description":"Test your understanding of perception systems, RGB-D cameras, VSLAM, object detection, and sensor fusion"}}');var i=s(4848),l=s(8453);const t={id:"quiz",title:"Chapter 07 Quiz - Perception Stack",sidebar_position:99,description:"Test your understanding of perception systems, RGB-D cameras, VSLAM, object detection, and sensor fusion"},o="Chapter 07 Quiz: Perception Stack",c={},d=[{value:"Question 1: RGB-D Camera Selection",id:"question-1-rgb-d-camera-selection",level:2},{value:"Question 2: Point Cloud Processing",id:"question-2-point-cloud-processing",level:2},{value:"Question 3: VSLAM Tracking Lost",id:"question-3-vslam-tracking-lost",level:2},{value:"Question 4: YOLOv8 Model Selection",id:"question-4-yolov8-model-selection",level:2},{value:"Question 5: Sensor Fusion with EKF",id:"question-5-sensor-fusion-with-ekf",level:2},{value:"Question 6: Grounding DINO vs YOLOv8",id:"question-6-grounding-dino-vs-yolov8",level:2},{value:"Question 7: Perception Latency Budget",id:"question-7-perception-latency-budget",level:2},{value:"Question 8: Production Deployment Optimization",id:"question-8-production-deployment-optimization",level:2},{value:"Score Interpretation",id:"score-interpretation",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function a(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-07-quiz-perception-stack",children:"Chapter 07 Quiz: Perception Stack"})}),"\n",(0,i.jsx)(n.p,{children:"Test your mastery of humanoid robot perception systems!"}),"\n",(0,i.jsx)(n.admonition,{title:"Quiz Instructions",type:"info",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"8 questions covering perception fundamentals through advanced integration"}),"\n",(0,i.jsx)(n.li,{children:"Each question has detailed explanations"}),"\n",(0,i.jsx)(n.li,{children:"Score 80% or higher to demonstrate mastery"}),"\n",(0,i.jsx)(n.li,{children:"Review relevant sections if you score below 80%"}),"\n"]})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-1-rgb-d-camera-selection",children:"Question 1: RGB-D Camera Selection"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"You're building a humanoid for indoor household tasks (navigation, manipulation). Which RGB-D camera is BEST for this application?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) Microsoft Azure Kinect DK (3840x2160, 0.5-5.46m depth range)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Suboptimal - Discontinued Product"})]}),(0,i.jsx)(n.p,{children:"While Azure Kinect has excellent specifications, it's discontinued by Microsoft (2024)."}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Problems:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"No longer manufactured"}),"\n",(0,i.jsx)(n.li,{children:"Limited spare parts availability"}),"\n",(0,i.jsx)(n.li,{children:"Uncertain long-term SDK support"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Better choice:"})," Intel RealSense D455 (active product, strong ecosystem)."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) Intel RealSense D455 (1280x720, 0.6-6m depth range)"}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Correct - Best Choice!"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Intel RealSense D455 is optimal for indoor humanoids:"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key advantages:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth range:"})," 0.6-6m covers typical indoor spaces (rooms, hallways)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 support:"})," Native ",(0,i.jsx)(n.code,{children:"realsense2_camera"})," package"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Price:"})," $329 (best value)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ecosystem:"})," Large community, extensive documentation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU included:"})," Built-in 9-DoF IMU for sensor fusion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Frame rate:"})," 90 FPS (smooth motion)"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Real-world validation:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Used in research humanoids (Berkeley, Stanford projects)"}),"\n",(0,i.jsx)(n.li,{children:"Proven reliability (millions deployed)"}),"\n",(0,i.jsx)(n.li,{children:"Active development (Intel continues updates)"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Alternative:"})," Stereolabs ZED 2i for outdoor use (0.2-20m range) but more expensive ($449)."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) Orbbec Astra Pro (640x480, 0.6-8m depth range)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Budget Option but Limited"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Limitations:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Low resolution (640x480 vs 1280x720)"}),"\n",(0,i.jsx)(n.li,{children:"Lower frame rate (30 FPS)"}),"\n",(0,i.jsx)(n.li,{children:"Community ROS support (not official)"}),"\n",(0,i.jsx)(n.li,{children:"Less accurate depth (compared to RealSense)"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"When to use:"})," Prototyping on tight budget (<$200)."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For production humanoids:"})," RealSense D455 is worth the extra $180."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Stereolabs ZED 2i (4416x1242, 0.2-20m depth range)"}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Excellent but Overkill for Indoor Use"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"ZED 2i strengths:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"20m depth range (outdoor robotics)"}),"\n",(0,i.jsx)(n.li,{children:"4K resolution"}),"\n",(0,i.jsx)(n.li,{children:"120 FPS"}),"\n",(0,i.jsx)(n.li,{children:"Better in sunlight (stereo vs infrared)"}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why not optimal for indoor humanoids:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Price:"})," $449 vs $329 (RealSense)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Overkill range:"})," Indoor scenes rarely >6m"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Higher compute:"})," 4K processing requires more GPU"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"When to use ZED 2i:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Outdoor humanoids"}),"\n",(0,i.jsx)(n.li,{children:"Long-range navigation"}),"\n",(0,i.jsx)(n.li,{children:"Bright sunlight environments"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For indoor household tasks:"})," RealSense D455 is sufficient and cheaper."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-2-point-cloud-processing",children:"Question 2: Point Cloud Processing"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Your point cloud has 921,600 raw points from 1280x720 depth image. After voxel downsampling (1cm voxels) and outlier removal, how many points should remain for real-time processing?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) 500,000-700,000 points (minimal reduction)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Too Many - Will Cause Latency"})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem:"})," 500K+ points are too dense for real-time humanoid control."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Expected latency with 500K points:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Point cloud processing: 80-120ms"}),"\n",(0,i.jsx)(n.li,{children:"Collision checking: 40-60ms"}),"\n",(0,i.jsx)(n.li,{children:"Total: >100ms (violates real-time requirement)"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Target:"})," 50,000-100,000 points for <30ms processing."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) 50,000-100,000 points (90%+ reduction)"}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Correct!"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"50K-100K points is optimal for real-time humanoid perception."})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why this range:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sufficient detail:"})," 1cm voxels preserve object geometry"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time processing:"})," 20-30ms on RTX 4070 Ti"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory efficient:"})," ~2-4 MB (fits in GPU cache)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Standard practice:"})," ORB-SLAM3, navigation stacks use similar density"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Voxel downsampling example:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import open3d as o3d\n\npcd = o3d.geometry.PointCloud()\npcd.points = o3d.utility.Vector3dVector(points)  # 921K points\n\n# Downsample with 1cm voxels\npcd_downsampled = pcd.voxel_down_sample(voxel_size=0.01)\nprint(f"Points after downsampling: {len(pcd_downsampled.points)}")\n# Output: ~70,000 points\n\n# Outlier removal\npcd_filtered, _ = pcd_downsampled.remove_statistical_outlier(\n    nb_neighbors=20,\n    std_ratio=2.0\n)\nprint(f"Points after filtering: {len(pcd_filtered.points)}")\n# Output: ~55,000 points (final)\n'})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Performance:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Latency: 25ms"}),"\n",(0,i.jsx)(n.li,{children:"Memory: 2.6 MB"}),"\n",(0,i.jsx)(n.li,{children:"Perfect for VLA integration (<100ms total perception latency)"}),"\n"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) 5,000-10,000 points (99% reduction)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Too Sparse - Loss of Detail"})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem:"})," <10K points lose important geometric detail."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Issues:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Thin objects disappear (cups, bottles, utensils)"}),"\n",(0,i.jsx)(n.li,{children:"Surface normals inaccurate"}),"\n",(0,i.jsx)(n.li,{children:"Grasping failures (poor contact estimation)"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"When to use sparse clouds:"})," Coarse navigation (not manipulation)."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Keep all 921,600 points (no downsampling)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Unacceptable for Real-Time"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Latency with full-resolution cloud:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Point cloud generation: 60ms"}),"\n",(0,i.jsx)(n.li,{children:"Collision checking: 120ms"}),"\n",(0,i.jsx)(n.li,{children:"VLA processing timeout"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Always downsample"})," for humanoid control."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-3-vslam-tracking-lost",children:"Question 3: VSLAM Tracking Lost"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:'Your ORB-SLAM3 loses tracking and outputs "LOST" state. The robot was moving at 0.8 m/s. What\'s the MOST LIKELY cause?'})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) Camera lens is dirty"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Possible but Unlikely"})]}),(0,i.jsx)(n.p,{children:"Dirty lens causes blurry images but rarely complete tracking loss unless extremely dirty."}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Check:"})," Clean lens and retry. If problem persists, it's motion blur."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) Motion blur from fast movement exceeding VSLAM limits"}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Correct - Most Likely Cause!"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"ORB-SLAM3 fails at high speeds due to motion blur."})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why motion blur breaks VSLAM:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feature matching fails:"})," ORB features can't track between frames"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pose estimation error:"})," RANSAC can't find consistent matches"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tracking lost:"})," System declares LOST state"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Typical speed limits for ORB-SLAM3:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Walking:"})," 0.3-0.5 m/s (safe)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fast walking:"})," 0.6-0.8 m/s (marginal)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Running:"})," >0.8 m/s (tracking loss likely)"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Solutions:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reduce robot speed"})," to <0.5 m/s during navigation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Increase camera frame rate"})," (30 FPS \u2192 60 FPS)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use global shutter camera"})," (avoid rolling shutter artifacts)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Switch to VINS-Fusion"})," (better at high-speed motion with IMU fusion)"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Implementation:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Velocity controller with SLAM-aware speed limits\nclass SafeNavigationController:\n    def __init__(self):\n        self.max_velocity = 0.5  # m/s (VSLAM-safe speed)\n        self.slam_tracking_state = 2  # OK\n\n    def slam_callback(self, msg):\n        self.slam_tracking_state = msg.tracking_state\n        # tracking_state: 0=NOT_INIT, 1=UNSTABLE, 2=OK, 3=LOST\n\n    def compute_velocity(self, target_velocity):\n        if self.slam_tracking_state != 2:  # Not OK\n            return 0.1  # Slow down to regain tracking\n\n        # Clip to SLAM-safe speed\n        return min(target_velocity, self.max_velocity)\n"})})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) Low-texture environment (white walls)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Possible but Speed is More Likely"})]}),(0,i.jsx)(n.p,{children:"Low-texture scenes cause gradual tracking degradation, not sudden LOST at 0.8 m/s."}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"If low texture:"})," Add visual markers (AprilTags, posters)."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) RealSense D455 incompatible with ORB-SLAM3"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Incorrect - Fully Compatible"})]}),(0,i.jsx)(n.p,{children:"RealSense D455 is widely used with ORB-SLAM3. Not a compatibility issue."})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-4-yolov8-model-selection",children:"Question 4: YOLOv8 Model Selection"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"You need real-time object detection (60+ FPS) with maximum accuracy on a single RTX 4070 Ti (12GB VRAM). Which YOLOv8 model should you deploy?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) YOLOv8n (3.2M params, 37.3% mAP, 150 FPS)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Too Fast, Not Accurate Enough"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"YOLOv8n (nano) is too small:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," 37.3% mAP (low for manipulation tasks)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 150 FPS (overkill - 60 FPS is sufficient)"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use case for YOLOv8n:"})," Edge devices (Jetson Nano), prototyping."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For humanoids:"})," Need higher accuracy."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) YOLOv8s (11.2M params, 44.9% mAP, 120 FPS)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Better but Still Suboptimal"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"YOLOv8s (small) is middle ground:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," 44.9% mAP (mediocre)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 120 FPS (faster than needed)"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem:"})," You have 12 GB VRAM budget - use it for better model!"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) YOLOv8l (43.7M params, 52.9% mAP, 60 FPS)"}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Correct - Optimal Balance!"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"YOLOv8l (large) is perfect for RTX 4070 Ti:"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why this is optimal:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," 52.9% mAP (+15.6% vs YOLOv8n)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 60 FPS (meets real-time requirement)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VRAM:"})," 6.5 GB (fits comfortably in 12 GB)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Headroom:"})," 5.5 GB free for other modules (SAM 2, VLA)"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Performance breakdown:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inference:"})," 16ms per frame"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"FPS:"})," 62 FPS"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Batch size:"})," 1 (real-time streaming)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," Excellent for household objects (cups, bottles, etc.)"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Real-world deployment:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n\nmodel = YOLO('yolov8l.pt')  # Load YOLOv8-Large\n\n# Run inference\nresults = model(\n    rgb_image,\n    conf=0.5,      # Confidence threshold\n    device='cuda',  # GPU acceleration\n    verbose=False\n)\n\n# Expected performance on RTX 4070 Ti:\n# - Latency: 15-17ms\n# - FPS: 58-62\n# - VRAM: 6.5 GB\n# - Accuracy: 52.9% mAP\n"})})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) YOLOv8x (68.2M params, 53.9% mAP, 40 FPS)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Too Slow for Real-Time"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"YOLOv8x (extra-large) is overkill:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," 53.9% mAP (+1.0% vs YOLOv8l - marginal gain)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 40 FPS (violates 60 FPS requirement)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VRAM:"})," 9.5 GB (leaves only 2.5 GB for other modules)"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"When to use YOLOv8x:"})," Offline analysis, accuracy-critical research (not real-time humanoids)."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-5-sensor-fusion-with-ekf",children:"Question 5: Sensor Fusion with EKF"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"In visual-inertial odometry fusion, the IMU runs at 200 Hz and camera at 30 Hz. What's the correct fusion strategy?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) Downsample IMU to 30 Hz to match camera frequency"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Wrong - Loses High-Frequency Information"})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem:"})," IMU provides critical high-frequency motion information."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why you need 200 Hz IMU:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Detect fast rotations (turning head)"}),"\n",(0,i.jsx)(n.li,{children:"Capture vibrations, impacts"}),"\n",(0,i.jsx)(n.li,{children:"Smooth pose estimation between camera frames"}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Never downsample IMU!"})})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) Use IMU for prediction (200 Hz) and camera for correction (30 Hz) in EKF"}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Correct - Standard EKF Fusion!"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"This is the correct visual-inertial fusion strategy:"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How it works:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Prediction step (200 Hz):"})," IMU provides high-frequency pose estimates"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Accelerometer: Linear acceleration"}),"\n",(0,i.jsx)(n.li,{children:"Gyroscope: Angular velocity"}),"\n",(0,i.jsx)(n.li,{children:"Integrate to get position, orientation"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Update step (30 Hz):"})," Camera corrects accumulated IMU drift"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Visual features: True position from scene matching"}),"\n",(0,i.jsx)(n.li,{children:"Corrects IMU bias, drift errors"}),"\n"]}),"\n"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"EKF state:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"State (16D):\n- Position: x, y, z\n- Velocity: vx, vy, vz\n- Orientation: qw, qx, qy, qz (quaternion)\n- IMU biases: ba_x, ba_y, ba_z, bg_x, bg_y, bg_z\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Benefits:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Smooth trajectory:"})," 200 Hz output (not jerky 30 Hz)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Drift-free:"})," Camera prevents long-term IMU drift"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robust:"})," Works during camera occlusion (IMU carries pose)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Low-latency:"})," Instant IMU response, periodic camera corrections"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"ROS 2 implementation:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# robot_localization EKF config\nekf_filter_node:\n  ros__parameters:\n    frequency: 200.0  # Output frequency (IMU rate)\n\n    # IMU: High-frequency predictions\n    imu0: /camera/imu\n    imu0_config: [false, false, false,    # x, y, z (position)\n                  true, true, true,        # roll, pitch, yaw (orientation)\n                  false, false, false,     # vx, vy, vz\n                  true, true, true,        # angular velocities\n                  true, true, true]        # linear accelerations\n\n    # Camera odometry: Low-frequency corrections\n    odom0: /orbslam3/odom\n    odom0_config: [true, true, true,      # x, y, z (position)\n                   false, false, false,   # roll, pitch, yaw\n                   false, false, false,   # velocities\n                   false, false, false,   # angular velocities\n                   false, false, false]   # accelerations\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Result:"})," Best of both sensors - IMU frequency, camera accuracy."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) Use camera for position, IMU for orientation (separate estimates)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Suboptimal - No Fusion"})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem:"})," Not actually fusing sensors - just using them separately."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Issues:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"No drift correction"}),"\n",(0,i.jsx)(n.li,{children:"Jumps when switching between sources"}),"\n",(0,i.jsx)(n.li,{children:"No uncertainty quantification"}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Use EKF to properly fuse!"})})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Upsample camera to 200 Hz using interpolation"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Wrong - Creates Artificial Data"})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem:"})," Interpolation doesn't add real information."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Result:"})," Smooth but inaccurate pose between camera frames."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Better:"})," Use IMU prediction for real high-frequency estimates."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-6-grounding-dino-vs-yolov8",children:"Question 6: Grounding DINO vs YOLOv8"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:'You need to detect "the red cup on the left" (not just any cup). Which model should you use?'})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) YOLOv8 with custom fine-tuning on red cups"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Overkill and Limited"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Problems:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Need training data (100+ red cup images)"}),"\n",(0,i.jsx)(n.li,{children:'Can\'t handle "on the left" (spatial reasoning)'}),"\n",(0,i.jsx)(n.li,{children:"Separate model per color (red cup, blue cup, green cup...)"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Better:"})," Use open-vocabulary detection."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:'B) Grounding DINO with text prompt "red cup on the left"'}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Correct - Best for Fine-Grained Queries!"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Grounding DINO excels at fine-grained, compositional queries:"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why this works:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Open-vocabulary:"})," Detects ANY object from text description"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Color understanding:"}),' "red cup" vs "blue cup" (no training needed)']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Spatial reasoning:"}),' "on the left" narrows candidates']}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Implementation:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from groundingdino.util.inference import Model\n\nmodel = Model(\n    model_config_path="GroundingDINO_SwinT_OGC.py",\n    model_checkpoint_path="groundingdino_swint_ogc.pth"\n)\n\n# Fine-grained query\nTEXT_PROMPT = "red cup on the left"\n\ndetections = model.predict_with_classes(\n    image=rgb_image,\n    classes=[TEXT_PROMPT],\n    box_threshold=0.35,\n    text_threshold=0.25\n)\n\n# Returns: Bounding boxes for red cups in left half of image\n'})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Advantages over YOLOv8:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"No training data required"}),"\n",(0,i.jsx)(n.li,{children:'Handles novel objects ("smartphone with cracked screen")'}),"\n",(0,i.jsx)(n.li,{children:'Compositional reasoning ("red cup next to blue bottle")'}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Trade-off:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 150ms (vs 16ms for YOLOv8)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use case:"})," Fine-grained queries, not real-time tracking"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Hybrid approach:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"YOLOv8:"})," Fast general detection (60 FPS)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grounding DINO:"})," Refine detections when needed (specific color, location)"]}),"\n"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) YOLOv8 + color filtering in post-processing"}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Workable Alternative"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Approach:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"YOLOv8 detects all cups"}),"\n",(0,i.jsx)(n.li,{children:"Post-process: Filter by red HSV range"}),"\n",(0,i.jsx)(n.li,{children:"Select leftmost red cup"}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Pros:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Faster than Grounding DINO"}),"\n",(0,i.jsx)(n.li,{children:"No additional models"}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Cons:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Brittle to lighting changes"}),"\n",(0,i.jsx)(n.li,{children:'Hard to handle complex queries ("metallic cup")'}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"When to use:"})," Simple color filtering, speed-critical applications."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) SAM 2 for segmentation, then color analysis"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Wrong Tool"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"SAM 2 segments but doesn't detect:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Needs bounding box prompt (from YOLO or Grounding DINO first)"}),"\n",(0,i.jsx)(n.li,{children:'Can\'t understand "red" without external color analysis'}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Use SAM 2 after detection, not for detection."})})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-7-perception-latency-budget",children:"Question 7: Perception Latency Budget"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Your VLA model requires perception results within 100ms. You have YOLOv8 (16ms), depth processing (25ms), and point cloud filtering (18ms). What's the maximum allowed latency for camera capture + preprocessing?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) 41ms (100 - 16 - 25 - 18)"}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Correct!"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Latency breakdown:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Total budget:     100ms (VLA requirement)\n\nPerception pipeline:\n- Camera capture: <5ms (hardware)\n- Preprocessing:  X ms (solve for this)\n- YOLOv8:        16ms (given)\n- Depth proc:    25ms (given)\n- Point cloud:   18ms (given)\n- Overhead:      ~10ms (ROS messaging, scheduling)\n\nTotal: 5 + X + 16 + 25 + 18 + 10 = 74 + X \u2264 100\n\nSolution: X \u2264 26ms\n\nAnswer: 41ms margin total (preprocessing + buffer)\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Preprocessing tasks (<20ms target):"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Image rectification: 3-5ms"}),"\n",(0,i.jsx)(n.li,{children:"Depth alignment: 4-6ms"}),"\n",(0,i.jsx)(n.li,{children:"Noise filtering: 2-3ms"}),"\n",(0,i.jsx)(n.li,{children:"Format conversion: 1-2ms"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Total preprocessing: ~15ms"})," (well within budget)"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Practical allocation:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Camera: 5ms"}),"\n",(0,i.jsx)(n.li,{children:"Preprocessing: 15ms"}),"\n",(0,i.jsx)(n.li,{children:"YOLOv8: 16ms"}),"\n",(0,i.jsx)(n.li,{children:"Depth: 25ms"}),"\n",(0,i.jsx)(n.li,{children:"Point cloud: 18ms"}),"\n",(0,i.jsx)(n.li,{children:"Overhead: 10ms"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total: 89ms"})," (11ms safety margin)"]}),"\n"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) 100ms (no other constraints)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Incorrect - Ignores Other Modules"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"You can't use all 100ms for one component!"})}),(0,i.jsx)(n.p,{children:"Must account for all pipeline stages + overhead."})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) 59ms (100 - 16 - 25)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Forgot Point Cloud Filtering"})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Missing:"})," 18ms for point cloud filtering."]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Correct:"})," 100 - 16 - 25 - 18 = 41ms"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) 20ms (assume 20% of total budget)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Arbitrary Allocation"})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Don't use percentages"})," - measure actual component latencies and allocate precisely."]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-8-production-deployment-optimization",children:"Question 8: Production Deployment Optimization"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"You're deploying perception on a physical humanoid with RTX 4070 Ti. Current performance: YOLOv8l runs at 38 FPS (target: 60 FPS). What's the BEST optimization?"})}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"A) Switch to YOLOv8m (smaller model)"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Loses Accuracy Unnecessarily"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"YOLOv8m trade-off:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 80 FPS (good)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," 50.2% mAP (-2.7% vs YOLOv8l)"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem:"})," You have GPU headroom - optimize before sacrificing accuracy."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"B) Export YOLOv8l to TensorRT engine"}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Correct - Best Optimization!"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"TensorRT provides 2-3x speedup with zero accuracy loss:"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How it works:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Fuses layers (conv + batch norm + activation)"}),"\n",(0,i.jsx)(n.li,{children:"Optimizes CUDA kernels for RTX GPU"}),"\n",(0,i.jsx)(n.li,{children:"Reduces memory bandwidth"}),"\n",(0,i.jsx)(n.li,{children:"Enables FP16 precision (optional)"}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Implementation:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n\n# Export to TensorRT (one-time)\nmodel = YOLO('yolov8l.pt')\nmodel.export(format='engine')  # Creates yolov8l.engine\n\n# Load TensorRT model for inference\ntrt_model = YOLO('yolov8l.engine')\n\n# Inference (2-3x faster)\nresults = trt_model(rgb_image)\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Performance gains on RTX 4070 Ti:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Before (PyTorch):"})," 38 FPS (26ms latency)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"After (TensorRT):"})," 95 FPS (10.5ms latency)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speedup:"})," 2.5x"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," Identical (52.9% mAP)"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why this is optimal:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"No accuracy loss"}),"\n",(0,i.jsx)(n.li,{children:"Maximum performance from available GPU"}),"\n",(0,i.jsx)(n.li,{children:"One-time export (5 minutes)"}),"\n",(0,i.jsx)(n.li,{children:"Standard practice for production deployment"}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Other TensorRT benefits:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Lower power consumption (important for battery-powered robots)"}),"\n",(0,i.jsx)(n.li,{children:"Reduced thermal throttling"}),"\n",(0,i.jsx)(n.li,{children:"Better multi-stream performance"}),"\n"]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"C) Reduce input resolution from 640x640 to 416x416"}),(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Also Effective but Trades Accuracy"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Lower resolution trade-off:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 65-75 FPS (faster)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accuracy:"})," 48-49% mAP (-4% for small objects)"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"When to use:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"After TensorRT optimization (if still not fast enough)"}),"\n",(0,i.jsx)(n.li,{children:"Objects are large (no small object detection needed)"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Better:"})," Try TensorRT first (zero accuracy cost)."]})]}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsx)("summary",{children:"D) Run YOLOv8 on CPU to free GPU for VLA"}),(0,i.jsxs)(n.p,{children:["\u274c ",(0,i.jsx)(n.strong,{children:"Terrible Performance"})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"CPU inference:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speed:"})," 2-5 FPS (vs 38 FPS GPU)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Unusable"})," for real-time control"]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Never run perception on CPU"})," if GPU is available!"]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"score-interpretation",children:"Score Interpretation"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Calculate your score:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["8/8 correct: ",(0,i.jsx)(n.strong,{children:"Expert"})," - Ready for production perception deployment"]}),"\n",(0,i.jsxs)(n.li,{children:["6-7/8 correct: ",(0,i.jsx)(n.strong,{children:"Proficient"})," - Strong understanding, minor gaps"]}),"\n",(0,i.jsxs)(n.li,{children:["4-5/8 correct: ",(0,i.jsx)(n.strong,{children:"Developing"})," - Review key perception sections"]}),"\n",(0,i.jsxs)(n.li,{children:["0-3/8 correct: ",(0,i.jsx)(n.strong,{children:"Needs Review"})," - Re-read Chapter 07"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsx)(n.p,{children:"If you struggled with specific topics, review these sections:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Question"}),(0,i.jsx)(n.th,{children:"Topic"}),(0,i.jsx)(n.th,{children:"Review Section"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q1"}),(0,i.jsx)(n.td,{children:"Camera Selection"}),(0,i.jsx)(n.td,{children:"RGB-D Camera Setup"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q2"}),(0,i.jsx)(n.td,{children:"Point Cloud Processing"}),(0,i.jsx)(n.td,{children:"Depth Processing and Point Clouds"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q3"}),(0,i.jsx)(n.td,{children:"VSLAM Debugging"}),(0,i.jsx)(n.td,{children:"Visual SLAM (VSLAM) + Debugging"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q4"}),(0,i.jsx)(n.td,{children:"Model Selection"}),(0,i.jsx)(n.td,{children:"Object Detection and Segmentation"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q5"}),(0,i.jsx)(n.td,{children:"Sensor Fusion"}),(0,i.jsx)(n.td,{children:"Sensor Fusion + EKF"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q6"}),(0,i.jsx)(n.td,{children:"Detection Methods"}),(0,i.jsx)(n.td,{children:"Grounding DINO vs YOLOv8"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q7"}),(0,i.jsx)(n.td,{children:"Latency Budgets"}),(0,i.jsx)(n.td,{children:"Performance Optimization"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Q8"}),(0,i.jsx)(n.td,{children:"Production Optimization"}),(0,i.jsx)(n.td,{children:"Performance Optimization"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"After completing this quiz:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Score >= 80%:"})," Proceed to Chapter 08 (Bipedal Locomotion)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Score < 80%:"})," Review Chapter 07 sections and retry quiz"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hands-on:"})," Set up RealSense D455 and run perception stack"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advanced:"})," Integrate perception with VLA for end-to-end pipeline"]}),"\n"]}),"\n",(0,i.jsxs)(n.admonition,{title:"Practice Challenge",type:"tip",children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Build complete perception system:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up Intel RealSense D455 on ROS 2"}),"\n",(0,i.jsx)(n.li,{children:"Run ORB-SLAM3 for localization"}),"\n",(0,i.jsx)(n.li,{children:"Deploy YOLOv8l for object detection (>85% accuracy)"}),"\n",(0,i.jsx)(n.li,{children:"Generate point clouds with voxel downsampling"}),"\n",(0,i.jsx)(n.li,{children:"Measure total perception latency (<100ms target)"}),"\n",(0,i.jsx)(n.li,{children:"Visualize all outputs in RViz2"}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Time budget:"})," 3-4 hours\n",(0,i.jsx)(n.strong,{children:"Difficulty:"})," Intermediate-Advanced\n",(0,i.jsx)(n.strong,{children:"Hardware:"})," RealSense D455, RTX 4070 Ti, Ubuntu 22.04"]})]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>o});var r=s(6540);const i={},l=r.createContext(i);function t(e){const n=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);