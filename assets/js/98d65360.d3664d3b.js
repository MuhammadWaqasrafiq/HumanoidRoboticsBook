"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[4684],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(a.Provider,{value:n},e.children)}},9240:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"perception-stack/index","title":"7. Perception Stack \u2013 VSLAM, Depth, Segmentation","description":"This chapter will delve into the critical role of the perception stack in physical AI and humanoid robotics. We will explore key technologies such as Visual Simultaneous Localization and Mapping (VSLAM), depth sensing, and semantic segmentation, understanding how robots perceive and interpret their environment.","source":"@site/docs/07-perception-stack/index.mdx","sourceDirName":"07-perception-stack","slug":"/perception-stack/","permalink":"/HumanoidRoboticsBook/docs/perception-stack/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/07-perception-stack/index.mdx","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"7. Perception Stack \u2013 VSLAM, Depth, Segmentation"},"sidebar":"tutorialSidebar","previous":{"title":"6. NVIDIA Isaac Platform Deep Dive","permalink":"/HumanoidRoboticsBook/docs/isaac-platform-deep-dive/"},"next":{"title":"8. Bipedal Locomotion and Balance Control","permalink":"/HumanoidRoboticsBook/docs/bipedal-locomotion/"}}');var o=t(4848),a=t(8453);const s={sidebar_position:7,title:"7. Perception Stack \u2013 VSLAM, Depth, Segmentation"},r="7. Perception Stack \u2013 VSLAM, Depth, Segmentation",c={},l=[{value:"Visual SLAM (VSLAM)",id:"visual-slam-vslam",level:2},{value:"Depth Sensing",id:"depth-sensing",level:2},{value:"Semantic Segmentation",id:"semantic-segmentation",level:2},{value:"Further Exploration",id:"further-exploration",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"7-perception-stack--vslam-depth-segmentation",children:"7. Perception Stack \u2013 VSLAM, Depth, Segmentation"})}),"\n",(0,o.jsx)(n.p,{children:"This chapter will delve into the critical role of the perception stack in physical AI and humanoid robotics. We will explore key technologies such as Visual Simultaneous Localization and Mapping (VSLAM), depth sensing, and semantic segmentation, understanding how robots perceive and interpret their environment."}),"\n",(0,o.jsx)(n.h2,{id:"visual-slam-vslam",children:"Visual SLAM (VSLAM)"}),"\n",(0,o.jsx)(n.p,{children:"VSLAM allows robots to simultaneously map their surroundings and localize themselves within that map using visual input."}),"\n",(0,o.jsx)(n.h2,{id:"depth-sensing",children:"Depth Sensing"}),"\n",(0,o.jsx)(n.p,{children:"Understanding the 3D structure of the environment is crucial. This section will cover various depth sensing technologies like stereo vision, time-of-flight (ToF) cameras, and structured light sensors."}),"\n",(0,o.jsx)(n.h2,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,o.jsx)(n.p,{children:"Semantic segmentation enables robots to classify each pixel in an image, allowing them to identify and understand different objects and regions in their environment."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Raw Sensor Data] --\x3e B{VSLAM}\n    A --\x3e C{Depth Sensors}\n    A --\x3e D{Semantic Segmentation}\n    B --\x3e E[Environmental Map & Localization]\n    C --\x3e E\n    D --\x3e E\n    E --\x3e F[Perceived Environment]\n"})}),"\n",(0,o.jsx)(n.h2,{id:"further-exploration",children:"Further Exploration"}),"\n",(0,o.jsx)(n.p,{children:"(Content to be added later)"})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);