"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[8724],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var i=t(6540);const r={},o=i.createContext(r);function s(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(o.Provider,{value:n},e.children)}},9644:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"voice-to-action/index","title":"Chapter 11 - Voice-to-Action Pipeline","description":"Build complete voice-controlled humanoid robot pipeline - Whisper ASR \u2192 LLM \u2192 VLA \u2192 ROS 2 actions in &lt;3 seconds","source":"@site/docs/voice-to-action/index.mdx","sourceDirName":"voice-to-action","slug":"/voice-to-action/","permalink":"/HumanoidRoboticsBook/docs/voice-to-action/","draft":false,"unlisted":false,"editUrl":"https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook/tree/main/docs/voice-to-action/index.mdx","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"id":"index","title":"Chapter 11 - Voice-to-Action Pipeline","sidebar_position":11,"description":"Build complete voice-controlled humanoid robot pipeline - Whisper ASR \u2192 LLM \u2192 VLA \u2192 ROS 2 actions in &lt;3 seconds","keywords":["voice-control","whisper","speech-recognition","llm","voice-to-action","end-to-end-pipeline"]},"sidebar":"tutorialSidebar","previous":{"title":"10. Vision-Language-Action Models","permalink":"/HumanoidRoboticsBook/docs/vla-models/"},"next":{"title":"12. Sim-to-Real Transfer","permalink":"/HumanoidRoboticsBook/docs/sim-to-real/"}}');var r=t(4848),o=t(8453);const s={id:"index",title:"Chapter 11 - Voice-to-Action Pipeline",sidebar_position:11,description:"Build complete voice-controlled humanoid robot pipeline - Whisper ASR \u2192 LLM \u2192 VLA \u2192 ROS 2 actions in &lt;3 seconds",keywords:["voice-control","whisper","speech-recognition","llm","voice-to-action","end-to-end-pipeline"]},a="Chapter 11: Voice-to-Action Pipeline",c={},l=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"The Voice-to-Action Challenge",id:"the-voice-to-action-challenge",level:2},{value:"Voice-to-Action Pipeline Architecture",id:"voice-to-action-pipeline-architecture",level:2},{value:"Stage 1: Audio Capture",id:"stage-1-audio-capture",level:2},{value:"Microphone Setup",id:"microphone-setup",level:3},{value:"Python Audio Capture",id:"python-audio-capture",level:3},{value:"Stage 2: Speech Recognition with Whisper",id:"stage-2-speech-recognition-with-whisper",level:2},{value:"Why Whisper?",id:"why-whisper",level:3},{value:"Whisper Model Sizes",id:"whisper-model-sizes",level:3},{value:"Whisper Implementation",id:"whisper-implementation",level:3},{value:"Stage 3: Command Parsing with LLM",id:"stage-3-command-parsing-with-llm",level:2},{value:"LLM Parsing Options",id:"llm-parsing-options",level:3},{value:"Llama 2-7B Parsing",id:"llama-2-7b-parsing",level:3},{value:"Stage 4: VLA Action Generation",id:"stage-4-vla-action-generation",level:2},{value:"Stage 5: Complete End-to-End Pipeline",id:"stage-5-complete-end-to-end-pipeline",level:2},{value:"Testing &amp; Evaluation",id:"testing--evaluation",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-11-voice-to-action-pipeline",children:"Chapter 11: Voice-to-Action Pipeline"})}),"\n",(0,r.jsx)(n.admonition,{title:"Chapter Overview",type:"info",children:(0,r.jsx)(n.p,{children:"Build the complete voice-controlled robot system: speak commands \u2192 robot executes tasks. Integrate Whisper (speech recognition), LLM (command parsing), VLA models (action generation), and ROS 2 (robot control) into a <3 second end-to-end pipeline."})}),"\n",(0,r.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you'll be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Integrate Whisper for real-time speech recognition (<500ms latency)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Use LLMs (Llama 2, GPT-4) for natural language command parsing"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Connect voice \u2192 VLA \u2192 ROS 2 in complete end-to-end pipeline"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Achieve <3 second latency (speech \u2192 robot action start)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Handle error cases (speech recognition failures, ambiguous commands)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Deploy on consumer hardware (RTX 4070 Ti + microphone)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Test with 20 real voice commands and measure success rate"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Optimize pipeline for production deployment"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before starting this chapter, you should:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 Complete ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"../vla-models",children:"Chapter 10: VLA Models"})})," (OpenVLA, GR00T N1)"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 Complete ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"../isaac-platform",children:"Chapter 06: Isaac Platform"})})," (ROS 2 integration)"]}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Have Python 3.10+, PyTorch 2.0+, ROS 2 Humble installed"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Have RTX GPU with 12+ GB VRAM (for VLA + Whisper)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Have working microphone (USB or built-in)"}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{title:"Hardware Requirements",type:"tip",children:(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Minimum:"})," RTX 4060 Ti (16 GB) + USB microphone\n",(0,r.jsx)(n.strong,{children:"Recommended:"})," RTX 4070 Ti (12 GB) + quality microphone\n",(0,r.jsx)(n.strong,{children:"Professional:"})," RTX 4080/4090 + professional audio interface"]})}),"\n",(0,r.jsx)(n.h2,{id:"the-voice-to-action-challenge",children:"The Voice-to-Action Challenge"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," Traditional robot programming requires coding every command:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Traditional approach - manual programming\nif user_says("pick up cup"):\n    robot.move_to_object("cup")\n    robot.grasp()\n    robot.lift()\nelif user_says("place on table"):\n    robot.move_to_location("table")\n    robot.release()\n# ... hundreds of if-else statements\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"VLA Solution:"})," End-to-end learning from voice to actions:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Modern approach - voice-to-action pipeline\naudio = microphone.record()\ntext = whisper.transcribe(audio)  # "pick up the red cup"\nactions = vla_model(camera_image, text)  # [joint positions]\nrobot.execute(actions)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"voice-to-action-pipeline-architecture",children:"Voice-to-Action Pipeline Architecture"}),"\n",(0,r.jsx)(n.p,{children:"The complete pipeline has 5 stages:"}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart LR\n    subgraph "Stage 1: Audio Capture"\n        A1[Microphone]\n        A2[Audio Buffer<br/>16kHz WAV]\n    end\n\n    subgraph "Stage 2: Speech Recognition"\n        S1[Whisper Model<br/>Base/Medium/Large]\n        S2[Transcribed Text<br/>\'pick up red cup\']\n    end\n\n    subgraph "Stage 3: Command Parsing"\n        L1[LLM<br/>Llama 2-7B / GPT-4]\n        L2[Structured Command<br/>JSON format]\n    end\n\n    subgraph "Stage 4: Action Generation"\n        V1[VLA Model<br/>OpenVLA / GR00T N1]\n        V2[Robot Actions<br/>7-DoF vector]\n    end\n\n    subgraph "Stage 5: Robot Execution"\n        R1[ROS 2 Publisher]\n        R2[Robot Hardware]\n    end\n\n    A1 --\x3e A2\n    A2 --\x3e S1\n    S1 --\x3e S2\n    S2 --\x3e L1\n    L1 --\x3e L2\n    L2 --\x3e V1\n    V1 --\x3e V2\n    V2 --\x3e R1\n    R1 --\x3e R2\n\n    style S1 fill:#00adef\n    style L1 fill:#ffcc00\n    style V1 fill:#76b900\n    style R2 fill:#9966ff'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Latency Budget (Target: <3 seconds):"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Stage"}),(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Target Latency"}),(0,r.jsx)(n.th,{children:"Typical Latency"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"1"}),(0,r.jsx)(n.td,{children:"Audio Capture"}),(0,r.jsx)(n.td,{children:"Streaming"}),(0,r.jsx)(n.td,{children:"50-100ms buffer"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"2"}),(0,r.jsx)(n.td,{children:"Whisper ASR"}),(0,r.jsx)(n.td,{children:"<500ms"}),(0,r.jsx)(n.td,{children:"200-400ms (base), 400-800ms (large)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"3"}),(0,r.jsx)(n.td,{children:"LLM Parsing"}),(0,r.jsx)(n.td,{children:"<1000ms"}),(0,r.jsx)(n.td,{children:"500-1500ms (depends on model)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4"}),(0,r.jsx)(n.td,{children:"VLA Inference"}),(0,r.jsx)(n.td,{children:"<500ms"}),(0,r.jsx)(n.td,{children:"80-200ms (OpenVLA INT8)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"5"}),(0,r.jsx)(n.td,{children:"ROS 2 Publish"}),(0,r.jsx)(n.td,{children:"<50ms"}),(0,r.jsx)(n.td,{children:"10-30ms"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Total"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"End-to-End"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"<3000ms"})}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.strong,{children:"840-2630ms"})," \u2705"]})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"stage-1-audio-capture",children:"Stage 1: Audio Capture"}),"\n",(0,r.jsx)(n.h3,{id:"microphone-setup",children:"Microphone Setup"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sample Rate:"})," 16 kHz (Whisper's native rate)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Channels:"})," Mono (single microphone)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," 16-bit PCM WAV"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency:"})," <100ms buffering"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Hardware recommendations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Budget:"})," Built-in laptop mic or USB webcam"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mid-range:"})," Blue Yeti USB microphone ($100)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Professional:"})," Audio-Technica AT2020 + audio interface ($150-300)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"python-audio-capture",children:"Python Audio Capture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport wave\nimport numpy as np\n\nclass AudioCapture:\n    def __init__(self, rate=16000, channels=1, chunk=1024):\n        self.rate = rate\n        self.channels = channels\n        self.chunk = chunk\n        self.audio = pyaudio.PyAudio()\n\n    def record(self, duration=5.0):\n        """\n        Record audio for specified duration\n\n        Args:\n            duration: Recording time in seconds\n\n        Returns:\n            audio_data: numpy array of audio samples\n        """\n        stream = self.audio.open(\n            format=pyaudio.paInt16,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        print(f"\ud83c\udfa4 Recording for {duration} seconds...")\n\n        frames = []\n        for _ in range(int(self.rate / self.chunk * duration)):\n            data = stream.read(self.chunk)\n            frames.append(np.frombuffer(data, dtype=np.int16))\n\n        stream.stop_stream()\n        stream.close()\n\n        audio_data = np.concatenate(frames)\n        print(f"\u2705 Recorded {len(audio_data)} samples")\n\n        return audio_data\n\n    def save_wav(self, audio_data, filename="command.wav"):\n        """Save audio to WAV file"""\n        with wave.open(filename, \'wb\') as wf:\n            wf.setnchannels(self.channels)\n            wf.setsampwidth(self.audio.get_sample_size(pyaudio.paInt16))\n            wf.setframerate(self.rate)\n            wf.writeframes(audio_data.tobytes())\n\n        print(f"\ud83d\udcbe Saved to {filename}")\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Usage:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'mic = AudioCapture()\naudio = mic.record(duration=3.0)\nmic.save_wav(audio, "robot_command.wav")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"stage-2-speech-recognition-with-whisper",children:"Stage 2: Speech Recognition with Whisper"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Whisper"})," (OpenAI) is the state-of-the-art speech recognition model."]}),"\n",(0,r.jsx)(n.h3,{id:"why-whisper",children:"Why Whisper?"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Multilingual:"})," Supports 99 languages (English, Urdu, Hindi, Arabic, etc.)\n\u2705 ",(0,r.jsx)(n.strong,{children:"Robust:"})," Works in noisy environments\n\u2705 ",(0,r.jsx)(n.strong,{children:"Fast:"})," Base model <400ms on RTX GPU\n\u2705 ",(0,r.jsx)(n.strong,{children:"Open-source:"})," MIT license, runs locally\n\u2705 ",(0,r.jsx)(n.strong,{children:"Accurate:"})," 95%+ word accuracy on clear speech"]}),"\n",(0,r.jsx)(n.h3,{id:"whisper-model-sizes",children:"Whisper Model Sizes"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Parameters"}),(0,r.jsx)(n.th,{children:"VRAM"}),(0,r.jsx)(n.th,{children:"Speed (RTX 4070 Ti)"}),(0,r.jsx)(n.th,{children:"Accuracy"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Tiny"}),(0,r.jsx)(n.td,{children:"39M"}),(0,r.jsx)(n.td,{children:"1 GB"}),(0,r.jsx)(n.td,{children:"~100ms"}),(0,r.jsx)(n.td,{children:"Good (88%)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Base"}),(0,r.jsx)(n.td,{children:"74M"}),(0,r.jsx)(n.td,{children:"1 GB"}),(0,r.jsx)(n.td,{children:"~200ms"}),(0,r.jsx)(n.td,{children:"Better (92%)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Small"}),(0,r.jsx)(n.td,{children:"244M"}),(0,r.jsx)(n.td,{children:"2 GB"}),(0,r.jsx)(n.td,{children:"~400ms"}),(0,r.jsx)(n.td,{children:"Great (95%)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Medium"}),(0,r.jsx)(n.td,{children:"769M"}),(0,r.jsx)(n.td,{children:"5 GB"}),(0,r.jsx)(n.td,{children:"~800ms"}),(0,r.jsx)(n.td,{children:"Excellent (97%)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Large"}),(0,r.jsx)(n.td,{children:"1.5B"}),(0,r.jsx)(n.td,{children:"10 GB"}),(0,r.jsx)(n.td,{children:"~1500ms"}),(0,r.jsx)(n.td,{children:"Best (98%)"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommendation:"})," ",(0,r.jsx)(n.strong,{children:"Whisper Base"})," for robotics (200ms, 92% accuracy - best speed/accuracy trade-off)."]}),"\n",(0,r.jsx)(n.h3,{id:"whisper-implementation",children:"Whisper Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import whisper\nimport numpy as np\n\nclass WhisperASR:\n    def __init__(self, model_size="base", device="cuda"):\n        """\n        Initialize Whisper ASR\n\n        Args:\n            model_size: "tiny", "base", "small", "medium", "large"\n            device: "cuda" or "cpu"\n        """\n        print(f"[INFO] Loading Whisper {model_size} model...")\n        self.model = whisper.load_model(model_size, device=device)\n        self.device = device\n        print(f"[SUCCESS] Whisper loaded on {device}")\n\n    def transcribe(self, audio_path_or_array, language="en"):\n        """\n        Transcribe audio to text\n\n        Args:\n            audio_path_or_array: Path to WAV file or numpy array\n            language: Language code ("en", "ur", "hi", etc.)\n\n        Returns:\n            text: Transcribed text string\n            confidence: Confidence score (0-1)\n        """\n        result = self.model.transcribe(\n            audio_path_or_array,\n            language=language,\n            fp16=(self.device == "cuda")  # Use FP16 on GPU\n        )\n\n        text = result["text"].strip()\n        segments = result["segments"]\n\n        # Calculate average confidence\n        if segments:\n            confidence = np.mean([seg.get("confidence", 0.0) for seg in segments if "confidence" in seg])\n        else:\n            confidence = 0.0\n\n        return text, confidence\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Usage:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'asr = WhisperASR(model_size="base")\ntext, conf = asr.transcribe("robot_command.wav")\n\nprint(f"\ud83d\udcdd Transcription: \'{text}\'")\nprint(f"\ud83c\udfaf Confidence: {conf:.2%}")\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\ud83d\udcdd Transcription: 'pick up the red cup and place it on the table'\n\ud83c\udfaf Confidence: 94.3%\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"stage-3-command-parsing-with-llm",children:"Stage 3: Command Parsing with LLM"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," Raw transcription needs to be structured for robot execution."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Raw:"})," ",(0,r.jsx)(n.code,{children:'"pick up the red cup and place it on the table"'}),"\n",(0,r.jsx)(n.strong,{children:"Structured:"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "action": "pick_and_place",\n  "object": "cup",\n  "object_properties": {"color": "red"},\n  "destination": "table"\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"llm-parsing-options",children:"LLM Parsing Options"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Option 1: Local LLM (Llama 2-7B)"})," - Fast, private, no API cost\n",(0,r.jsx)(n.strong,{children:"Option 2: Cloud API (GPT-4)"})," - More capable, API cost"]}),"\n",(0,r.jsx)(n.h3,{id:"llama-2-7b-parsing",children:"Llama 2-7B Parsing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from transformers import AutoTokenizer, AutoModelForCausalLM\nimport json\n\nclass CommandParser:\n    def __init__(self, model_name="meta-llama/Llama-2-7b-chat-hf"):\n        print(f"[INFO] Loading {model_name}...")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype="float16",\n            device_map="auto"\n        )\n        print("[SUCCESS] LLM loaded")\n\n    def parse_command(self, text):\n        """\n        Parse natural language command into structured JSON\n\n        Args:\n            text: Transcribed voice command\n\n        Returns:\n            command_dict: Structured command dictionary\n        """\n        prompt = f"""[INST] You are a robot command parser. Convert the user\'s natural language command into a JSON object with these fields:\n- action: one of [pick_up, place, move, grasp, release, navigate]\n- object: target object name\n- object_properties: dict with color, size, etc.\n- destination: target location (if applicable)\n\nUser command: "{text}"\n\nOutput only valid JSON, nothing else. [/INST]"""\n\n        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)\n\n        outputs = self.model.generate(\n            **inputs,\n            max_new_tokens=256,\n            temperature=0.1,  # Low temperature for deterministic parsing\n            do_sample=False\n        )\n\n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # Extract JSON from response\n        try:\n            # Find JSON object in response\n            json_start = response.find("{")\n            json_end = response.rfind("}") + 1\n            json_str = response[json_start:json_end]\n\n            command_dict = json.loads(json_str)\n            return command_dict\n        except (json.JSONDecodeError, ValueError) as e:\n            print(f"[ERROR] Failed to parse LLM output: {e}")\n            return {"action": "unknown", "object": text}\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Usage:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'parser = CommandParser()\ncommand = parser.parse_command("pick up the red cup")\n\nprint(json.dumps(command, indent=2))\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "action": "pick_up",\n  "object": "cup",\n  "object_properties": {"color": "red"},\n  "destination": null\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"stage-4-vla-action-generation",children:"Stage 4: VLA Action Generation"}),"\n",(0,r.jsx)(n.p,{children:"Connect parsed command to VLA model (from Chapter 10):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from transformers import AutoModelForVision2Seq, AutoProcessor\nimport torch\n\nclass VLAActionGenerator:\n    def __init__(self, model_name="openvla/openvla-7b", quantization="int8"):\n        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n\n        if quantization == "int8":\n            from transformers import BitsAndBytesConfig\n            quant_config = BitsAndBytesConfig(load_in_8bit=True)\n            self.model = AutoModelForVision2Seq.from_pretrained(\n                model_name,\n                quantization_config=quant_config,\n                device_map="auto",\n                trust_remote_code=True\n            )\n        else:\n            self.model = AutoModelForVision2Seq.from_pretrained(\n                model_name,\n                torch_dtype=torch.float16,\n                device_map="auto",\n                trust_remote_code=True\n            )\n\n    def generate_actions(self, image, parsed_command):\n        """\n        Generate robot actions from image + parsed command\n\n        Args:\n            image: PIL Image from robot camera\n            parsed_command: Structured command dict from LLM\n\n        Returns:\n            actions: numpy array of 7-DoF actions\n        """\n        # Convert structured command back to natural language\n        instruction = self._command_to_text(parsed_command)\n\n        inputs = self.processor(\n            text=instruction,\n            images=image,\n            return_tensors="pt"\n        ).to(self.model.device)\n\n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, max_new_tokens=512)\n\n        action_str = self.processor.decode(outputs[0], skip_special_tokens=True)\n        actions = np.array([float(x) for x in action_str.split(",")])\n\n        return actions\n\n    def _command_to_text(self, parsed_command):\n        """Convert structured command back to natural language"""\n        action = parsed_command.get("action", "")\n        obj = parsed_command.get("object", "")\n        props = parsed_command.get("object_properties", {})\n\n        # Build instruction\n        if action == "pick_up":\n            color = props.get("color", "")\n            return f"pick up the {color} {obj}".strip()\n        elif action == "place":\n            dest = parsed_command.get("destination", "table")\n            return f"place the {obj} on the {dest}"\n        else:\n            return f"{action} {obj}"\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"stage-5-complete-end-to-end-pipeline",children:"Stage 5: Complete End-to-End Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"Integrate all stages into single pipeline:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nComplete Voice-to-Action Pipeline\nMicrophone \u2192 Whisper \u2192 LLM \u2192 VLA \u2192 ROS 2\n\nEnd-to-end latency target: &lt;3 seconds\n"""\n\nimport time\nimport numpy as np\nfrom PIL import Image\n\n# Import our pipeline components\nfrom audio_capture import AudioCapture\nfrom whisper_asr import WhisperASR\nfrom command_parser import CommandParser\nfrom vla_generator import VLAActionGenerator\n\n# ROS 2 integration\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, Image as ImageMsg\nfrom cv_bridge import CvBridge\n\nclass VoiceControlledRobot(Node):\n    def __init__(self):\n        super().__init__(\'voice_controlled_robot\')\n\n        print("=" * 80)\n        print("Voice-Controlled Humanoid Robot Pipeline")\n        print("=" * 80)\n\n        # Initialize pipeline components\n        print("\\n[1/5] Initializing audio capture...")\n        self.audio = AudioCapture()\n\n        print("\\n[2/5] Loading Whisper ASR (base)...")\n        self.asr = WhisperASR(model_size="base")\n\n        print("\\n[3/5] Loading LLM command parser...")\n        self.parser = CommandParser()\n\n        print("\\n[4/5] Loading VLA model (OpenVLA-7B INT8)...")\n        self.vla = VLAActionGenerator(quantization="int8")\n\n        print("\\n[5/5] Setting up ROS 2 publishers...")\n        self.joint_pub = self.create_publisher(JointState, \'/joint_commands\', 10)\n        self.bridge = CvBridge()\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            ImageMsg, \'/camera/image_raw\', self.image_callback, 10\n        )\n        self.latest_image = None\n\n        print("\\n\u2705 Pipeline ready! Speak commands into microphone.\\n")\n\n    def image_callback(self, msg):\n        """Store latest camera image"""\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n        self.latest_image = Image.fromarray(cv_image)\n\n    def process_voice_command(self, duration=3.0):\n        """\n        Complete voice-to-action pipeline\n\n        Args:\n            duration: Recording duration in seconds\n\n        Returns:\n            success: Whether command was executed successfully\n        """\n        pipeline_start = time.time()\n\n        # Stage 1: Audio Capture\n        print("\\n\ud83c\udfa4 [Stage 1/5] Listening...")\n        audio_data = self.audio.record(duration=duration)\n        self.audio.save_wav(audio_data, "last_command.wav")\n\n        # Stage 2: Speech Recognition\n        print("\ud83d\udcdd [Stage 2/5] Transcribing speech...")\n        stage2_start = time.time()\n        text, confidence = self.asr.transcribe("last_command.wav")\n        stage2_time = time.time() - stage2_start\n\n        print(f"    Text: \'{text}\'")\n        print(f"    Confidence: {confidence:.1%}")\n        print(f"    Time: {stage2_time*1000:.0f}ms")\n\n        if confidence < 0.7:\n            print("\u26a0\ufe0f  Low confidence - please repeat command")\n            return False\n\n        # Stage 3: Command Parsing\n        print("\ud83e\udde0 [Stage 3/5] Parsing command...")\n        stage3_start = time.time()\n        parsed_cmd = self.parser.parse_command(text)\n        stage3_time = time.time() - stage3_start\n\n        print(f"    Action: {parsed_cmd.get(\'action\')}")\n        print(f"    Object: {parsed_cmd.get(\'object\')}")\n        print(f"    Time: {stage3_time*1000:.0f}ms")\n\n        # Stage 4: Action Generation\n        if self.latest_image is None:\n            print("\u26a0\ufe0f  No camera image available")\n            return False\n\n        print("\ud83e\udd16 [Stage 4/5] Generating actions...")\n        stage4_start = time.time()\n        actions = self.vla.generate_actions(self.latest_image, parsed_cmd)\n        stage4_time = time.time() - stage4_start\n\n        print(f"    Actions: {actions}")\n        print(f"    Time: {stage4_time*1000:.0f}ms")\n\n        # Stage 5: ROS 2 Execution\n        print("\ud83d\udce1 [Stage 5/5] Publishing to robot...")\n        stage5_start = time.time()\n\n        joint_msg = JointState()\n        joint_msg.header.stamp = self.get_clock().now().to_msg()\n        joint_msg.name = [\'joint1\', \'joint2\', \'joint3\', \'joint4\', \'joint5\', \'joint6\', \'gripper\']\n        joint_msg.position = actions.tolist()\n\n        self.joint_pub.publish(joint_msg)\n        stage5_time = time.time() - stage5_start\n\n        print(f"    Published to /joint_commands")\n        print(f"    Time: {stage5_time*1000:.0f}ms")\n\n        # Total pipeline latency\n        total_time = time.time() - pipeline_start\n\n        print("\\n" + "=" * 80)\n        print(f"\u2705 PIPELINE COMPLETE")\n        print(f"   Total Latency: {total_time:.2f}s")\n        print(f"   Breakdown:")\n        print(f"     - Audio Capture: {duration:.2f}s")\n        print(f"     - Speech Recognition: {stage2_time:.3f}s")\n        print(f"     - Command Parsing: {stage3_time:.3f}s")\n        print(f"     - Action Generation: {stage4_time:.3f}s")\n        print(f"     - ROS 2 Publish: {stage5_time:.3f}s")\n        print(f"   Target: &lt;3.0s | Actual: {total_time:.2f}s | {\'\u2705 PASS\' if total_time < 3.0 else \'\u274c FAIL\'}")\n        print("=" * 80 + "\\n")\n\n        return total_time < 3.0\n\ndef main():\n    rclpy.init()\n    robot = VoiceControlledRobot()\n\n    try:\n        while True:\n            input("\\nPress ENTER to record command (or Ctrl+C to quit)...")\n            robot.process_voice_command(duration=3.0)\n\n    except KeyboardInterrupt:\n        print("\\n\\n\ud83d\udc4b Shutting down voice control...")\n\n    robot.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"testing--evaluation",children:"Testing & Evaluation"}),"\n",(0,r.jsx)(n.p,{children:"Test with 20 diverse voice commands:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Pick & Place:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:'"Pick up the red cup"'}),"\n",(0,r.jsx)(n.li,{children:'"Place the book on the table"'}),"\n",(0,r.jsx)(n.li,{children:'"Grab the blue block"'}),"\n",(0,r.jsx)(n.li,{children:'"Put the object on the shelf"'}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Navigation:"}),'\n5. "Move to the kitchen"\n6. "Go to the door"\n7. "Navigate to the table"']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Manipulation:"}),'\n8. "Open the gripper"\n9. "Close the hand"\n10. "Rotate the object"']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Complex:"}),'\n11. "Pick up the red cup and place it on the table"\n12. "Move the blue block to the left"\n13. "Grasp the object carefully"\n14. "Put down what you\'re holding"']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Error Cases:"}),'\n15. [Mumbled speech]\n16. [Background noise]\n17. "Uhhh... pick up... the thing"\n18. [Silent - no speech]']}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Success Criteria (SC-003):"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"16/20 commands succeed"})," (80% success rate)"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"End-to-end latency <3 seconds"})]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Graceful error handling"})," (low confidence, no camera, etc.)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Whisper Base is optimal"})," - 200ms latency, 92% accuracy for robotics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LLM parsing adds robustness"})," - Structured commands easier for VLA"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"INT8 quantization essential"})," - Fit Whisper + VLA on single GPU"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"<3 second latency achievable"})," - On consumer RTX 4070 Ti hardware"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Error handling critical"})," - Low confidence, ambiguous commands, no camera"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Practice:"})," Run ",(0,r.jsx)(n.code,{children:"code-examples/voice-pipeline/voice_robot.py"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Test:"})," Record 20 commands, measure success rate"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimize:"})," Reduce latency with model quantization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deploy:"})," Integrate with real humanoid robot (Chapter 12)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Advanced:"})," Multi-turn dialogue, context awareness"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Continue to ",(0,r.jsx)(n.strong,{children:"Chapter 12: Full Integration"})," to deploy your complete voice-controlled humanoid robot system!"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);