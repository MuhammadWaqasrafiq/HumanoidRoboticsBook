"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[4441],{3546:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"voice-to-action/index","title":"11. Voice-to-Action Pipeline","description":"This chapter details the construction of a complete voice-to-action pipeline, enabling humanoid robots to respond to spoken natural language commands. We will integrate components for speech recognition, natural language understanding, task planning, and robot control.","source":"@site/docs/11-voice-to-action/index.mdx","sourceDirName":"11-voice-to-action","slug":"/voice-to-action/","permalink":"/HumanoidRoboticsBook/docs/voice-to-action/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/11-voice-to-action/index.mdx","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11,"title":"11. Voice-to-Action Pipeline"},"sidebar":"tutorialSidebar","previous":{"title":"10. Vision-Language-Action Models (VLA)","permalink":"/HumanoidRoboticsBook/docs/vla-models/"},"next":{"title":"12. Sim-to-Real Transfer Cookbook","permalink":"/HumanoidRoboticsBook/docs/sim-to-real/"}}');var i=o(4848),a=o(8453);const s={sidebar_position:11,title:"11. Voice-to-Action Pipeline"},r="11. Voice-to-Action Pipeline",c={},l=[{value:"Speech Recognition (ASR)",id:"speech-recognition-asr",level:2},{value:"Natural Language Understanding (NLU) for Robotics",id:"natural-language-understanding-nlu-for-robotics",level:2},{value:"Task Planning and Action Sequencing",id:"task-planning-and-action-sequencing",level:2},{value:"Further Exploration",id:"further-exploration",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",p:"p",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"11-voice-to-action-pipeline",children:"11. Voice-to-Action Pipeline"})}),"\n",(0,i.jsx)(n.p,{children:"This chapter details the construction of a complete voice-to-action pipeline, enabling humanoid robots to respond to spoken natural language commands. We will integrate components for speech recognition, natural language understanding, task planning, and robot control."}),"\n",(0,i.jsx)(n.h2,{id:"speech-recognition-asr",children:"Speech Recognition (ASR)"}),"\n",(0,i.jsx)(n.p,{children:"Converting spoken commands into text is the first step. This section explores Automatic Speech Recognition (ASR) systems suitable for robotic applications."}),"\n",(0,i.jsx)(n.h2,{id:"natural-language-understanding-nlu-for-robotics",children:"Natural Language Understanding (NLU) for Robotics"}),"\n",(0,i.jsx)(n.p,{children:'Once speech is transcribed, the robot needs to understand the intent and extract relevant entities (e.g., "red cup", "table") from the command.'}),"\n",(0,i.jsx)(n.h2,{id:"task-planning-and-action-sequencing",children:"Task Planning and Action Sequencing"}),"\n",(0,i.jsx)(n.p,{children:"Translating NLU output into a sequence of robot actions requires robust task planning. This involves breaking down high-level commands into low-level movements."}),"\n",(0,i.jsx)(n.h2,{id:"further-exploration",children:"Further Exploration"}),"\n",(0,i.jsx)(n.p,{children:"(Content to be added later)"})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>r});var t=o(6540);const i={},a=t.createContext(i);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);