<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-vla-models/quiz" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 10 Quiz - Vision-Language-Action Models | Embodied AI: The Future of Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://muhammadwaqasrafiq.github.io/HumanoidRoboticsBook/img/social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://muhammadwaqasrafiq.github.io/HumanoidRoboticsBook/img/social-card.jpg"><meta data-rh="true" property="og:url" content="https://muhammadwaqasrafiq.github.io/HumanoidRoboticsBook/docs/vla-models/quiz"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 10 Quiz - Vision-Language-Action Models | Embodied AI: The Future of Robotics"><meta data-rh="true" name="description" content="Test your understanding of VLA models, OpenVLA, GR00T N1, and robot learning"><meta data-rh="true" property="og:description" content="Test your understanding of VLA models, OpenVLA, GR00T N1, and robot learning"><link data-rh="true" rel="icon" href="/HumanoidRoboticsBook/img/new_logo.svg"><link data-rh="true" rel="canonical" href="https://muhammadwaqasrafiq.github.io/HumanoidRoboticsBook/docs/vla-models/quiz"><link data-rh="true" rel="alternate" href="https://muhammadwaqasrafiq.github.io/HumanoidRoboticsBook/docs/vla-models/quiz" hreflang="en"><link data-rh="true" rel="alternate" href="https://muhammadwaqasrafiq.github.io/HumanoidRoboticsBook/docs/vla-models/quiz" hreflang="x-default"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css"><link rel="stylesheet" href="/HumanoidRoboticsBook/assets/css/styles.a6d7e190.css">
<script src="/HumanoidRoboticsBook/assets/js/runtime~main.803ab506.js" defer="defer"></script>
<script src="/HumanoidRoboticsBook/assets/js/main.a5cf34ea.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/HumanoidRoboticsBook/"><div class="navbar__logo"><img src="/HumanoidRoboticsBook/img/new_logo.svg" alt="Embodied AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/HumanoidRoboticsBook/img/new_logo.svg" alt="Embodied AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Embodied AI</b></a><a class="navbar__item navbar__link" href="/HumanoidRoboticsBook/docs/intro">Book</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 10 Quiz: Vision-Language-Action Models</h1></header>
<p>Test your mastery of VLA concepts, model architectures, and deployment strategies!</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Quiz Instructions</div><div class="admonitionContent_BuS1"><ul>
<li class="">8 questions covering VLA fundamentals through advanced topics</li>
<li class="">Each question has detailed explanations</li>
<li class="">Score 80% or higher to demonstrate mastery</li>
<li class="">Review relevant sections if you score below 80%</li>
</ul></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="question-1-vla-architecture-components">Question 1: VLA Architecture Components<a href="#question-1-vla-architecture-components" class="hash-link" aria-label="Direct link to Question 1: VLA Architecture Components" title="Direct link to Question 1: VLA Architecture Components" translate="no">​</a></h2>
<p><strong>Which component is responsible for generating actual robot control commands in a VLA model?</strong></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>A) Vision Encoder (DINOv2, SigLIP)</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>The vision encoder extracts visual features from camera images but does NOT generate actions.</p><p><strong>What it does:</strong></p><ul>
<li class="">Input: 224x224 RGB image</li>
<li class="">Output: 768-1024D feature vector</li>
<li class="">Purpose: Scene understanding, object detection</li>
</ul><p><strong>For action generation:</strong> You need the Action Decoder (MLP, Diffusion, or Flow Matching).</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>B) Language Model (Llama 2, Gemma)</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>The language model processes text commands but doesn&#x27;t directly output robot actions.</p><p><strong>What it does:</strong></p><ul>
<li class="">Input: Natural language string</li>
<li class="">Output: Contextualized embeddings</li>
<li class="">Purpose: Command understanding, task reasoning</li>
</ul><p><strong>For actions:</strong> The fusion layer combines language + vision, then the action decoder generates motor commands.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>C) Fusion Layer (Cross-Attention)</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>The fusion layer combines vision and language features but doesn&#x27;t generate actions.</p><p><strong>What it does:</strong></p><ul>
<li class="">Input: Vision features + Language embeddings</li>
<li class="">Output: Fused multimodal representation</li>
<li class="">Purpose: Align visual observations with language commands</li>
</ul><p><strong>Next step:</strong> Fused features go to the action decoder.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>D) Action Decoder (MLP, Diffusion, Flow Matching)</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct!</strong></p><p>The <strong>action decoder</strong> is the final component that generates robot control commands.</p><p><strong>Three main types:</strong></p><ol>
<li class="">
<p><strong>MLP Decoder (OpenVLA)</strong></p>
<ul>
<li class="">Direct regression: fused_features  actions</li>
<li class="">Speed: 80-120ms (8-12 Hz)</li>
<li class="">Use case: Fast, deterministic control</li>
</ul>
</li>
<li class="">
<p><strong>Diffusion Decoder (Octo)</strong></p>
<ul>
<li class="">Iterative denoising: noise  actions</li>
<li class="">Speed: ~100ms (10-20 Hz)</li>
<li class="">Use case: Smooth trajectories, multimodal distributions</li>
</ul>
</li>
<li class="">
<p><strong>Flow Matching (0, GR00T N1)</strong></p>
<ul>
<li class="">Direct flow: noise  actions (no iteration)</li>
<li class="">Speed: 8-20ms (50-120 Hz)</li>
<li class="">Use case: Real-time humanoid control</li>
</ul>
</li>
</ol><p><strong>Output format:</strong> Typically 7-DoF vector: [joint1, joint2, joint3, joint4, joint5, joint6, gripper]</p></div></div></details>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="question-2-openvla-performance">Question 2: OpenVLA Performance<a href="#question-2-openvla-performance" class="hash-link" aria-label="Direct link to Question 2: OpenVLA Performance" title="Direct link to Question 2: OpenVLA Performance" translate="no">​</a></h2>
<p><strong>OpenVLA-7B outperforms RT-2-X (55B) by how much on the Open X-Embodiment benchmark?</strong></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>A) +5.2% absolute task success rate</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>OpenVLA&#x27;s improvement is much larger than 5.2%.</p><p><strong>Actual performance:</strong> +16.5% absolute task success rate with 7x fewer parameters.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>B) +16.5% absolute task success rate</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct!</strong></p><p>OpenVLA-7B achieves <strong>+16.5% absolute improvement</strong> over RT-2-X (55B) on 29 tasks.</p><p><strong>Key results:</strong></p><ul>
<li class=""><strong>OpenVLA-7B:</strong> 72.1% task success rate (7.23B parameters)</li>
<li class=""><strong>RT-2-X:</strong> 55.6% task success rate (55B parameters)</li>
<li class=""><strong>Improvement:</strong> +16.5 percentage points</li>
<li class=""><strong>Efficiency:</strong> 7x fewer parameters</li>
</ul><p><strong>Why OpenVLA wins:</strong></p><ol>
<li class=""><strong>Dual vision encoders:</strong> DINOv2 + SigLIP (complementary features)</li>
<li class=""><strong>Better training data:</strong> 970K trajectories from Open X-Embodiment</li>
<li class=""><strong>Efficient architecture:</strong> Llama 2-7B backbone optimized for robotics</li>
</ol><p><strong>Implication:</strong> Smaller, open-source models can outperform large proprietary models!</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>C) -8.3% (RT-2-X is better)</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>This is backwards! OpenVLA significantly outperforms RT-2-X.</p><p><strong>Actual:</strong> OpenVLA +16.5% better than RT-2-X.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>D) Equal performance (same success rate)</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>OpenVLA substantially outperforms RT-2-X.</p><p><strong>Actual:</strong> +16.5% absolute improvement with 7x fewer parameters.</p></div></div></details>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="question-3-model-quantization">Question 3: Model Quantization<a href="#question-3-model-quantization" class="hash-link" aria-label="Direct link to Question 3: Model Quantization" title="Direct link to Question 3: Model Quantization" translate="no">​</a></h2>
<p><strong>You have an RTX 4060 Ti with 16 GB VRAM. OpenVLA-7B requires 14 GB in FP16. What&#x27;s the BEST quantization strategy for production deployment?</strong></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>A) Keep FP16 precision (14 GB VRAM)</summary><div><div class="collapsibleContent_i85q"><p><strong>Suboptimal</strong></p><p>While FP16 fits on 16 GB VRAM, it leaves only 2 GB for:</p><ul>
<li class="">OS and other processes</li>
<li class="">Inference batch processing</li>
<li class="">Safety margin</li>
</ul><p><strong>Better:</strong> INT8 gives 50% VRAM reduction with &lt;1% accuracy loss.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>B) Use INT8 quantization (7 GB VRAM)</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct - Best Choice!</strong></p><p><strong>INT8 is optimal for production:</strong></p><p><strong>Advantages:</strong></p><ul>
<li class=""><strong>VRAM:</strong> 7 GB (50% reduction from FP16)</li>
<li class=""><strong>Speed:</strong> 1.5-2x faster inference</li>
<li class=""><strong>Accuracy:</strong> 71.6% vs 72.1% FP16 (-0.5%, negligible)</li>
<li class=""><strong>Headroom:</strong> 9 GB free for batch processing, OS, safety margin</li>
</ul><p><strong>Code:</strong></p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> transformers </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> BitsAndBytesConfig</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">quant_config </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> BitsAndBytesConfig</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    load_in_8bit</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    llm_int8_threshold</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">6.0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> AutoModelForVision2Seq</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;openvla/openvla-7b&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    quantization_config</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">quant_config</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    device_map</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div><p><strong>Production recommendation:</strong> INT8 for best balance of speed, accuracy, and resource efficiency.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>C) Use INT4 quantization (3.5 GB VRAM)</summary><div><div class="collapsibleContent_i85q"><p><strong>Over-aggressive</strong></p><p>INT4 saves maximum VRAM but sacrifices too much accuracy for production:</p><p><strong>Trade-offs:</strong></p><ul>
<li class=""><strong>VRAM:</strong> 3.5 GB (great!)</li>
<li class=""><strong>Speed:</strong> 2-3x faster (great!)</li>
<li class=""><strong>Accuracy:</strong> 69.8% vs 72.1% FP16 (-2.3%, <strong>significant</strong>)</li>
</ul><p><strong>When to use INT4:</strong></p><ul>
<li class="">Experimentation on low-VRAM GPUs (RTX 3060 8GB)</li>
<li class="">Rapid prototyping</li>
<li class="">Non-critical applications</li>
</ul><p><strong>For production:</strong> INT8 is better - minimal accuracy loss with good VRAM savings.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>D) Don&#x27;t quantize, buy more VRAM</summary><div><div class="collapsibleContent_i85q"><p>L <strong>Impractical</strong></p><p>Upgrading GPU is expensive and unnecessary when INT8 gives 50% VRAM reduction with &lt;1% accuracy loss.</p><p><strong>Better:</strong> Use INT8 quantization for production deployment.</p></div></div></details>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="question-4-gr00t-n1-dual-system-architecture">Question 4: GR00T N1 Dual-System Architecture<a href="#question-4-gr00t-n1-dual-system-architecture" class="hash-link" aria-label="Direct link to Question 4: GR00T N1 Dual-System Architecture" title="Direct link to Question 4: GR00T N1 Dual-System Architecture" translate="no">​</a></h2>
<p><strong>In GR00T N1&#x27;s dual-system architecture, what is the primary role of System 1?</strong></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>A) High-level task planning and reasoning</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>That&#x27;s <strong>System 2&#x27;s</strong> role (the VLM).</p><p><strong>System 2 (slow thinking):</strong></p><ul>
<li class="">Frequency: 1-2 Hz</li>
<li class="">Purpose: Reasons about scene, generates high-level plan</li>
<li class="">Example: &quot;To pick cup: 1) approach, 2) grasp, 3) lift, 4) move to table&quot;</li>
</ul><p><strong>System 1:</strong> Fast motor control (120 Hz).</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>B) Fast reactive motor control at 120Hz</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct!</strong></p><p><strong>System 1 (fast thinking)</strong> executes reactive motor control at <strong>120Hz</strong>.</p><p><strong>System 1 characteristics:</strong></p><ul>
<li class=""><strong>Frequency:</strong> 120 Hz (8ms per action)</li>
<li class=""><strong>Input:</strong> High-level plan from System 2 + current visual observations</li>
<li class=""><strong>Output:</strong> Continuous action sequence (joint positions, velocities)</li>
<li class=""><strong>Purpose:</strong> Real-time reactive control, balance recovery, obstacle avoidance</li>
</ul><p><strong>System 2 characteristics:</strong></p><ul>
<li class=""><strong>Frequency:</strong> 1-2 Hz</li>
<li class=""><strong>Input:</strong> Scene image + natural language command</li>
<li class=""><strong>Output:</strong> High-level task plan</li>
<li class=""><strong>Purpose:</strong> Deliberate reasoning, task understanding</li>
</ul><p><strong>Analogy to human cognition:</strong></p><ul>
<li class=""><strong>System 2:</strong> Conscious planning (&quot;I need to pick up this cup&quot;)</li>
<li class=""><strong>System 1:</strong> Automatic execution (muscle memory, reflexes)</li>
</ul><p><strong>Why this matters for humanoids:</strong></p><ul>
<li class="">Bipedal balance requires fast feedback (100+ Hz)</li>
<li class="">Task planning can be slower (1-2 Hz)</li>
<li class="">Separation allows optimization of each system independently</li>
</ul></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>C) Vision encoding from DINOv2</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>Vision encoding is part of both systems but not the primary role of System 1.</p><p><strong>System 1&#x27;s role:</strong> Fast motor control at 120Hz, not vision processing.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>D) Language command parsing</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>Language parsing is done by <strong>System 2</strong> (the VLM).</p><p><strong>System 1:</strong> Fast motor control based on System 2&#x27;s plan.</p></div></div></details>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="question-5-vla-model-selection">Question 5: VLA Model Selection<a href="#question-5-vla-model-selection" class="hash-link" aria-label="Direct link to Question 5: VLA Model Selection" title="Direct link to Question 5: VLA Model Selection" translate="no">​</a></h2>
<p><strong>You&#x27;re building a mobile manipulator for warehouse automation. Which VLA model is MOST appropriate?</strong></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>A) OpenVLA-7B</summary><div><div class="collapsibleContent_i85q"><p><strong>Good but not optimal</strong></p><p>OpenVLA is excellent for learning and research but:</p><ul>
<li class=""><strong>Strength:</strong> Open-source, well-documented, strong baseline</li>
<li class=""><strong>Limitation:</strong> Not specifically optimized for mobile manipulation or open-world scenarios</li>
</ul><p><strong>Better choice:</strong> 0.5 (designed for mobile manipulators with open-world generalization).</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>B) GR00T N1</summary><div><div class="collapsibleContent_i85q"><p>L <strong>Incorrect - Wrong Use Case</strong></p><p>GR00T N1 is designed for <strong>humanoid robots</strong>, not mobile manipulators.</p><p><strong>GR00T N1 strengths:</strong></p><ul>
<li class="">Bipedal locomotion</li>
<li class="">Whole-body coordination</li>
<li class="">Humanoid-specific training data (Fourier GR-1, 1X NEO)</li>
</ul><p><strong>For warehouse mobile manipulation:</strong> Use 0.5 or Octo instead.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>C) 0.5 (Physical Intelligence)</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct - Best Choice!</strong></p><p><strong>0.5 is optimal for warehouse mobile manipulation:</strong></p><p><strong>Key advantages:</strong></p><ol>
<li class=""><strong>Open-world generalization:</strong> Can clean entirely new environments (kitchens, bedrooms) zero-shot</li>
<li class=""><strong>Mobile manipulator focus:</strong> Trained on mobile manipulator platforms</li>
<li class=""><strong>Flow matching:</strong> 50Hz smooth trajectories</li>
<li class=""><strong>Large dataset:</strong> 10,000+ hours from 7 robot platforms</li>
</ol><p><strong>Real-world demonstration:</strong></p><ul>
<li class="">Physical Intelligence deployed 0.5 on mobile manipulators</li>
<li class="">Successfully cleaned new, unseen environments</li>
<li class="">Robust to environmental variations</li>
</ul><p><strong>Alternative:</strong> Octo (best for multi-embodiment transfer, fast fine-tuning)</p><p><strong>Why not others:</strong></p><ul>
<li class="">OpenVLA: Good baseline but not specialized for mobile manipulation</li>
<li class="">GR00T N1: Humanoid-specific</li>
<li class="">Helix: Humanoid upper-body only (Figure 02)</li>
</ul></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>D) Helix (Figure AI)</summary><div><div class="collapsibleContent_i85q"><p>L <strong>Incorrect - Wrong Platform</strong></p><p>Helix is designed for <strong>humanoid robots</strong> (specifically Figure 02), not mobile manipulators.</p><p><strong>Helix strengths:</strong></p><ul>
<li class="">Full humanoid upper-body control (35 DoF)</li>
<li class="">200Hz ultra-low-latency</li>
<li class="">Dexterous manipulation</li>
</ul><p><strong>For warehouse mobile manipulation:</strong> Use 0.5 instead.</p></div></div></details>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="question-6-action-decoder-types">Question 6: Action Decoder Types<a href="#question-6-action-decoder-types" class="hash-link" aria-label="Direct link to Question 6: Action Decoder Types" title="Direct link to Question 6: Action Decoder Types" translate="no">​</a></h2>
<p><strong>Which action decoder type produces the SMOOTHEST trajectories at the FASTEST inference speed?</strong></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>A) MLP Regressor (Direct prediction)</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>MLP is fast but NOT the smoothest.</p><p><strong>MLP characteristics:</strong></p><ul>
<li class=""><strong>Speed:</strong> 80-120ms (8-12 Hz) - moderate</li>
<li class=""><strong>Smoothness:</strong> Deterministic but can be jerky</li>
<li class=""><strong>Use case:</strong> OpenVLA, simple tasks</li>
</ul><p><strong>Smoothest + fastest:</strong> Flow Matching (0, GR00T N1) at 50-120 Hz.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>B) Diffusion Policy (Iterative denoising)</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>Diffusion produces smooth trajectories but is SLOWER.</p><p><strong>Diffusion characteristics:</strong></p><ul>
<li class=""><strong>Speed:</strong> ~100ms (10-20 Hz) - slow due to iteration</li>
<li class=""><strong>Smoothness:</strong> Excellent (gradual denoising)</li>
<li class=""><strong>Use case:</strong> Octo, complex multi-modal tasks</li>
</ul><p><strong>Faster:</strong> Flow Matching eliminates iteration, achieving 50-120 Hz.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>C) Flow Matching (Direct flow)</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct!</strong></p><p><strong>Flow Matching</strong> achieves both smoothness AND speed.</p><p><strong>How it works:</strong></p><ul>
<li class=""><strong>Method:</strong> Direct flow from noise distribution to action distribution (no iteration)</li>
<li class=""><strong>Speed:</strong> 8-20ms per action (50-120 Hz)</li>
<li class=""><strong>Smoothness:</strong> Continuous flow  smooth trajectories</li>
<li class=""><strong>Used by:</strong> 0 (50Hz), 0-FAST (50+Hz), GR00T N1 (120Hz)</li>
</ul><p><strong>Comparison:</strong></p><table><thead><tr><th>Decoder Type</th><th>Speed (Hz)</th><th>Smoothness</th><th>Inference Method</th></tr></thead><tbody><tr><td><strong>MLP</strong></td><td>8-12</td><td>Moderate</td><td>Direct regression</td></tr><tr><td><strong>Diffusion</strong></td><td>10-20</td><td>Excellent</td><td>Iterative (20-100 steps)</td></tr><tr><td><strong>Flow Matching</strong></td><td>50-120</td><td>Excellent</td><td>Direct (1 step)</td></tr></tbody></table><p><strong>Key innovation:</strong> Flow matching gets diffusion&#x27;s smoothness without iteration penalty.</p><p><strong>Why it matters:</strong></p><ul>
<li class="">Humanoid control needs 50+ Hz for balance</li>
<li class="">Flow matching enables real-time reactive control</li>
<li class="">Smoother than MLP, faster than diffusion</li>
</ul></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>D) Transformer Decoder (Autoregressive)</summary><div><div class="collapsibleContent_i85q"><p><strong>Incorrect</strong></p><p>Transformer autoregressive decoding is typically SLOWER than all three options.</p><p><strong>Transformer characteristics:</strong></p><ul>
<li class=""><strong>Speed:</strong> Sequential generation (slow)</li>
<li class=""><strong>Smoothness:</strong> Depends on training</li>
<li class=""><strong>Use case:</strong> Language modeling, not typical for VLA action decoders</li>
</ul><p><strong>Best for actions:</strong> Flow Matching (fast + smooth).</p></div></div></details>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="question-7-training-data-scale">Question 7: Training Data Scale<a href="#question-7-training-data-scale" class="hash-link" aria-label="Direct link to Question 7: Training Data Scale" title="Direct link to Question 7: Training Data Scale" translate="no">​</a></h2>
<p><strong>Which VLA model was trained on the LARGEST dataset?</strong></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>A) OpenVLA - 970K robot trajectories</summary><div><div class="collapsibleContent_i85q"><p><strong>Large but not the largest</strong></p><p>OpenVLA uses 970,000 trajectories from Open X-Embodiment dataset.</p><p><strong>Estimated hours:</strong> ~5,000-7,000 hours of robot operation.</p><p><strong>Largest:</strong> 0 with 10,000 hours from 7 platforms.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>B) Octo - 800K robot trajectories</summary><div><div class="collapsibleContent_i85q"><p>L <strong>Not the largest</strong></p><p>Octo uses 800,000 trajectories from 25 datasets.</p><p><strong>Estimated hours:</strong> ~4,000-6,000 hours.</p><p><strong>Largest:</strong> 0 with 10,000 hours.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>C) 0 - 10,000 hours from 7 robot platforms</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct!</strong></p><p><strong>0 has the largest training dataset:</strong> 10,000 hours of robot operation.</p><p><strong>Dataset breakdown:</strong></p><ul>
<li class=""><strong>Platforms:</strong> 7 different robot embodiments</li>
<li class=""><strong>Tasks:</strong> 68 unique task categories</li>
<li class=""><strong>Data type:</strong> Real-world teleoperation demonstrations</li>
<li class=""><strong>Quality:</strong> High-quality, multi-operator data</li>
</ul><p><strong>Why this matters:</strong></p><ol>
<li class=""><strong>Better generalization:</strong> More diverse scenarios covered</li>
<li class=""><strong>Robust policies:</strong> Learned from many edge cases</li>
<li class=""><strong>Zero-shot transfer:</strong> Can handle novel situations</li>
</ol><p><strong>Comparison:</strong></p><ul>
<li class=""><strong>0:</strong> 10,000 hours (largest)</li>
<li class=""><strong>OpenVLA:</strong> ~6,000 hours equivalent (970K trajectories)</li>
<li class=""><strong>Octo:</strong> ~5,000 hours equivalent (800K trajectories)</li>
<li class=""><strong>Helix:</strong> 500 hours (smallest but highest quality)</li>
</ul><p><strong>Note:</strong> Helix&#x27;s 500 hours is multi-robot, multi-operator, specifically for humanoids.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>D) RT-2-X - Web-scale vision-language data + robot data</summary><div><div class="collapsibleContent_i85q"><p><strong>Different metric</strong></p><p>RT-2-X uses web-scale pretraining BUT has less robot-specific data.</p><p><strong>Robot data:</strong> Estimated &lt;1,000 hours of robot trajectories
<strong>Web data:</strong> Billions of image-text pairs (not robot-specific)</p><p><strong>0 has more robot-specific training data:</strong> 10,000 hours pure robot demonstrations.</p></div></div></details>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="question-8-practical-deployment">Question 8: Practical Deployment<a href="#question-8-practical-deployment" class="hash-link" aria-label="Direct link to Question 8: Practical Deployment" title="Direct link to Question 8: Practical Deployment" translate="no">​</a></h2>
<p><strong>You need to deploy a VLA model on a Jetson AGX Orin (32GB RAM, 8-core ARM CPU, moderate GPU). What&#x27;s your BEST strategy?</strong></p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>A) Run OpenVLA-7B in FP16 (14 GB VRAM)</summary><div><div class="collapsibleContent_i85q"><p>L <strong>Won&#x27;t fit</strong></p><p>Jetson AGX Orin has integrated GPU sharing system RAM, not 14+ GB dedicated VRAM.</p><p><strong>Jetson AGX Orin specs:</strong></p><ul>
<li class=""><strong>Total RAM:</strong> 32 GB (shared CPU/GPU)</li>
<li class=""><strong>GPU VRAM:</strong> ~8-12 GB effective after OS</li>
<li class=""><strong>FP16 OpenVLA:</strong> Requires 14 GB  <strong>won&#x27;t fit</strong></li>
</ul><p><strong>Solution:</strong> Use INT8 or smaller model.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>B) Run OpenVLA-7B with INT8 quantization (7 GB VRAM)</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct - Best Strategy!</strong></p><p><strong>INT8 quantization makes OpenVLA viable on Jetson:</strong></p><p><strong>Why this works:</strong></p><ul>
<li class=""><strong>VRAM:</strong> 7 GB (fits in Jetson&#x27;s effective GPU memory)</li>
<li class=""><strong>Accuracy:</strong> 71.6% (-0.5% from FP16, negligible)</li>
<li class=""><strong>Speed:</strong> 1.5-2x faster than FP16 (important for ARM CPU)</li>
<li class=""><strong>Deployment:</strong> Production-ready with minimal accuracy loss</li>
</ul><p><strong>Implementation:</strong></p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> transformers </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> BitsAndBytesConfig</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">quant_config </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> BitsAndBytesConfig</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">load_in_8bit</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> AutoModelForVision2Seq</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;openvla/openvla-7b&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    quantization_config</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">quant_config</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    device_map</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;auto&quot;</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Automatic GPU placement</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div><p><strong>Alternative:</strong> 0-FAST (3.5 GB, optimized for edge deployment).</p><p><strong>Jetson deployment tips:</strong></p><ol>
<li class="">Use INT8 quantization</li>
<li class="">Enable TensorRT optimization (NVIDIA native)</li>
<li class="">Reduce batch size to 1 for real-time inference</li>
<li class="">Monitor temperature (Jetson can throttle under load)</li>
</ol></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>C) Use cloud API (Gemini Robotics)</summary><div><div class="collapsibleContent_i85q"><p>L <strong>Defeats purpose of edge deployment</strong></p><p>Cloud APIs require internet connectivity, introducing:</p><ul>
<li class=""><strong>Latency:</strong> 50-200ms network round-trip (too slow for real-time control)</li>
<li class=""><strong>Reliability:</strong> Fails when internet drops</li>
<li class=""><strong>Privacy:</strong> Sends camera/sensor data to cloud</li>
<li class=""><strong>Cost:</strong> API fees per request</li>
</ul><p><strong>Edge deployment purpose:</strong> Low latency, offline operation, privacy.</p><p><strong>Better:</strong> INT8 quantized local model.</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>D) Run GR00T N1 with INT4 quantization</summary><div><div class="collapsibleContent_i85q"><p><strong>Possible but not optimal</strong></p><p>GR00T N1 requires more resources than OpenVLA even with INT4:</p><ul>
<li class=""><strong>Architecture:</strong> Dual-system (System 1 + System 2) = 2 models</li>
<li class=""><strong>Complexity:</strong> 120Hz control loop requires more compute</li>
<li class=""><strong>VRAM:</strong> ~6-8 GB with INT4 (tight fit on Jetson)</li>
</ul><p><strong>Better:</strong> OpenVLA-7B INT8 is simpler and proven on edge devices.</p><p><strong>GR00T N1 edge deployment:</strong> Wait for optimized edge-specific version.</p></div></div></details>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="score-interpretation">Score Interpretation<a href="#score-interpretation" class="hash-link" aria-label="Direct link to Score Interpretation" title="Direct link to Score Interpretation" translate="no">​</a></h2>
<p><strong>Calculate your score:</strong></p>
<ul>
<li class="">8/8 correct: <strong>Expert</strong> - Ready for VLA production deployment</li>
<li class="">6-7/8 correct: <strong>Proficient</strong> - Strong understanding, minor gaps</li>
<li class="">4-5/8 correct: <strong>Developing</strong> - Review key sections</li>
<li class="">0-3/8 correct: <strong>Needs Review</strong> - Re-read Chapter 10</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways" translate="no">​</a></h2>
<p>If you struggled with specific topics, review these sections:</p>
<table><thead><tr><th>Question</th><th>Topic</th><th>Review Section</th></tr></thead><tbody><tr><td>Q1</td><td>VLA Architecture</td><td>VLA Architecture Fundamentals</td></tr><tr><td>Q2</td><td>OpenVLA Performance</td><td>OpenVLA: Your First VLA Model</td></tr><tr><td>Q3</td><td>Quantization</td><td>Model Quantization (Reduce VRAM)</td></tr><tr><td>Q4</td><td>GR00T N1 Architecture</td><td>GR00T N1: Humanoid-Specific VLA</td></tr><tr><td>Q5</td><td>Model Selection</td><td>VLA Model Comparison</td></tr><tr><td>Q6</td><td>Action Decoders</td><td>VLA Architecture  Action Decoder</td></tr><tr><td>Q7</td><td>Training Data</td><td>Research: VLA Model Landscape</td></tr><tr><td>Q8</td><td>Edge Deployment</td><td>Model Quantization + Practical Tips</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="next-steps">Next Steps<a href="#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps" translate="no">​</a></h2>
<p>After completing this quiz:</p>
<ol>
<li class=""><strong>Score &gt;= 80%:</strong> Proceed to Chapter 11 (Voice-to-Action Pipeline)</li>
<li class=""><strong>Score &lt; 80%:</strong> Review Chapter 10 sections and retry quiz</li>
<li class=""><strong>Hands-on:</strong> Run <code>code-examples/vla/openvla-inference.py</code></li>
<li class=""><strong>Advanced:</strong> Experiment with INT8 vs INT4 quantization</li>
</ol>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Practice Challenge</div><div class="admonitionContent_BuS1"><p><strong>Deploy OpenVLA on your GPU:</strong></p><ol>
<li class="">Install dependencies (PyTorch, transformers, bitsandbytes)</li>
<li class="">Run <code>openvla-inference.py</code> with INT8 quantization</li>
<li class="">Test with 5 different natural language commands</li>
<li class="">Measure inference latency and VRAM usage</li>
<li class="">Compare FP16 vs INT8 performance</li>
</ol><p><strong>Time budget:</strong> 30-45 minutes
<strong>Difficulty:</strong> Intermediate
<strong>Prerequisites:</strong> RTX GPU (8+ GB VRAM), Python 3.10+</p></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook/tree/main/docs/vla-models/quiz.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#question-1-vla-architecture-components" class="table-of-contents__link toc-highlight">Question 1: VLA Architecture Components</a></li><li><a href="#question-2-openvla-performance" class="table-of-contents__link toc-highlight">Question 2: OpenVLA Performance</a></li><li><a href="#question-3-model-quantization" class="table-of-contents__link toc-highlight">Question 3: Model Quantization</a></li><li><a href="#question-4-gr00t-n1-dual-system-architecture" class="table-of-contents__link toc-highlight">Question 4: GR00T N1 Dual-System Architecture</a></li><li><a href="#question-5-vla-model-selection" class="table-of-contents__link toc-highlight">Question 5: VLA Model Selection</a></li><li><a href="#question-6-action-decoder-types" class="table-of-contents__link toc-highlight">Question 6: Action Decoder Types</a></li><li><a href="#question-7-training-data-scale" class="table-of-contents__link toc-highlight">Question 7: Training Data Scale</a></li><li><a href="#question-8-practical-deployment" class="table-of-contents__link toc-highlight">Question 8: Practical Deployment</a></li><li><a href="#score-interpretation" class="table-of-contents__link toc-highlight">Score Interpretation</a></li><li><a href="#key-takeaways" class="table-of-contents__link toc-highlight">Key Takeaways</a></li><li><a href="#next-steps" class="table-of-contents__link toc-highlight">Next Steps</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learn</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/HumanoidRoboticsBook/docs/intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/HumanoidRoboticsBook/docs/why-physical-ai">Why Physical AI</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Repo<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook/tree/main/code-examples" target="_blank" rel="noopener noreferrer" class="footer__link-item">Code Samples<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Connect</div><ul class="footer__items clean-list"><li class="footer__item">
<div style="display:flex; gap:1.5rem; margin-bottom:1rem;">
  <a href="https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook" target="_blank" rel="noopener noreferrer" aria-label="GitHub" style="color:#fff;font-size:1.6rem;">
    <i class="fa-brands fa-github"></i>
  </a>
  <a href="https://linkedin.com/in/MuhammadWaqasrafiq" target="_blank" rel="noopener noreferrer" aria-label="LinkedIn" style="color:#fff;font-size:1.6rem;">
    <i class="fa-brands fa-linkedin"></i>
  </a>
  <a href="https://wa.me/923463033195" target="_blank" rel="noopener noreferrer" aria-label="WhatsApp" style="color:#fff;font-size:1.6rem;">
    <i class="fa-brands fa-whatsapp"></i>
  </a>
  <a href="https://www.youtube.com/watch?v=dZTfXiPSZyE" target="_blank" rel="noopener noreferrer" aria-label="YouTube" style="color:#fff;font-size:1.6rem;">
    <i class="fa-brands fa-youtube"></i>
  </a>
</div>
              </li><li class="footer__item"><a href="https://github.com/MuhammadWaqasrafiq/HumanoidRoboticsBook/blob/main/LICENSE" target="_blank" rel="noopener noreferrer" class="footer__link-item">MIT License<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">© 2025 Embodied AI • Muhammad Waqas Rafiq • MIT & CC-BY-4.0</div></div></div></footer></div>
</body>
</html>